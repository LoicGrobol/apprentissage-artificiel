---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
TP 4‚ÄØ: *Na√Øve Bayes*
=======================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display
```

## Pitch

La **classification de documents** est une t√¢che de TAL qui consiste √† ranger un document dans une
et une seule classe parmi un ensemble pr√©d√©fini.

Elle est tr√®s importante historiquement, car elle a √©t√© une des passerelles entre l'informatique (o√π
on peut la voir comme un cas particulier de la t√¢che g√©n√©rale de classification) et le TAL. En
pratique, comme beaucoup des t√¢ches de TAL peuvent se reformuler comme des t√¢ches de classification,
elle est aussi d'une importance cruciale.

En ce qui nous concerne, elle est aussi int√©ressante parce que les techniques classiques de
classification par apprentissage vont nous donner l'occasion de (re?)d√©couvrir des concepts qui vont
nous servir dans toute la suite de ce cours. On va l'aborder au mod√®le *Na√Øve Bayes* (le ¬´‚ÄØmod√®le
bay√©sien na√Øf‚ÄØ¬ª) appliqu√©es au mod√®le de repr√©sentation des documents par **sacs de mots**.

On se basera pour la th√©orie et les notations sur les chapitres 4 de [*Speech and Language
Processing*](https://web.stanford.edu/~jurafsky/slp3/) de Daniel Jurafsky et James H. Martin, qu'il
est donc bon de garder √† port√©e de main. On pourra aussi aller regarder pour des maths plus
rigoureuses l'article [*Multinomial Naive Bayes for Text Categorization
Revisited*](https://link.springer.com/chapter/10.1007/978-3-540-30549-1_43) (Kibriya et al., 2006). 


Pour √©viter d'avoir √† pr√©dater des donn√©es, on va se servir du [dataset d'exemple de `scikit-learn`
*20
newsgroups*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html),
en revanche on √©vitera de se servir directement des fonctions de `scikit-learn`. On sait d√©j√† faire
et l'objectif ici est de le faire √† la main pour bien comprendre ce qui se passe. Par contre en
dehors de ce TP, ne r√©inventez pas la roue, et n'h√©sitez pas √† aller lire [la
doc](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) de
scikit-learn qui, comme d'habitude est particuli√®rement int√©ressante.

**C'est parti‚ÄØ!**

```python
%pip install -U numpy scikit-learn
```

```python
import numpy as np
```

## Le dataset

```python
from sklearn.datasets import fetch_20newsgroups

categories = [
    "sci.crypt",
    "sci.electronics",
    "sci.med",
    "sci.space",
]

data_train = fetch_20newsgroups(
    subset='train',
    categories=categories,
    shuffle=True,
)

data_test = fetch_20newsgroups(
    subset='test',
    categories=categories,
    shuffle=True,
)
```

```python
print(data_train.DESCR)
```

<!-- #region slideshow={"slide_type": "subslide"} -->
Bon, voyons ce que ce truc a concr√®tement dans le ventre
<!-- #endregion -->

```python
print(len(data_train.data))
print(len(data_test.data))
```

```python
data_train.keys()
```

```python
type(data_train.data)
```

```python
data_train.data[0]
```

```python
type(data_train.target)
```

```python
data_train.target[0]
```

```python
data_train.target_names[0]
```

Bon, c'est plut√¥t clair

## Sac de mots


On va commencer par transformer ces documents en sacs de mots.

```python
import re
def poor_mans_tokenizer_and_normalizer(s):
    # Cette fois-ci on vire les nombres, les signes de ponctuation et les trucs bizarres
    return [w.lower() for w in re.split(r"\s|\W", s.strip()) if w and all(c.isalpha() for c in w)]
```

```python
from collections import Counter

def get_counts(doc):
    return Counter(poor_mans_tokenizer_and_normalizer(doc))

get_counts(data_train.data[0])
```

Maintenant, il faut le faire pour tous les docs üèπ

```python
bows = [get_counts(doc) for doc in data_train.data]
```

Et il nous faut r√©cup√©rer tout le vocabulaire

```python
vocab = sorted(set().union(*bows)) # Pourquoi `sorted` √† votre avis‚ÄØ?
len(vocab)
```

Et pour rendre le tout facile √† manipuler on va en en faire un tableau NumPy de la forme
`len(train_data)√ólen(vocab)` qui tel que le contenu de la cellule `(i, j)` soit le nombre
d'occurrences du mot `i` dans le document `j`.


On commence par faire un dict avec le vocabulaire

```python
w_to_i = {w: i for i, w in enumerate(vocab)}
```

```python
bow_array = np.zeros((len(bows), len(vocab)))
for i, bag in enumerate(bows):
    for w, c in bag.items():
        bow_array[i, w_to_i[w]] = c
```

On peut aussi faire comme √ßa, mais c'est plus lent. Est-ce que vous pouvez devinez pourquoi‚ÄØ?

```python
# bow_array = np.array(
#     [
#         [bag[w] for w in vocab]
#         for bag in bows
#     ]
# )
```

```python
bow_array
```


## üßôüèª Exo üßôüèª

√Ä vous de bosser maintenant. √âcrivez

1\. Une fonction qui prend en argument un tableau comme `data_train.target`¬†et qui renvoie un
tableau `class_probs` tel que `class_probs[c]` soit $P(c)$. On choisira le mod√®le de vraisemblance
maximal, soit ici simplement celui qui utilise pour probabilit√©s les fr√©quences empiriques $P(c) =
\frac{\text{nombre d'occurrences de $c$}}{\text{taille de l'√©chantillon}}$.

```python
def get_class_probs(target):
    pass  # Votre code ici
```

2\. Une fonction qui prend en argument un tableau comme `bow_array`¬†et un tableau de classes
`target` comme pr√©c√©demment et renvoie un tableau `word_probs` tel que `word_probs[c][w]` soit
$P(w|c)$. On utilise toujours le mod√®le de vraisemblance maximal, mais avec un **lissage
laplacien**‚ÄØ: $P(w|c)=\frac{\text{nombre d'occurences de $w$ dans $c$} + 1}{\text{nombre de mots
dans l'ensemble des documents de $c$}+\text{taille du vocabulaire}}$.

N'h√©sitez pas √† √©crire des boucles, au moins pour commencer, avant de passer √† du NumPy fancy.

```python
def get_word_probs(bows):
    pass  # Votre code ici
```

Voil√†, on a un mod√®le de classification *Na√Øve Bayes* üëèüèª


Il reste √† savoir comment s'en servir. N'h√©sitez pas √† faire des fonctions auxiliaires et √† aller
lire le chapitre *Na√Øve Bayes* de *Speech and Language Processing.


3\. √âcrire une fonction qui prend en argument un document et renvoie la classe la plus probable
notre mod√®le. Pensez √† travailler en log-probabilit√©s

```python
def predict_class(doc):
    pass  # Votre code ici
```

Vous pouvez maintenant √©valuer le mod√®le en calculant son exactitude sur l'ensemble de test.


4\. Un script avec deux commandes‚ÄØ: une qui entra√Æne le mod√®le et le sauvegarde (sous la forme qui
vous para√Æt la plus appropri√©e) et une qui charge le mod√®le et pr√©dit la classe de chacun des
documents d'un corpus.

Courage, c'est pour votre bien. Si vous vous ennuyez √ßa peut √™tre le bon moment pour d√©couvrir
[click](https://click.palletsprojects.com/en/8.0.x/).
