---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

Mod√®les de langues √† n-grammes‚ÄØ: corrections
============================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2022-09-28

## ‚úÇÔ∏è Tokenization ‚úÇÔ∏è

1\. √âcrire une fonction `crude_tokenizer` qui prend comme argument une chaine de caract√®res et
    renvoie la liste des mots de cette cha√Æne en s√©parant sur les espaces.

```python
def crude_tokenizer(s):
    return s.split()

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', "l'existence", 'du', 'kiwi-fruit.'
]
```

2\. Modifier la fonction `crude_tokenizer` pour qu'elle s√©pare aussi suivant les caract√®res
   non alphanum√©riques. **Indice** √ßa peut √™tre utile de revoir [la doc sur les expressions
   r√©guli√®res](https://docs.python.org/3/library/re.html) ou de relire [un tuto √† ce
   sujet](https://realpython.com/regex-python/).

```python
import re
def crude_tokenizer(s):
    return [w for w in re.split(r"\s|\W", s.strip()) if w]

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', 'l', 'existence', 'du', 'kiwi', 'fruit'
]
```

3\. On aimerait maintenant garder les apostrophes √† la fin du mot qui les pr√©c√®de, ainsi que les
mots compos√©s ensemble.

```python
import re  # Si jamais on a pas ex√©cut√© la cellule pr√©c√©dente
def crude_tokenizer(s):
    return re.findall(r"\b\w+?\b(?:'|(?:-\w+?\b)*)?", s)

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit'
]
```

4\. √âcrire une fonction `crude_tokenizer_and_normalizer` qui en plus de tokenizer comme pr√©c√©demment
met tous les mots en minuscules

On peut √©videmment copier-coller le code au-dessus, mais on peut aussi r√©utiliser ce qu'on a d√©j√†
d√©fini‚ÄØ:

```python
def crude_tokenizer_and_normalizer(s):
    return crude_tokenizer(s.lower())

assert crude_tokenizer_and_normalizer("Je reconnais l'existence du kiwi-fruit.") == [
    'je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit'
]
```

## üíú Extraire les bigrammes üíú

√âcrire une fonction `extract_bigrams` qui prend en entr√©e une liste de mots et renvoie la liste des
bigrammes correspondants sous forme de couples de mots.


Version directe

```python
def extract_bigrams(words):
    res = []
    for i in range(len(words)-1):
        res.append((words[i], words[i+1]))
    return res

assert extract_bigrams(['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']) == [
    ('je', 'reconnais'),
     ('reconnais', "l'"),
     ("l'", 'existence'),
     ('existence', 'du'),
     ('du', 'kiwi-fruit')
]
```

Version artistique

```python
def extract_bigrams(words):
    return list(zip(words[:-1], words[1:]))

assert extract_bigrams(['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']) == [
    ('je', 'reconnais'),
     ('reconnais', "l'"),
     ("l'", 'existence'),
     ('existence', 'du'),
     ('du', 'kiwi-fruit')
]
```

Si vous trouvez √ßa obscur essayez le code ci-dessous, et allez voir ce qu'il donne [sur Python Tutor](https://pythontutor.com/render.html#code=tokenized%20%3D%20%5B'Je',%20'reconnais',%20%22l'%22,%20'existence',%20'du',%20'kiwi-fruit'%5D%0A%0Afirst_words%20%3D%20tokenized%5B%3A-1%5D%0Asecond_words%20%3D%20tokenized%5B1%3A%5D%0A%0Afor%20t%20in%20zip%28first_words,%20second_words%29%3A%0A%20%20%20%20print%28t%29&cumulative=false&curInstr=10&heapPrimitives=nevernest&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false)

```python
tokenized = ['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']

first_words = tokenized[:-1]
second_words = tokenized[1:]

print(first_words)
print(second_words)

for t in zip(first_words, second_words):
    print(t)
```


## üî¢ Compter üî¢

√âcrire une fonction `read_corpus` qui prend en argument un chemin vers un fichier texte, l'ouvre, le
tokenize et y compte les unigrammes et les bigrammes en renvoyant deux `Counter` associant
respectivement √† chaque mot et √† chaque bigramme leurs nombres d'occurrences.

```python
from collections import Counter
    
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            unigrams.update(words)
            bigrams.update(extract_bigrams(words))
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('de', 5292), ('la', 3565), ('les', 2746), ('il', 2443)]
assert bigram_counts.most_common(4) == [
    (('de', 'la'), 754),
     (("qu'", 'il'), 424),
     (('√†', 'la'), 336),
     (("d'", 'une'), 321)
]
```

## ü§ì Estimer les probas ü§ì

√âcrire une fonction `get_probs`, qui prend en entr√©e les compteurs de bigrammes et
d'unigrammes et renvoie le dictionnaire `probs`

```python
def get_probs(unigram_counts, big√†ram_counts):
    probs = dict()
    for (v, w), c in bigram_counts.items():
        if v not in probs:
            # Si on a pas encore rencontr√© de bigrammes commen√ßant par `v`, il faut
            # commencer par cr√©er `probs[v]`
            probs[v] = dict()
        probs[v][w] = c/unigram_counts[v]
    return probs

probs = get_probs(unigram_counts, bigram_counts)
assert probs["je"]["d√©jeune"] == 0.002232142857142857
```

Avec `collections.defaultdict`‚ÄØ:

```python
from collections import defaultdict

def get_probs(unigram_counts, bigram_counts):
    # Un dictionnaire de dictionnaires cr√©√©s automatiquement √† l'acc√®s,
    # √áa √©vite de faire un test et √ßa rend souvent le code plus lisible
    probs = defaultdict(dict)
    for (v, w), c in bigram_counts.items():
        probs[v][w] = c/unigram_counts[v]

    # Pour ne pas masquer des erreurs pendant le sampling, on en refait un dict normal
    return dict(probs)

probs = get_probs(unigram_counts, bigram_counts)
assert probs["je"]["d√©jeune"] == 0.002232142857142857
```

## üíÅüèª G√©n√©rer un mot üíÅüèª

> √Ä vous de jouer‚ÄØ: √©crire une fonction `gen_next_word` qui prend en entr√©e le dictionnaire `probs`
> et un mot et renvoie en sortie un mot suivant, choisi en suivant les probabilit√©s estim√©es
> pr√©c√©demment

```python
import random

def gen_next_word(probs, prompt):
    # On convertit en liste pour s'assurer que les mots et les poids sont bien dans le m√™me ordre
    candidates = list(probs[prompt].keys())
    weights = [probs[prompt][c] for c in candidates]
    return random.choices(candidates, weights)[0]
```


## ü§î G√©n√©rer ü§î

1\. Modifier `read_corpus` pour ajouter √† la vol√©e `<s>` au d√©but de chaque ligne et `</s>`
√† la fin de chaque ligne.

```python
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            words.insert(0, "<s>")
            words.append("</s>")
            unigrams.update(words)
            bigrams.update(extract_bigrams(words))
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('<s>', 8945), ('</s>', 8945), ('de', 5292), ('la', 3565)]
assert bigram_counts.most_common(4) == [
    (('<s>', '</s>'), 1811),
    (('<s>', 'il'), 775),
    (('de', 'la'), 754),
    (('<s>', 'elle'), 576)
]
```

2\. Modifier `read_corpus` pour ignorer les lignes vides

```python
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            if line.isspace():
                continue
            words = crude_tokenizer_and_normalizer(line.strip())
            words.insert(0, "<s>")
            words.append("</s>")
            unigrams.update(words)
            bigrams.update(extract_bigrams(words))
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('<s>', 7145), ('</s>', 7145), ('de', 5292), ('la', 3565)]
assert bigram_counts.most_common(4) == [
    (('<s>', 'il'), 775),
    (('de', 'la'), 754),
    (('<s>', 'elle'), 576),
    (("qu'", 'il'), 424)
]

probs = get_probs(unigram_counts, bigram_counts)
assert probs["<s>"]["le"] == 0.0298110566829951
```

## üòå G√©n√©rer pour de vrai üòå

√âcrire une fonction `sample` qui prend en argument les probabilit√©s de bigrammes (sous la forme d'un
dictionnaire de dictionnaires comme notre `prob`) et g√©n√®re une phrase en partant de `<s>` et en
ajoutant des mots it√©rativement, s'arr√™tant quand `</s>` a √©t√© choisi.

```python
import random

def generate(bigram_probs):
    sent = ["<s>"]
    while sent[-1] != "</s>":
        sent.append(gen_next_word(bigram_probs, sent[-1]))
    return sent
```

Pas de `assert` ici comme on a de l'al√©atoire, mais la cellule suivante permet de tester si √ßa
marche‚ÄØ:

```python
print(generate(probs))
```

Et ici pour avoir du texte qui ressemble √† quelque chose‚ÄØ:

```python
print(" ".join(generate(probs)[1:-1]))
```

## üßê Aller plus loin üßê


### 3Ô∏è‚É£ Trigrammes 3Ô∏è‚É£

Coder un g√©n√©rateur de phrases √† partir de trigrammes.

On va reprendre les m√™mes id√©es qu'avant, cette fois sans tomber dans les pi√®ges, √ßa devrait aller
plus vite‚ÄØ! La seule diff√©rence, c'est qu'au lieu de compter les bigrammes on compte les trigrammes
afin de pouvoir choisir chaque mot en fonction des deux pr√©c√©dents.

Un petit changement suppl√©mentaire pour choisir le premier mot‚ÄØ: au lieu d'un seul marqueur de d√©but
de phrase `<s>`, on va maintenant devoir en mettre deux `<s> <s>`. Par contre, il suffit toujours
d'un seul `</s>`, vous suivez‚ÄØ?

```python
def extract_trigrams(words):
    res = []
    for i in range(len(words)-2):
        res.append((words[i], words[i+1], words[i+2]))
    return res

assert extract_trigrams(['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']) == [
    ('je', 'reconnais', "l'",),
    ('reconnais', "l'", "existence"),
    ("l'", 'existence', "du"),
    ('existence', 'du', "kiwi-fruit"),
]
```

```python
def read_corpus_for_trigrams(file_path):
    unigrams = Counter()
    trigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            words = ["<s>", "<s>"] + words
            words.append("</s>")
            unigrams.update(words)
            trigrams.update(extract_trigrams(words))
    
    return unigrams, trigrams
```


```python
def get_trigram_probs(unigram_counts, trigram_counts):
    probs = defaultdict(dict)
    for (w_1, w_2, w_3), c in trigram_counts.items():
        probs[(w_1, w_2)][w_3] = c/unigram_counts[w_3]

    return dict(probs)
```

```python
def generate_from_trigrams(trigram_probs):
    sent = ["<s>", "<s>"]
    while sent[-1] != "</s>":
        candidates = list(trigram_probs[(sent[-2], sent[-1])].keys())
        weights = [trigram_probs[(sent[-2], sent[-1])][c] for c in candidates]
        sent.append(random.choices(candidates, weights)[0])
    return sent
```

Et pour tester

```python
unigram_counts, trigram_counts = read_corpus_for_trigrams("data/zola_ventre-de-paris.txt")
trigram_probs = get_trigram_probs(unigram_counts, trigram_counts)
```

```python
print(" ".join(generate_from_trigrams(trigram_probs)[2:-1]))
```

### üá≥ N-grammes üá≥

Toujours la m√™me chose, simplement il va falloir r√©fl√©chir un peu pour g√©n√©raliser‚ÄØ:

```python
def extract_ngrams(words, n):
    res = []
    for i in range(len(words)-n+1):
        # Tuple pour pouvoir s'en servir comme cl√© de dictionnaire et donc OK avec `Counter`
        res.append(tuple(words[i:i+n]))
    return res
```

```python
def read_corpus_for_ngrams(file_path, n):
    unigrams = Counter()
    ngrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            # Il nous faut bien `n-1` symboles de d√©but de phrase 
            words = ["<s>"]*(n-1) + words
            words.append("</s>")
            unigrams.update(words)
            ngrams.update(extract_ngrams(words, n))
    
    return unigrams, ngrams
```


```python
def get_ngram_probs(unigram_counts, ngram_counts):
    probs = defaultdict(dict)
    for ngram, c in ngram_counts.items():
        probs[tuple(ngram[:-1])][ngram[-1]] = c/unigram_counts[ngram[-1]]

    return dict(probs)
```

On peut aussi √©crire √ßa comme ceci avec un
[*unpacking*](https://stackabuse.com/unpacking-in-python-beyond-parallel-assignment/) (voir aussi
[la doc](https://docs.python.org/3/reference/expressions.html#expression-lists) pour la syntaxe
abstraite et la [PEP 448](https://peps.python.org/pep-0448/) qui l'a introduite)

```python
def get_ngram_probs(unigram_counts, ngram_counts):
    probs = defaultdict(dict)
    for ngram, c in ngram_counts.items():
        *previous_words, target_word = ngram
        probs[tuple(previous_words)][target_word] = c/unigram_counts[target_word]

    return dict(probs)
```

voire m√™me

```python
def get_ngram_probs(unigram_counts, ngram_counts):
    probs = defaultdict(dict)
    for (*previous_words, target_word), c in ngram_counts.items():
        probs[tuple(previous_words)][target_word] = c/unigram_counts[target_word]

    return dict(probs)
```

```python
def generate_from_ngrams(ngram_probs, n):
    # On pourrait deviner `n` √† partir de `ngram_probs‚Ä¶
    sent = ["<s>"] * (n-1)
    while sent[-1] != "</s>":
        # Essayer de bien r√©fl√©chir pour comprendre le `1-n`
        previous_words = tuple(sent[1-n:])
        candidates = list(ngram_probs[previous_words].keys())
        weights = [ngram_probs[previous_words][c] for c in candidates]
        sent.append(random.choices(candidates, weights)[0])
    return sent
```

Et pour tester

```python
n = 5
unigram_counts, ngram_counts = read_corpus_for_ngrams("data/zola_ventre-de-paris.txt", n)
ngram_probs = get_ngram_probs(unigram_counts, ngram_counts)
```

```python
print(" ".join(generate_from_ngrams(ngram_probs, n)[n-1:-1]))
```
