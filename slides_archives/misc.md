\begin{equation}
    p_s = \frac{\text{nombre d'occurrences de la phrase dans le corpus}}{\text{taille du corpus}}
\end{equation}

Et on aurait alors un mod√®le de langue parfait.


Dans ce cas, ce serait facile d'√©valuer notre mod√®le de langue imparfait √† nous‚ÄØ: imaginons que pour
cette m√™me phrase, notre mod√®le donne une vraisemblance $\hat{p}_s$, on pourrait par exemple lui
donner un score $m_s$ pour cette phrase en calculant

\begin{equation}
    M_s = d(p_s, \hat{p}_s) = \left|p_s-\hat{p}_s\right|
\end{equation}


Apart√© important‚ÄØ: les barres verticales notent ici la [valeur absolue](https://fr.wikipedia.org/wiki/Valeur_absolue)‚ÄØ:

\begin{equation}
    \left|x\right| =
        \begin{cases}
            x &\text{si x ‚©æ 0}\\
            -x &\text{sinon}
        \end{cases}
\end{equation}

par exemple $\left|2713\right|=2713$ et $\left|-2713\right|=2713$.

En Python‚ÄØ:

```python
abs(-2713)
```

C'est un outil pour rendre un truc positif, mais la propri√©t√© qui nous int√©resse ici, c'est que la
valeur absolue d'une diff√©rence, c'est une
[*distance*](https://fr.wikipedia.org/wiki/Distance_(math%C3%A9matiques)) (au sens math√©matique du
terme). Un truc qui dit si deux choses sont loin l'une de l'autre. Ici $M_s$, on peut l'interpr√©ter
comme la *distance* entre $p_s$ et $\hat{p}_s$.


(C'est pour √ßa que j'ai √©crit $d(p_s, \hat{p}_s)$‚ÄØ!)


Bon, donc, toujours dans ce monde id√©al, on aurait alors un outil qui nous donnerait la qualit√© de
notre mod√®le pour une phrase. C'est pas tr√®s difficile de voir comment en d√©duire un score global‚ÄØ:
on a qu'√† faire la moyenne sur toutes les phrases possibles‚ÄØ! Si on imagine qu'il y a $n$ phrases
possibles dans la langue qu'on √©tudie (un tr√®s tr√®s grand $n$ donc) et qu'on les appelle $s_1$,
$s_2$, ‚Ä¶, $s_n$, le score global $M$ de notre mod√®le, on peut par exemple d√©cider que ce serait la
moyenne‚ÄØ:


\begin{equation}
    m = \frac{M_{s_1} + M_{s_2} + ‚Ä¶ + M_{s_n}}{n}
\end{equation}

<!-- #region slideshow={"slide_type": "slide"} -->
## üëú Exo‚ÄØ: les sacs de mots üëú
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
![](bow.png)

<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
### 1. Faire des sacs

- √âcrire un script (un fichier `.py`, quoi) qui prend en unique argument de ligne de commande un
  dossier contenant des documents (sous forme de fichier textes) et sort un fichier TSV donnant pour
  chaque document sa repr√©sentation en sac de mots (en nombre d'occurrences des mots du vocabulaire
  commun)
  - Un fichier par ligne, un mot par colonne
  - Pour it√©rer sur les fichiers dans un dossier on peut utiliser `for f in
    pathlib.Path(chemin_du_dossier).glob('*'):` avec le module
    [`pathlib`](https://docs.python.org/3/library/pathlib.html#module-pathlib) (il y a d'autres
    solutions‚Ä¶).
  - Pour r√©cup√©rer des arguments en ligne de commande‚ÄØ: [`argparse` ou
    `sys.argv`](https://docs.python.org/3/tutorial/stdlib.html#command-line-arguments)
  - Pour √©crire un `array` dans un fichier TSV‚ÄØ:
    [`np.savetxt`](https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html)
- Tester sur la partie positive du [mini-corpus imdb](../../data/imdb_smol.tar.gz)

Pensez √† ce qu'on a vu les cours pr√©c√©dents pour ne pas r√©inventer la roue. Par exemple vous savez
tokenizer.
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
### 2. Faire des sacs relatifs

Modifier le script pr√©c√©dent pour qu'il g√©n√®re des sacs de mots utilisant les fr√©quences relatives
plut√¥t que les fr√©quences absolues
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
### 3. Faire des tfidsacs
<!-- #endregion -->

Modifier le script de pr√©c√©dent pour qu'il renvoie non plus les fr√©quences relatives de chaque mot,
mais leur tf‚ãÖidf avec la d√©finition suivante pour un mot $w$, un document $D$ et un corpus $C$

- $\mathrm{tf}(w, D)$ est la fr√©quence relative de $w$ dans $D$
- $$\mathrm{idf}(w, C) = \log\!\left(\frac{\text{nombre de documents dans $C$}}{\text{nombre de
  documents de $C$ qui contiennent $w$}}\right)$$
- $\log$ est le logarithme naturel
  [`np.log`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)
- $\mathrm{tfidf}(w, D, C) = \mathrm{tf}(w, D)√ó\mathrm{idf}(w, C)$

Pistes de recherche‚ÄØ:

- L'option `keepdims` de `np.sum`
- `np.transpose`
- `np.count_nonzero`
- Regarder ce que donne `np.array([[1, 0], [2, 0]]) > 0`
