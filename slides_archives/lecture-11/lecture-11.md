---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 11‚ÄØ: Repr√©sentations lexicales vectorielles
=================================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-27
<!-- #endregion -->

```python
from IPython.display import display
```

## Repr√©sentaquoi‚ÄØ?

**Repr√©sentations lexicales vectorielles**, ou en √©tant moins p√©dant‚ãÖe ¬´‚ÄØrepr√©sentations
vectorielles de mots‚ÄØ¬ª. Comment on repr√©sente des mots par des vecteurs, quoi.


Mais qui voudrait faire √ßa, et pourquoi‚ÄØ?


Tout le monde, et pour plein de raisons


On va commencer par utiliser [`gensim`](https://radimrehurek.com/gensim), qui nous fournit plein de
mod√®les tout faits.

```python
%pip install -U gensim
```

et pour d√©marrer, on va t√©l√©charger un mod√®le tout fait

```python
import gensim.downloader as api
wv = api.load("glove-wiki-gigaword-50")
```

OK, super, qu'est-ce qu'on a r√©cup√©r√©‚ÄØ?

```python
type(wv)
```

C'est le bon moment pour aller voir [la
doc](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors).
On y voit qu'il s'agit d'un objet associant des mots √† des vecteurs.

```python
wv["monarch"]
```

Des vecteurs stock√©s comment‚ÄØ?

```python
type(wv["monarch"])
```

Ah parfait, on conna√Æt : c'est des tableaux numpy

```python
wv["king"]
```

```python
wv["queen"]
```

D'accord, tr√®s bien, on peut faire quoi avec √ßa‚ÄØ?


Si les vecteurs sont bien faits (et ceux-ci le sont), les vecteurs de deux mots ¬´‚ÄØproches‚ÄØ¬ª
devraient √™tre proches, par exemple au sens de la similarit√© cosinus

```python
import numpy as np
def cosine_similarity(x, y):
    """Le cosinus de l'angle entre `x` et `y`."""
    return np.inner(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))
```

```python
cosine_similarity(wv["monarch"], wv["king"])
```

```python
cosine_similarity(wv["monarch"], wv["cat"])
```

En fait le mod√®le nous donne directement les mots les plus proches en similarit√© cosinus.

```python
wv.most_similar(["monarch"])
```

Mais aussi les plus √©loign√©s

```python
wv.most_similar(negative=["monarch"])
```

### üß® Exo üß®

1\. Essayez avec d'autres mots. Quels semblent √™tre les crit√®res qui font que des mots sont proches
dans ce mod√®le.

2\. Comparer avec les vecteurs du mod√®le `"glove-twitter-100"`. Y a-t-il des diff√©rences‚ÄØ?
**Note**‚ÄØ: il peut √™tre long √† t√©l√©charger, commencez par √ßa.

3\. Entra√Æner un mod√®le [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html) avec
gensim sur les documents du dataset 20newsgroup. Comparer les vecteurs obtenus avec les pr√©c√©dents.

## S√©mantique lexicale distributionnelle

### Principe g√©n√©ral

Pour le dire vite‚ÄØ:

La *s√©mantique lexicale*, c'est l'√©tude du sens des mots. Rien que dire √ßa, c'est d√©j√† faire
l'hypoth√®se hautement non-triviale que les mots ont un (ou plus vraisemblablement des) sens.

C'est tout un pan de la linguistique et on ne rentrera pas ici dans les d√©tails (m√™mes s'il sont
passionnants‚ÄØ!) parce que notre objectif est *applicatif*‚ÄØ:

- Comment repr√©senter le sens d'un mot‚ÄØ?
- Peut-on, √† partir de donn√©es linguistiques, d√©terminer le sens des mots‚ÄØ?
- Et plus tard‚ÄØ: comment on peut s'en servir‚ÄØ?

Une fa√ßon de traiter le probl√®me, c'est de recourir √† de l'annotation manuelle (par exemple avec
[Jeux de mots](http://www.jeuxdemots.org), d'ailleurs, vous avez jou√© r√©cemment‚ÄØ?).

On ne se penchera pas plus dessus ici‚ÄØ: ce qui nous int√©resse, c'est comment traiter ce probl√®me
avec de l'apprentissage, et en particulier avec de l'apprentissage sur des donn√©es non-annot√©es.

Pour √ßa, la fa√ßon la plus populaire (et pour l'instant celle qui semble la plus efficace) repose sur
l'**hypoth√®se distributionnelle**, formul√©e ainsi par Firth

> You shall know a word by the company it keeps.

Autrement dit‚ÄØ: des mots dont le sens est similaire devraient appara√Ætre dans des contextes
similaires et vice-versa.


Si on pousse cette hypoth√®se √† sa conclusion naturelle‚ÄØ: on peut repr√©senter le sens d'un mot par
les contextes dans lesquels il appara√Æt.

Le principal d√©faut de cette vision des choses, c'est que ce n'est pas forc√©ment tr√®s interpr√©table,
contrairement par exemple √† des repr√©sentations en logique formelle. Mais √ßa nous donne des moyens
tr√®s concrets d'apprendre des repr√©sentations de mots √† partir de corpus non-annot√©s.

### Mod√®le par documents

Par exemple une fa√ßon tr√®s simple de l'appliquer, c'est de regarder dans quels documents d'un grand
corpus appara√Æt un mot‚ÄØ: des mots qui apparaissent dans les m√™mes documents avec des fr√©quences
similaires devraient avoir des sens proches.

Qu'est-ce que √ßa donne en pratique‚ÄØ? Et bien souvenez-vous du mod√®le des sacs de mots‚ÄØ: on peut
repr√©senter des documents par les fr√©quences des mots qui y apparaissent. √áa nous donne une
repr√©sentation vectorielle d'un corpus sous la forme d'une matrice avec autant de ligne que de
documents, autant de lignes que de mots dans le vocabulaire et o√π chaque cellule est une fr√©quence.

Jusque-l√† on s'en est servi en lisant les lignes pour r√©cup√©rer des repr√©sentations vectorielles des
documents, mais si on regarde les colonnes, on r√©cup√®re des **repr√©sentations vectorielles des
mots**‚ÄØ!

(Ce qui r√©pond √† la premi√®re question‚ÄØ: comment repr√©senter le sens‚ÄØ? Comme le reste, avec des
vecteurs‚ÄØ!)

### üê¢ Exo üê¢

√Ä partir du corpus 20newsgroup, construire un dictionnaire associant chaque mot du vocabulaire √† une
repr√©sentation vectorielle donnant ses occurrences dans chacun des documents du corpus.

**N'h√©sitez pas √† recycler du code**

Est-ce que les distances entre les vecteurs de mots ressemblent √† celles qu'on observait avec Gensim‚ÄØ?

Est-ce que vous voyez une autre fa√ßon de r√©cup√©rer des vecteurs de mots en utilisant ce corpus‚ÄØ?


### Cooccurrences

Une autre possibilit√©, plut√¥t que de regarder dans quels documents appara√Æt un mot, c'est de regarder directement les autres mots dans son voisinage. Autrement dit les cooccurrences.

L'id√©e est la suivante‚ÄØ: on choisit un param√®tre $n$ (la ¬´‚ÄØtaille de fen√™tre‚ÄØ¬ª) et on regarde pour
chaque mot du corpus les $n$ mots pr√©c√©dents et les $n$ mots suivants. Chacun de ces mots voisins
constitue une cooccurrence. Par exemple avec une fen√™tre de taille $2$, dans 

> Le petit chat est content

On a les cooccurrences `("le", "petit")`, `("le", "chat")`, `("petit", "chat")`, `("petit", "est")`‚Ä¶

Comment on se sert de √ßa pour r√©cup√©rer une repr√©sentation vectorielle des mots‚ÄØ? Comme
d'habitude‚ÄØ: on compte‚ÄØ! Ici on repr√©sentera chaque mot par un vecteur avec autant de coordonn√©es
qu'il y a de mots dans le vocabulaire, et chacune de ces coordonn√©es sera le nombre de cooccurrences
avec le mot correspondant.


### ü¶ò Exo ü¶ò

√Ä partir du corpus 20newsgroup, construire un dictionnaire associant chaque mot du vocabulaire √† une
repr√©sentation vectorielle par la m√©thode des cooccurrences pour une taille de fen√™tre choisie.

Est-ce que les distances entre les vecteurs de mots ressemblent √† celles qu'on observait avec les
repr√©sentations pr√©c√©dentes‚ÄØ?

## Extensions

Le d√©faut principal de ces repr√©sentations, c'est qu'elles sont tr√®s **creuses**‚ÄØ: beaucoup de
dimensions, mais qui contiennent surtout des z√©ros. Ce n'est pas tr√®s √©conomique √† manipuler et
c'est moins utile quand on veut les utiliser comme entr√©e pour des syst√®mes de TAL, comme des r√©seaux
de neurones

L'essentiel du travail fait ces dix derni√®res ann√©es dans ce domaine consiste √† trouver des
repr√©sentations **denses**‚ÄØ: moins de dimensions (au plus quelques centaines) mais peu de z√©ros. ON
parle alors en fran√ßais de *plongements* et en anglais de *word embeddings*.

Il y a beaucoup de fa√ßons de faire √ßa, *Speech and Language Processing* d√©taille la plus connue,
*word2vec* et je vous encourage √† aller voir comment √ßa marche.

Une autre possibilit√© d'extensions est de descendre en dessous de l'√©chelle du mot, et d'utiliser
des sous-mots, qui peuvent √©ventuellement avoir un sens linguistique (comme des morph√®mes), mais
sont eux aussi en g√©n√©ral appris de fa√ßon non-supervis√©e. C'est ce que fait
[FastText](https://fasttext.cc/docs/en/python-module.html), qui est plus ou moins ce qui se fait de
mieux en termes de repr√©sentations vectorielles de mots.

## üëΩ Exo üëΩ

(Pour les plus motiv√©‚ãÖe‚ãÖs, mais la doc vous dit d√©j√† presque tout)

1\. Entra√Æner un mod√®le non-supervis√© [`FastText`](https://fasttext.cc/docs/en/python-module.html)
sur 20 newsgroups et voir si les similarit√©s sont les m√™mes que pour les mod√®les pr√©c√©dents.

2\. Entra√Æner et tester un mod√®le de classification FastText sur 20 newsgroup.