---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

ModÃ¨les de langues Ã  n-grammesâ€¯: corrections
============================================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2022-09-28

## âœ‚ï¸ Tokenization âœ‚ï¸

1\. Ã‰crire une fonction `crude_tokenizer` qui prend comme argument une chaine de caractÃ¨res et
    renvoie la liste des mots de cette chaÃ®ne en sÃ©parant sur les espaces.

```python
def crude_tokenizer(s):
    return s.split()

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', "l'existence", 'du', 'kiwi-fruit.'
]
```

2\. Modifier la fonction `crude_tokenizer` pour qu'elle sÃ©pare aussi suivant les caractÃ¨res
   non alphanumÃ©riques. **Indice** Ã§a peut Ãªtre utile de revoir [la doc sur les expressions
   rÃ©guliÃ¨res](https://docs.python.org/3/library/re.html) ou de relire [un tuto Ã  ce
   sujet](https://realpython.com/regex-python/).

```python
import re
def crude_tokenizer(s):
    return [w for w in re.split(r"\s|\W", s.strip()) if w]

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', 'l', 'existence', 'du', 'kiwi', 'fruit'
]
```

3\. On aimerait maintenant garder les apostrophes Ã  la fin du mot qui les prÃ©cÃ¨de, ainsi que les
mots composÃ©s ensemble.

```python
import re  # Si jamais on a pas exÃ©cutÃ© la cellule prÃ©cÃ©dente
def crude_tokenizer(s):
    return re.findall(r"\b\w+?\b(?:'|(?:-\w+?\b)*)?", s)

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit'
]
```

4\. Ã‰crire une fonction `crude_tokenizer_and_normalizer` qui en plus de tokenizer comme prÃ©cÃ©demment
met tous les mots en minuscules

On peut Ã©videmment copier-coller le code au-dessus, mais on peut aussi rÃ©utiliser ce qu'on a dÃ©jÃ  dÃ©finiâ€¯:

```python
def crude_tokenizer_and_normalizer(s):
    return crude_tokenizer(s.lower())

asser = crude_tokenizer_and_normalizer("Je reconnais l'existence du kiwi-fruit.") == [
    'je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit'
]
```

## ğŸ’œ Extraire les bigrammes ğŸ’œ

Ã‰crire une fonction `extract_bigrams` qui prend en entrÃ©e une liste de mots et renvoie la liste des bigrammes correspondants sous forme de couples de mots.


Version directe

```python
def extract_bigrams(words):
    res = []
    for i in range(len(words)-1):
        res.append((words[i], words[i+1]))
    return res

assert extract_bigrams(['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']) == [
    ('je', 'reconnais'),
     ('reconnais', "l'"),
     ("l'", 'existence'),
     ('existence', 'du'),
     ('du', 'kiwi-fruit')
]
```

Version artistique

```python
def extract_bigrams(words):
    return list(zip(words[:-1], words[1:]))

assert extract_bigrams(['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']) == [
    ('je', 'reconnais'),
     ('reconnais', "l'"),
     ("l'", 'existence'),
     ('existence', 'du'),
     ('du', 'kiwi-fruit')
]
```

Si vous trouvez Ã§a obscur essayez le code ci-dessous, et allez voir ce qu'il donne [sur Python Tutor](https://pythontutor.com/render.html#code=tokenized%20%3D%20%5B'Je',%20'reconnais',%20%22l'%22,%20'existence',%20'du',%20'kiwi-fruit'%5D%0A%0Afirst_words%20%3D%20tokenized%5B%3A-1%5D%0Asecond_words%20%3D%20tokenized%5B1%3A%5D%0A%0Afor%20t%20in%20zip%28first_words,%20second_words%29%3A%0A%20%20%20%20print%28t%29&cumulative=false&curInstr=10&heapPrimitives=nevernest&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false)

```python
tokenized = ['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']

first_words = tokenized[:-1]
second_words = tokenized[1:]

print(first_words)
print(second_words)

for t in zip(first_words, second_words):
    print(t)
```


## ğŸ”¢ Compter ğŸ”¢


Ã‰crire une fonction `read_corpus` qui prend en argument un chemin vers un fichier texte, l'ouvre, le
tokenize et y compte les unigrammes et les bigrammes en renvoyant deux `Counter` associant
respectivement Ã  chaque mot et Ã  chaque bigramme leurs nombres d'occurrences.

```python
from collections import Counter
    
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            unigrams.update(words)
            bigrams.update(extract_bigrams(words))
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('de', 5292), ('la', 3565), ('les', 2746), ('il', 2443)]
assert bigram_counts.most_common(4) == [
    (('de', 'la'), 754),
     (("qu'", 'il'), 424),
     (('Ã ', 'la'), 336),
     (("d'", 'une'), 321)
]
```

## ğŸ¤“ Estimer les probas ğŸ¤“


On va ensuite estimer les probabilitÃ©s de transition, c'est-Ã -dire la probabilitÃ© de gÃ©nÃ©rer un
certain mot $w_1$ sachant que le mot prÃ©cÃ©dent est $w_0$. On le fait en utilisant la formule du
maximum de vraisemblanceâ€¯:

\begin{equation}
   P(w_1|w_0) := P\!\left([w_0, w_1]~|~[w_0, *]\right) = \frac{\text{nombre d'occurrences du bigramme $w_0 w_1$}}{\text{nombre d'occurrences de l'unigramme $w_0$}}
\end{equation}

Pour que ce soit plus agrÃ©able Ã  sampler on va utiliser un dictionnaire de dictionnairesâ€¯:
`probs[v][w]` stockera $P(w|v)$.

Ã€ vous de jouerâ€¯: Ã©crire une fonction `get_probs`, qui prend en entrÃ©e la les compteurs de bigrammes
et d'unigrammes et renvoie le dictionnaire `probs`

```python
def get_probs(unigram_counts, bigram_counts):
    probs = dict()
    for (v, w), c in bigram_counts.items():
        if v not in probs:
            # Si on a pas encore rencontrÃ© de bigrammes commenÃ§ant par `v`, il faut
            # commencer par crÃ©er `probs[v]`
            probs[v] = dict()
        probs[v][w] = c/unigram_counts[v]
    return probs

probs = get_probs(unigram_counts, bigram_counts)
assert probs["je"]["dÃ©jeune"] == 0.002232142857142857
```

Avec `collections.defaultdict`â€¯:

```python
from collections import defaultdict

def get_probs(unigram_counts, bigram_counts):
    # Un dictionnaire de dictionnaires crÃ©Ã©s automatiquement Ã  l'accÃ¨s,
    # Ã‡a Ã©vite de faire un test et Ã§a rend souvent le code plus lisible
    probs = defaultdict(dict)
    for (v, w), c in bigram_counts.items():
        probs[v][w] = c/unigram_counts[v]

    # Pour ne pas masquer des erreurs pendant le sampling, on en refait un dict normal
    return dict(probs)

probs = get_probs(unigram_counts, bigram_counts)
assert probs["je"]["dÃ©jeune"] == 0.002232142857142857
```

## ğŸ¤” GÃ©nÃ©rer ğŸ¤”

Pour l'instant on ne va pas se prÃ©occuper de sauvegarder le modÃ¨le on va l'utiliser directement pour
sampler. Le principe est simpleâ€¯: on choisit le premier mot, puis on choisit le deuxiÃ¨me mot en
prenant en compte celui qu'on vient de gÃ©nÃ©rer (le premier donc si vous suivez) et ainsi de suite.


**Questions**

- Comment on choisit le premier motâ€¯?
- Et quand est-ce qu'on dÃ©cide de s'arrÃªterâ€¯?


Jurafsky et Martin nous disent

>  Weâ€™ll first need to augment each sentence with a special symbol `<s>` at the beginning of the
> sentence, to give us the bigram context of the first word. Weâ€™ll also need a special end-symbol.
> `</s>`

Heureusement on a un fichier bien faitâ€¯: il y a une seule phrase par ligne.


1\. Modifier `read_corpus` pour ajouter ajouter Ã  la volÃ©e `<s>` au dÃ©but de chaque ligne et `</s>` Ã  la fin de chaque ligne.

```python
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            words.insert(0, "<s>")
            words.append("</s>")
            unigrams.update(words)
            bigrams.update(extract_bigrams(words))
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('<s>', 8945), ('</s>', 8945), ('de', 5292), ('la', 3565)]
assert bigram_counts.most_common(4) == [
    (('<s>', '</s>'), 1811),
    (('<s>', 'il'), 775),
    (('de', 'la'), 754),
    (('<s>', 'elle'), 576)
]
```

Il y a encore un petit problÃ¨me

```python
bigram_counts.most_common(1)
```

ğŸ¤”


On a comptÃ© les lignes vides ğŸ˜¤. Ã‡a ne posait pas de problÃ¨me jusque-lÃ  puisque Ã§a n'ajoutait rien
aux compteurs de n-grammes, mais maintenant Ã§a nous fait des `["<s>", "</s>"]`.


2\. Modifier `read_corpus` pour ignorer les lignes vides

```python
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            if line.isspace():
                continue
            words = crude_tokenizer_and_normalizer(line.strip())
            words.insert(0, "<s>")
            words.append("</s>")
            unigrams.update(words)
            bigrams.update(extract_bigrams(words))
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('<s>', 7145), ('</s>', 7145), ('de', 5292), ('la', 3565)]
assert bigram_counts.most_common(4) == [
    (('<s>', 'il'), 775),
    (('de', 'la'), 754),
    (('<s>', 'elle'), 576),
    (("qu'", 'il'), 424)
]

probs = get_probs(unigram_counts, bigram_counts)
assert probs["<s>"]["le"] == 0.0298110566829951
```

## ğŸ˜Œ GÃ©nÃ©rer pour de vrai ğŸ˜Œ

**Bon c'est bon maintenantâ€¯?**


Ã€ peu prÃ¨s. On va pouvoir sampler.


Pour Ã§a on va piocher dans le module [`random`](https://docs.python.org/3/library/random.html) de la
bibliothÃ¨que standard, et en particulier la fonction
[`random.choices`](https://docs.python.org/3/library/random.html#random.choices) qui permet de tirer
au sort dans une population finie en prÃ©cisant les probabilitÃ©s de chacun de Ã©lÃ©ments. Le poids
n'ont en principe pas besoin d'Ãªtre normalisÃ©s (mais ils le seront ici, Ã©videmment).

```python
import random
```

Voici par exemple comment choisir un mot qui suivrait Â«â€¯jeâ€¯Â»â€¯:

```python
# Les candidats mots qui peuvent suivre Â«â€¯jeâ€¯Â»
candidates = list(probs["je"].keys())
# Leurs poids, ce sont les probabilitÃ©s qu'on a dÃ©jÃ  calculÃ©
weights = [probs["je"][c] for c in candidates] 
random.choices(candidates, weights, k=1)[0]  # Attention `choices` renvoit une liste
```

Ã‰crire une fonction `sample` qui prend en argument les probabilitÃ©s de bigrammes (sous la forme d'un dictionnaire de dictionnaires comme notre `prob`) et gÃ©nÃ¨re une phrase en partant de `<s>` et en ajoutant des mots itÃ©rativement, s'arrÃªtant quand `</s>` a Ã©tÃ© choisi.

```python
def sample(bigram_probs):
    sent = ["<s>"]
    while sent[-1] != "</s>":
        candidates = list(probs[sent[-1]].keys())
        weights = [probs[sent[-1]][c] for c in candidates]
        sent.append(random.choices(candidates, weights)[0])
    return sent
```

Pas de assert ici comme on a de l'alÃ©atoire, mais la cellule suivante permet de tester si Ã§a marche

```python
print(sample(probs))
print(" ".join(sample(probs)[1:-1]))
```

C'est rigolo, heinâ€¯?


Qu'est-ce que vous pensez des textes qu'on gÃ©nÃ¨reâ€¯?

## ğŸ§ Aller plus loin ğŸ§


En vous inspirant de ce qui a Ã©tÃ© fait, coder un gÃ©nÃ©rateur de phrases Ã  partir de trigrammes,
tÃ©tragrammes (4), puis de n-grammes arbitraires.

