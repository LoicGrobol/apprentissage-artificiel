---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.18.1
  kernelspec:
    display_name: cours-ml
    language: python
    name: python3
---

Apprentissage non-supervis√©
===========================

**L. Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

Ce TP est en grande partie inspir√© d'√©l√©ments de la documentation de scikit-learn et umap-learn.
Vous devriez allez les lire en d√©tails pour en savoir plus.


Avant de commencer, allez jouer un peu avec [une version interactive de
K-Means](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) (ou cherchez en
d'autres‚ÄØ!) pour se rafra√Æchir les id√©es.


Le sujet est vraiment vaste et on ne va √©videmment pas √™tre exhaustifves, mais essayons de donner
quelques id√©es. D'abord quelques outils.

```python
import numpy as np
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_digits, load_iris, make_blobs
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP
```

Voil√† une fa√ßon de g√©n√©rer et afficher des donn√©es al√©atoires en 2d.

```python
# On r√©cup√®re un g√©n√©rateur de nombres pseudo-al√©atoires, en fixant la seed pour que les r√©sultats
# soient reproductibles (si vous voulez un autre √©chantillon, changez la seed)
gen = np.random.default_rng(0)
# On g√©n√®re un array de flottants al√©atoires compris entre -8.0 et 8.0 distribu√©s uniform√©ment, de
# taille 1024√ó2 (autrement dit l'√©quivalent de 1024 points). Oui j'ai une obsession avec les
# puissances de 2, demandez-moi pourquoi 10 minutes avant la fin du cours
points = gen.uniform(-8.0, 8.0, size=(1024, 2))
# On r√©cup√®re des objets pour dessiner avec pyplots. Dans ce cas simple on pourrait s'en passer,
# mais √ßa sera une habitude utile quand on voudra faire des graphiques plus complexes.
fig, ax = plt.subplots()
# Le plot proprement dit. Le param√®tre marker est un moyen rapide d'avoir des petits points.
ax.scatter(x=points[:, 0], y=points[:, 1], marker=".")
fig.show()
```

Notez que dans la vie, pour faire des jolis graphiques s√©rieusement, il vaut mieux utiliser quelque
chose comme [plotnine](https://plotnine.org/), mais ici on va rester sur pyplot, qui est un peu
rudimentaire mais va nous suffire.


Maintenant si on a un tableau qui associe une classe √† chaque point, on peut les visualiser avec des
couleurs comme √ßa (attention, il faut que les entr√©es soient des entiers)

```python
gen = np.random.default_rng(0)

points = gen.uniform(-8.0, 8.0, size=(1024, 2))

# Ici on leur donne al√©atoirement une classe, soit 0 soit 1
clusters = gen.integers(0, 2, size=(1024,))

fig, ax = plt.subplots()
ax.scatter(x=points[:, 0], y=points[:, 1], c=clusters, marker=".")
fig.show()

```

Si vous trouvez les couleurs moches, allez voir [la
doc](https://matplotlib.org/stable/gallery/color/colormap_reference.html). Notez que les points sont
au m√™me endroit que sur le graphique pr√©c√©dent‚ÄØ: normal, on a la m√™me seed :)


scikit-learn a aussi des fonctions pour g√©n√©rer des points avec des propri√©t√©s choisies, comme
[`make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)‚ÄØ:

```python
# On lui passe directement le nombre de point, la dimension, une seed, le nombre de blobs qu'on veut
# faire.
points, _y = make_blobs(n_samples=1024, n_features=2, centers=3, random_state=0)

fig, ax = plt.subplots()
ax.scatter(x=points[:, 0], y=points[:, 1], marker=".")
fig.show()
```

**Entra√Ænement**‚ÄØ: allez lire la doc de de `make_blobs` pour comprendre comment elle marche, puis
modifiez la cellule ci-dessous pour que les blobs soient colori√©s, et que leurs centres soient
g√©n√©r√©s dans la m√™me *bounding box* que nos points al√©atoires du d√©but.

Au fait, pourquoi j'avais mis un underscore dans `_y`‚ÄØ?

```python
points, _y = make_blobs(n_samples=1024, n_features=2, centers=3, random_state=0)

fig, ax = plt.subplots()
ax.scatter(x=points[:, 0], y=points[:, 1], marker=".")
fig.show()

```

Au fait, pourquoi on fait des jeux de donn√©es al√©atoires (une forme de donn√©es *synth√©tiques*)‚ÄØ?
Parce qu'en g√©n√©ral les jeux de donn√©es r√©els ont plus de deux dimensions (m√™me le tout petit
[Iris](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) en a quatre) et c'est
donc plus difficile √† visualiser directement. On s'oocupera de leurs cas plus tard.

## Zoologie des algos de clustering


Comme toujours, on va commencer par un peu de zoologie‚ÄØ: on va regarder quelques algos de clustering
bien connus sur des donn√©es qu'on ma√Ætrise, pour voir ce qui se passe. Voici par exemple comment
trouver des clusters avec K-Means sur un dataset de blobs‚ÄØ:

```python
points, _y = make_blobs(n_samples=1024, n_features=2, centers=3, random_state=0)

# On cr√©√©e l'estimateur
kmeans = KMeans(n_clusters=3, random_state=0)
# On le fit sur nos donn√©es
clusters = kmeans.fit_predict(points)

fig, ax = plt.subplots()
ax.scatter(x=points[:, 0], y=points[:, 1], c=clusters, marker=".")
fig.show()

```

√áa a l'air assez coh√©rent, est-ce que √ßa colle √† nos donn√©es‚ÄØ? On peut v√©rifier en
[affichant](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#visualize-the-results-on-pca-reduced-data)
les blobs originaux et les centro√Ødes par dessus le diagramme de Voronoi‚ÄØ:

```python
points, blobs = make_blobs(n_samples=1024, n_features=2, centers=3, random_state=0)

kmeans = KMeans(n_clusters=3, random_state=0)
clusters = kmeans.fit_predict(points)

# Un peu de travail pour afficher le diagramme de Voronoi.
# Step size of the mesh. Decrease to increase the quality of the VQ.
h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].
# Plot the decision boundary. For that, we will assign a color to each
x_min, x_max = points[:, 0].min() - 1, points[:, 0].max() + 1
y_min, y_max = points[:, 1].min() - 1, points[:, 1].max() + 1
grid, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
# Obtain labels for each point in mesh. Use last trained model.
z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(grid.shape)

fig, ax = plt.subplots()
ax.imshow(
    z,
    interpolation="nearest",
    extent=(grid.min(), grid.max(), yy.min(), yy.max()),
    aspect="auto",
    origin="lower",
)

# Plot the original points with their original blob
ax.scatter(x=points[:, 0], y=points[:, 1], c=blobs, marker=".")
# Plot the centroids as a white X
centroids = kmeans.cluster_centers_
ax.scatter(
    centroids[:, 0],
    centroids[:, 1],
    marker="x",
    s=169,
    linewidths=3,
    color="w",
    zorder=10,
)
fig.show()

```

Est-ce que c'est un bon clustering du coup‚ÄØ? R√©flechissez √† la question et on en d√©bat toustes
ensemble tout √† l'heure?


‚ö†Ô∏è **Attention** un diagramme de Voronoi, √ßa n'a √©videmment de sens que pour k-means et ses d√©riv√©s
(et √©ventuellement des trucs comme BGMM), √ßa n'en a par exemple pas du tout pour du clustering
hi√©rarchique.


**Entra√Ænement** Testez quelques trucs‚ÄØ:

- Clusteriser avec un $k$ plus grand ou plus petit que le nombre de blobs.
- Clusteriser des points g√©n√©r√©s uniform√©ment (donc o√π il n'y a pas de blobs a priori et donc pas de
  raisons de trouver des clusters en principe)

Faites vos tests √† la suite de cette cellule, prenez des notes, appellez moi pour des questions,
gardez votre travail pr√©cieusement, √ßa vous servira plus tard.

### √Ä vous de jouer


Examinez les comportements des algos suivants‚ÄØ:

- [Clustering Agglom√©ratif](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering)
- [M√©langes de Gaussiennes Bay√©siennes](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html)
- BIRCH
- [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
  - [Y a une visu!](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

Testez-les sur

- Des blobs (en faisant varier les param√®tres).
- De points g√©n√©r√©s uniform√©ment.
- [Des cercles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)
- [Des lunes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.htm)

Toujours pareil‚ÄØ: prenez des notes, faites des graphiques, lisez la doc.

Une fois que vous avez fait √ßa s√©rieusement et seulement √† ce moment l√†, allez voir [la gallerie de
scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)
pour une vision d'ensemble (si vous commencez par √ßa, √ßa vous apportera pas grand chose).

## R√©duction de dimension

Comme on l'a dit pr√©c√©demment, les donn√©es du monde r√©el ont la f√¢cheuse habitude de ne pas √™tre
entre 3 dimensions, et encore moins en deux. M√™me des donn√©es jouets‚ÄØ:

- [Iris](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) ‚Üí **4**
- [Wine](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset)‚ÄØ‚Üí
  **13**
- [MNIST](https://en.wikipedia.org/wiki/MNIST_database) images des 28√ó28 pixels‚ÄØ‚Üí **784**
  - M√™me chose pour [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) √©videmment
- [20ng
  vectoris√©](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized)
  ‚Üí **130107**!
- ‚Ä¶

√áa pose deux probl√®mes¬†:

- √áa rend ces donn√©es impossibles √† visualiser‚ÄØ: m√™me si votre cerveau arrivait √† penser en n
  dimensions, il reste que vous vivez dans un monde √† 3 dimensions, avec des organes sensoriels √† 2
  dimensions, impossible de repr√©senter plus.
- D'un point de vue computationnel, pour n'importe quelle manipulation qu'on veut faire sur ces
  donn√©s (apprentissage ou pas, supervis√© ou pas)
  - Plus on a de dimensions, plus les calculs sont complexes.
  - Plus on a de dimensions, plus la stabilit√© des approximations qu'on fait est fragile, pour les
    nombreuses raisons qu'on appelle parfois [¬´‚ÄØfl√©au de la
    dimension‚ÄØ¬ª](https://en.wikipedia.org/wiki/Curse_of_dimensionality)

De plus, en g√©n√©ral, les dimensions de nos donn√©es ne sont en g√©n√©ral pas ind√©pendantes (imaginer
pourquoi sur chacun des exemples pr√©c√©dents). D'un point de vue math√©matique, le support de nos
donn√©es a souvent une dimension intrins√®que bien inf√©rieure √† celle de l'espace ambiant. En
cons√©quence, non seulement les hautes dimensions nous compliquent l'existence, mais souvent avec peu
de valeur ajout√©e.

‚ö†Ô∏è Ce n'est √©videmment pas vrai pour des repr√©sentations de donn√©es qui ont √©t√© explicitement pour
pr√©vu pour √ßa, par exemple les repr√©sentations vectorielles denses (ou *embeddings*) dont on
reparlera.

On va donc souvent pouvoir r√©duire ses probl√®mes avec des techniques de r√©duction de dimension, dont
on peut r√©sumer l'id√©e par‚ÄØ:

> √âtant donn√© une famille de vecteurs $E = (x_1, ‚Ä¶, x_N)$ de $‚Ñù^d$, on cherche une transformation
> $T:‚Ñù^d\longrightarrow ‚Ñù^{d'}$ avec $d'<d$ telle que $E'=(T(x_1), ‚Ä¶, T(x_n))$ pr√©serve au moins
> approximativement certaines des structures de $E$.

En g√©n√©ral les structure en question seront de m√™me nature celles sur lesquelles on s'appuie pour
le clustering‚ÄØ: spatiales, m√©triques, topologiques‚Ä¶ Pour cel√†, on r√©alise souvent une optimisation
(sous contraintes) sp√©cifique‚ÄØ: on cherche $T$ telle que la variance de $E'$ soit la plus proche de
celle de $E$ (ACP), que la distribution des similarit√©s des paires de $E$ soit la plus proche
possible de celle de $E'$ (t-SNE)‚Ä¶

Si on a bien fait notre job, on se retrouve avec un jeu de donn√©es plus facile √† manipuler, et si
$d'‚©Ω3$, il sera m√™me probablement *visualisable*. Youpi.

En revanche **attention** √† part dans des cas tr√®s contraints (par exemple si une dimension est
constante), une r√©duction de dimension va d√©truire de l'information (aucun de nos algos n'est
*bijectif*), on peut r√©duire la dimension mais pas la r√©augmenter (en tout cas pas dans ce cadre).
√áa aura n√©c√©ssairement une cons√©quence (pas forc√©menet toujours n√©gative cependant) sur les
manipulations que vous appliquez aux donn√©es transform√©es.

### ACP

Commencez par [visualiser](https://setosa.io/ev/principal-component-analysis/) de quoi il s'agit.


Comme d'habitude, avec scikit-learn, c'est pas tr√®s sorcier, on peut juste utiliser [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)‚ÄØ:

```python
# Si on a des points
points, blobs = make_blobs(n_samples=32, n_features=5, centers=5, random_state=0)
print("Points:", points)

# Il suffit d'instancier l'estimateur en disant combien de dimesion on veut garder
mon_super_pca = PCA(n_components=4)

# On trouve la d√©composion et on transforme nos points
points_reduced = mon_super_pca.fit_transform(points)

print("R√©duits:", points_reduced)
```

On est donc pass√© de 5 dimensions √† 4. √âvidemment tout seul √ßa ne nous est pas forc√©ment tr√®s utile.
Mais voyons ce qui se passe avec des dimensions qu'on peut visualiser.


Recette :comment faire un plot en 3d (avec `%matplotlib widget` pour le rendre interactif dans jupyter).

```python
%matplotlib widget

points, blobs = make_blobs(n_samples=1024, n_features=3, centers=3, random_state=0)

fig = plt.figure()
ax = fig.add_subplot(aspect="equal", projection="3d")

ax.scatter(xs=points[:, 0], ys=points[:, 1], zs=points[:, 2], c=blobs, marker=".")
# Pour une raison inconnue, fig.show() affiche deux fois la figure, je d√©teste un peu matplotlib
plt.show()
```

Maintenant voyons l'effet d'une ACP

```python
%matplotlib widget

points, blobs = make_blobs(n_samples=1024, n_features=3, centers=3, random_state=0)
pca = PCA(n_components=2)
points_proj = pca.fit_transform(points)  # ‚Üê toute ce qui est important se passe ici

fig = plt.figure()
ax3d = fig.add_subplot(1, 2, 1, aspect="equal", projection="3d")
ax2d = fig.add_subplot(1, 2, 2, aspect="equal")

ax3d.scatter(xs=points[:, 0], ys=points[:, 1], zs=points[:, 2], c=blobs, marker=".")
ax2d.scatter(x=points_proj[:, 0], y=points_proj[:, 1], c=blobs, marker=".")

# Show projection axes
print(pca.components_)
ax3d.plot(
    [0.0, 4 * pca.components_[0, 0]],
    [0.0, 4 * pca.components_[0, 1]],
    [0.0, 4 * pca.components_[0, 2]],
    c="red",
)
ax3d.plot(
    [0.0, 4 * pca.components_[1, 0]],
    [0.0, 4 * pca.components_[1, 1]],
    [0.0, 4 * pca.components_[1, 2]],
    c="blue",
)

ax2d.plot(
    [0.0, 4.0],
    [0.0, 0.0],
    c="red",
)
ax2d.plot(
    [0.0, 0.0],
    [0.0, 4.0],
    c="blue",
)
plt.show()
```

√âvidemment, comme d'habitude toujours, scikit-learn propose une impl√©mentation avec toutes les
astuces possibles pour que √ßa se passe bien, et vous donne le contr√¥le dessus avec plein
d'hyperparam√®tres dont les valeurs par d√©faut sont g√©n√©ralement pas mal, mais qu'il est bon d'aller
chercher √† comprendre.


Astuce‚ÄØ: si vous voulez les valeurs propres de la matrice de covariance (donc les axes dans l'espace
de d√©part), comme vous le voyez ci-dessus, vous pouvez les r√©cup√©rer apre√®s `fit` dans
`pca.components_`.

#### üì° Entra√Ænement üì°

√Ä vous de jouer‚ÄØ! Utilisez des ACP pour visualiser les classes des datasets Iris et Wine et voyez si
√ßa marche bien avec leurs features.


- Exo: calculer les variances suivant les axes et faire un plot variance = f(d')

### t-SNE

Disponible comme
[sklearn.manifold.TSNE](scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), voir
aussi [OpenTSNE](https://opentsne.readthedocs.io) pour une impl√©mentation plus agr√©able par certains
aspects (notamment une m√©thode `transform` qui marche), allez voir leur
[d√©mo](https://opentsne.readthedocs.io/en/stable/examples/01_simple_usage/01_simple_usage.html).

```python
%matplotlib widget

points, blobs = make_blobs(n_samples=1024, n_features=3, centers=3, random_state=0)
tsne = TSNE(n_components=2)
points_proj = tsne.fit_transform(points)  # ‚Üê toute ce qui est important se passe ici

fig = plt.figure()
ax3d = fig.add_subplot(1, 2, 1, aspect="equal", projection="3d")
ax2d = fig.add_subplot(1, 2, 2, aspect="equal")

ax3d.scatter(xs=points[:, 0], ys=points[:, 1], zs=points[:, 2], c=blobs, marker=".")
ax2d.scatter(x=points_proj[:, 0], y=points_proj[:, 1], c=blobs, marker=".")

# Pas d'axes de projection, c'est pas comme √ßa que marche t-SNE!
plt.show()

```

Et pour Iris

```python
%matplotlib widget


features, species = load_iris(return_X_y=True)
tsne = TSNE(n_components=2)
data_proj = tsne.fit_transform(features)  # ‚Üê toute ce qui est important se passe ici

fig = plt.figure()
ax = fig.add_subplot()

ax.scatter(data_proj[:, 0], data_proj[:, 1], c=species, marker=".")

plt.show()

```

Est-ce que c'est mieux qu'une ACP‚ÄØ? Peut-√™tre un peu mais √ßa a peu de sens pour des donn√©es aussi petites.

#### ü™ª Entra√Ænement ü™ª

Essayez en 3d, puis comparez les r√©sultats obtenus en

- Faisant un t-SNE en 3D et en ne gardant que les deux premi√®res coordonn√©es
- Faisant directement un t-SNE en 2D

Pour une ACP √ßa donne √©videmment le m√™me r√©sultat, mais pour t-SNE il n'y a pas de raison que ce soit le cas‚ÄØ!

#### üç∑ Entra√Ænement üç∑

Essayez sur Wine et MNIST (`sklearn.datasets.load_digits`) pour voir.

#### ü§î


Essayons d'appliquer t-SNE √† des donn√©es qui n'ont pas de raison d'avoir une structure‚ÄØ:

```python
%matplotlib widget

gen = np.random.default_rng(0)
points = gen.uniform(-8.0, 8.0, size=(1024, 8))
# Un truc al√©atoire mais avec une vague forme
points = points*np.arange(points.shape[1])
tsne = TSNE(n_components=2)
points_proj = tsne.fit_transform(points)  # ‚Üê toute ce qui est important se passe ici

fig = plt.figure()
ax = fig.add_subplot()

ax.scatter(points_proj[:, 0], points_proj[:, 1], marker=".")

plt.show()

```

Qu'est-ce que vous en pensez‚ÄØ? Appellez moi qu'on en discute ensemble.


Autre point √† noter, qui est dit assez clairement dans les docs et les articles sur t-SNE mais que
les gens oublient souvent‚ÄØ: t-SNE gal√®re en tr√®s haute dimension, √ßa peut donc valoir le coup de
faire d'abord une ACP pour passer les donn√©es en quelque chose comme 50 dimensions pour ensuite
appliquer t-SNE dans cet espace moins complexe.

### UMAP

Un des derniers variants de la famille de t-SNE (en tout cas bas√© sur les m√™mes principes, mais en
mieux). C'est celui que je vous conseille. Il n'est pas dans scikit-learn (pour l'instant‚ÄØ?), mais
il a un package bien fait‚ÄØ: [`umap-learn`](https://umap-learn.readthedocs.io).

```python
%matplotlib widget

points, blobs = make_blobs(n_samples=1024, n_features=3, centers=3, random_state=0)
# Rappel: je mets les imports en d√©but de notebook et vous devriez aussi
umap = UMAP(n_components=2)
points_proj = umap.fit_transform(points)

fig = plt.figure()
ax3d = fig.add_subplot(1, 2, 1, aspect="equal", projection="3d")
ax2d = fig.add_subplot(1, 2, 2, aspect="equal")

ax3d.scatter(xs=points[:, 0], ys=points[:, 1], zs=points[:, 2], c=blobs, marker=".")
ax2d.scatter(x=points_proj[:, 0], y=points_proj[:, 1], c=blobs, marker=".")

plt.show()

```

Voyons sur MNIST

```python
%matplotlib widget

points, digits = load_digits(return_X_y=True)
# Rappel: je mets les imports en d√©but de notebook et vous devriez aussi
umap = UMAP(n_components=2)
points_proj = umap.fit_transform(points)

fig = plt.figure()
ax = fig.add_subplot(aspect="equal")

sc = ax.scatter(x=points_proj[:, 0], y=points_proj[:, 1], c=digits, marker=".")

# Ceci pour faire une l√©gende. Je vous ai d√©j√† dit que je d√©teste pyplot‚ÄØ?
handles = [plt.plot([], color=sc.get_cmap()(sc.norm(c)), ls="", marker="o")[0] for c in range(10)]
labels = list(range(10))
ax.legend(handles, labels)

plt.show()

```

#### üëó Entra√Ænement üëó

- Voir ce que √ßa donne en 3D
- Tester aussi sur [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)
  - T√©l√©chargez les donn√©es et chargez les √† la mano, sans leur `mnist_reader`, √ßa vous apprendra.
