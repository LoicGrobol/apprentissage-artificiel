---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 10‚ÄØ: R√©gression logistique
===============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-20
<!-- #endregion -->

```python
from IPython.display import display
```

## Classifieur lin√©aire

On consid√®re des vecteurs de *features* de dimension $n$

$$\mathbf{x} = (x‚ÇÅ, ‚Ä¶, x_n)$$

Un vecteur de poids de dimension $n$

$$\mathbf{w} = (w‚ÇÅ, ‚Ä¶, w_n)$$

et un biais $b$ scalaire (un nombre quoi).

Pour r√©aliser une classification on consid√®re le nombre $z$ (on parle parfois de *logit*)

$$z=w‚ÇÅ√óx‚ÇÅ + ‚Ä¶ + w_n√óx_n + b = \sum_iw_ix_i + b$$

Ce qu'on note aussi

$$z = \mathbf{w}‚ãÖ\mathbf{x}+b$$

$\mathbf{w}‚ãÖ\mathbf{x}$ se lit ¬´‚ÄØw scalaire x‚ÄØ¬ª, on parle de *produit scalaire* en fran√ßais et de *inner product* en anglais.

(ou pour les math√©maticien‚ãÖne‚ãÖs acharn√©‚ãÖe‚ãÖs $z = \langle w\ |\ x \rangle + b$)

Quelle que soit la fa√ßon dont on le note, on affectera √† $\mathbf{x}$ la classe $0$ si $z < 0$ et la classe $1$ sinon.

## üò¥ Exo üò¥

√âcrire une fonction qui prend en entr√©e un vecteur de features et un vecteur de poids sous forme de
tableaux numpy $x$ et $w$ de dimensions `(n,)` et un biais $b$ sous forme d'un tableau numpy de
dimensions `(1,)` et renvoie $z=\sum_iw_ix_i + b$.

```python
import numpy as np
```

```python
def affine_combination(x, w, b):
    pass # √Ä vous de jouer‚ÄØ!

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

```python
def affine_combination(x, w, b):
    res = np.zeros(1)
    for wi, xi in zip(w, x):
        res += wi*xi
    res += b
    return res

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

```python
def affine_combination(x, w, b):
    return np.inner(w, x) + b

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

## La fonction logistique


$$œÉ(z) = \frac{1}{1 + e^{‚àíz}} = \frac{1}{1 + \exp(‚àíz)}$$

Elle permet de *normaliser* $z$‚ÄØ: $z$ peut √™tre n'importe quel nombre entre $-‚àû$ et $+‚àû$, mais on aura toujours $0 <¬†œÉ(z) < 1$, ce qui permet de l'interpr√©ter facilement comme une *vraisemblance*. Autrement dit, $œÉ(z)$ sera proche de $1$ s'il para√Æt vraisemblable que $x$ appartienne √† la classe $1$ et proche de $0$ sinon.

## üìà Exo üìà

Tracer avec matplotlib la courbe repr√©sentative de la fonction logistique.

```python
def logistic(z):
    return 1/(1+np.exp(-z))
```

```python
%matplotlib inline
import matplotlib.pyplot as plt
x = np.linspace(-10, 10, 5000)
y = logistic(x)
plt.plot(x, y)
```

## R√©gression logistique


Formellement‚ÄØ: on suppose qu'il existe une fonction $f$ qui pr√©dit parfaitement les classes, donc
telle que pour tout couple exemple/√©tiquette $(x, y)$ avec $y$ valant $0$ ou $1$, $f(x) = y$. On
approcher cette fonction par une fonction $g$ de la forme

$$g(x) = œÉ(w‚ãÖx+b)$$

Si on choisit les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur
notre ensemble d'apprentissage, on dit que $g$ est la *r√©gression logistique de $f$* sur cet
ensemble.

Un classifieur logistique, c'est simplement un classifieur qui pour un exemple $x$ renvoie $0$ si
$g(x) < 0.5$ et $1$ sinon.

## üß† Exo üß†

1\. √Ä l'aide d'un lexique de sentiment (par exemple
[VADER](https://github.com/cjhutto/vaderSentiment)), √©crivez une fonction qui prend en entr√©e un
texte en anglais et renvoie sa repr√©sentation sous forme d'un vecteur de features √† deux traits‚ÄØ:
nombre de mots positifs et nombre de mot n√©gatifs.

```python
import re

def poor_mans_tokenizer_and_normalizer(s):
    return [w.lower() for w in re.split(r"\s|\W", s.strip()) if w and all(c.isalpha() for c in w)]

def read_vader(vader_path):
    res = dict()
    with open(vader_path) as in_stream:
        for row in in_stream:
            word, polarity, *_ = row.lstrip().split("\t", maxsplit=2)
            is_positive = float(polarity) > 0
            res[word] = is_positive
    return res

def featurize(text, lexicon):
    words = poor_mans_tokenizer_and_normalizer(text)
    features = np.empty(2)
    features[0] = sum(1 for w in words if lexicon.get(w, False))
    features[1] = sum(1 for w in words if not lexicon.get(w, True))
    return features

lexicon = read_vader("../../data/vader_lexicon.txt")
doc = "I came in in the middle of this film so I had no idea about any credits or even its title till I looked it up here, where I see that it has received a mixed reception by your commentators. I'm on the positive side regarding this film but one thing really caught my attention as I watched: the beautiful and sensitive score written in a Coplandesque Americana style. My surprise was great when I discovered the score to have been written by none other than John Williams himself. True he has written sensitive and poignant scores such as Schindler's List but one usually associates his name with such bombasticities as Star Wars. But in my opinion what Williams has written for this movie surpasses anything I've ever heard of his for tenderness, sensitivity and beauty, fully in keeping with the tender and lovely plot of the movie. And another recent score of his, for Catch Me if You Can, shows still more wit and sophistication. As to Stanley and Iris, I like education movies like How Green was my Valley and Konrack, that one with John Voigt and his young African American charges in South Carolina, and Danny deVito's Renaissance Man, etc. They tell a necessary story of intellectual and spiritual awakening, a story which can't be told often enough. This one is an excellent addition to that genre."
doc_features = featurize(doc, lexicon)
doc_features
```

2\. Appliquer la fonction pr√©c√©dente sur [le mini-corpus IMDB](../../data/imdb_smol.tar.gz)


3\. √âcrire un classifieur logistique qui prend en entr√©e les vecteurs de features
pr√©c√©dents et utilise les poids respectifs $0.6$ et $-0.4$ et un biais de $0$. Appliquez ce
classifieur sur le mini-corpus IMDB et calculez son exactitude.

```python
def hardcoded_classifier(x):
    return logistic(
        np.inner(
            np.array([0.6, -0.4]),
            x,
        )
    )

hardcoded_classifier(doc_features)
```

## Fonction de co√ªt

On formalise ¬´‚ÄØ√™tre le plus proche possible‚ÄØ¬ª de la section pr√©c√©dente comme minimiser une certaine
fonction de co√ªt (*loss*) $L$.

Autrement dit, √©tant donn√© un ensemble de test $(x‚ÇÅ, y‚ÇÅ),‚ÄØ‚Ä¶, (x_n, y_n)$, on va mesurer la qualit√©
du classifieur logistique $g$

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

Dans le cas de la r√©gression logistique, on va utiliser la *log-vraisemblance n√©gative* (*negative
log-likelihood*)‚ÄØ:

On d√©finit la *vraisemblance* $V$ comme
$$
V(a, y) =
    \begin{cases}
        a & \text{si $y = 1$}\\
        1-a & \text{sinon}
    \end{cases}
$$

Intuitivement, il s'agit de la vraisemblance affect√©e par le mod√®le √† la classe correcte $y$. Il ne
s'agit donc pas d'un co√ªt, mais d'un *gain* (si sa valeur est haute, c'est que le mod√®le est bon)

La *log-vraisemblance n√©gative* $L$ est alors d√©finie par

$$L(a, y) = -\log(V(a, y))$$

Le $\log$ permet de r√©gulariser la valeur (c'est plus facile √† apprendre) et le $-$ √† s'assurer
qu'on a bien un co√ªt (plus la valeur est basse, meilleur le mod√®le est).

Une autre fa√ßon de le voir‚ÄØ: $L(a, y)$, c'est la
[surprise](https://en.wikipedia.org/wiki/Information_content) de $y$ au sens de la th√©orie de
l'information. Autrement dit‚ÄØ: si j'estime qu'il y a une probabilit√© $a$ d'observer la classe $y$,
$L(a, y)$ mesure √† quel point il serait surprenant d'observer effectivement $y$.


On peut l'√©crire en une ligne‚ÄØ: pour un exemple $x$, le co√ªt de l'exemple $(x, y)$ est

$$L(g(x), y) = -\log\left[g(x)√óy + (1-g(x))√ó(1-y)\right]$$

C'est un *trick*, l'astuce c'est que comme $y$ vaut soit $0$ soit $1$, soit $y=0$, soit $1-y=0$ et
donc la somme dans le $\log$ se simplifie dans tous les cas. Rien de transcendant l√†-dedans.

La formule diff√®re un peu de celle de *Speech and Language Processing* mais les r√©sultats sont les
m√™mes et celle-ci est mieux pour notre probl√®me‚ÄØ!

<small>En fait la leur est la formule g√©n√©rale de l'entropie crois√©e pour des distributions de proba
√† support dans $\{0, 1\}$, ce qui est une autre intuition pour cette fonction de co√ªt, mais ici elle
nous complique la vie.</small>

## üìâ Exo üìâ

√âcrire une fonction qui prend en entr√©e

- Un vecteur de features $x$ de taille $n$
- Un vecteur de poids $w$ de taille $n$ et un biais $b$ (de taille $1$)
- Une classe cible $y$ ($0$ ou $1$)

Et renvoie la log-vraisemblance n√©gative du classifieur logistique de poids $(w, b)$ pour l'exemple
$(x, y)$.

Servez vous-en pour calculer le co√ªt du classifieur de l'exercise pr√©c√©dent sur le mini-corpus IMDB.

## Descente de gradient

Pour un classifieur logistique

$$\frac{‚àÇL(g(x, y))}{‚àÇw_i} = (g(x)-y)√óx_i$$

et

$$\frac{‚àÇL(g(x, y))}{‚àÇb} = (g(x)-y)$$

## üßê Exo üßê

Reprendre la fonction qui calcule la fonction de co√ªt, mais faire en sorte qu'elle renvoie √©galement
le gradient par rapport √† $w$ et la d√©riv√©e partielle par rapport √† $b$ en $(x, y)$.

## üò© Exo üò©

S'en servir pour apprendre les poids √† donner aux features pr√©c√©dentes √† l'aide du  [mini-corpus
IMDB](../../data/imdb_smol.tar.gz) en utilisant l'algorithme de descente de gradient stochastique.
