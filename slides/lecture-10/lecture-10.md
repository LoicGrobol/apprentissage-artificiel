---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 10‚ÄØ: R√©gression logistique
===============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-27
<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
```

## Vectorisations arbitraires de documents

On a vu des fa√ßons de traiter des documents vus comme des sacs des mots en les repr√©sentant comme
des vecteurs dont les coordonn√©es correspondaient √† des nombres d'occurrences.

Mais on aimerait ‚Äî‚ÄØentre autres‚ÄØ‚Äî pouvoir travailler avec des repr√©sentations arbitraires, on peut
par exemple imaginer vouloir repr√©senter un document par ≈Äa polarit√© (au sens de l'analyse du
sentiment) de ses mots.

## üß† Exo üß†

### 1. Vectoriser un document

√Ä l'aide d'un lexique de sentiment (par exemple [VADER](https://github.com/cjhutto/vaderSentiment)),
√©crivez une fonction qui prend en entr√©e un texte en anglais et renvoie sa repr√©sentation sous forme
d'un vecteur de features √† deux traits‚ÄØ: polarit√© positive moyenne (la somme des polarit√©s positives
des mots qu'il contient divis√©e par sa longueur en nombre de mots) et polarit√© n√©gative moyenne.

```python
def read_vader(vader_path):
    pass  #¬†√Ä vous de jouer
```

```python
def featurize(doc, lexicon):
    pass # √Ä vous de jouer‚ÄØ!
```

```python
lexicon = read_vader("../../data/vader_lexicon.txt")
doc = "I came in in the middle of this film so I had no idea about any credits or even its title till I looked it up here, where I see that it has received a mixed reception by your commentators. I'm on the positive side regarding this film but one thing really caught my attention as I watched: the beautiful and sensitive score written in a Coplandesque Americana style. My surprise was great when I discovered the score to have been written by none other than John Williams himself. True he has written sensitive and poignant scores such as Schindler's List but one usually associates his name with such bombasticities as Star Wars. But in my opinion what Williams has written for this movie surpasses anything I've ever heard of his for tenderness, sensitivity and beauty, fully in keeping with the tender and lovely plot of the movie. And another recent score of his, for Catch Me if You Can, shows still more wit and sophistication. As to Stanley and Iris, I like education movies like How Green was my Valley and Konrack, that one with John Voigt and his young African American charges in South Carolina, and Danny deVito's Renaissance Man, etc. They tell a necessary story of intellectual and spiritual awakening, a story which can't be told often enough. This one is an excellent addition to that genre."
doc_features = featurize(doc, lexicon)
doc_features
```

### üß† Correction 1 üß†

On commence par recycler notre tokenizer/normaliseur

```python
import re

def poor_mans_tokenizer_and_normalizer(s):
    return [w.lower() for w in re.split(r"\s|\W", s.strip()) if w and all(c.isalpha() for c in w)]
```

On lit le lexique

```python
def read_vader(vader_path):
    res = dict()
    with open(vader_path) as in_stream:
        for row in in_stream:
            word, polarity, *_ = row.lstrip().split("\t", maxsplit=2)
            res[word] = float(polarity)
    return res
lexicon = read_vader("../../data/vader_lexicon.txt")
lexicon
```

Et voil√† comment on r√©cup√®re la repr√©sentation d'un document

```python
def featurize(text, lexicon):
    words = poor_mans_tokenizer_and_normalizer(text)
    features = np.empty(2)
    # Le max permet de remonter les polarit√©s n√©gatives √† 0
    features[0] = sum(max(lexicon.get(w, 0), 0) for w in words)/len(words)
    features[1] = sum(max(-lexicon.get(w, 0), 0) for w in words)/len(words)
    return features
```

On teste‚ÄØ?

```python
doc = "I came in in the middle of this film so I had no idea about any credits or even its title till I looked it up here, where I see that it has received a mixed reception by your commentators. I'm on the positive side regarding this film but one thing really caught my attention as I watched: the beautiful and sensitive score written in a Coplandesque Americana style. My surprise was great when I discovered the score to have been written by none other than John Williams himself. True he has written sensitive and poignant scores such as Schindler's List but one usually associates his name with such bombasticities as Star Wars. But in my opinion what Williams has written for this movie surpasses anything I've ever heard of his for tenderness, sensitivity and beauty, fully in keeping with the tender and lovely plot of the movie. And another recent score of his, for Catch Me if You Can, shows still more wit and sophistication. As to Stanley and Iris, I like education movies like How Green was my Valley and Konrack, that one with John Voigt and his young African American charges in South Carolina, and Danny deVito's Renaissance Man, etc. They tell a necessary story of intellectual and spiritual awakening, a story which can't be told often enough. This one is an excellent addition to that genre."
doc_features = featurize(doc, lexicon)
doc_features
```

### 2. Vectoriser un corpus

Appliquer la fonction pr√©c√©dente sur [le mini-corpus IMDB](../../data/imdb_smol.tar.gz)

### üß† Correction 2 üß†

Commen√ßons par l'extraire

```bash
cd ../../local
tar -xzf ../data/imdb_smol.tar.gz 
ls -lah imdb_smol
```

Maintenant on parcourt le dossier pour construire nos repr√©sentations

```python
from collections import defaultdict
import pathlib  # Manipuler des chemins et des fichiers agr√©ablement

def featurize_dir(corpus_root, lexicon):
    corpus_root = pathlib.Path(corpus_root)
    res = defaultdict(list)
    for clss in corpus_root.iterdir():
        # On peut aussi utiliser une compr√©hension de liste et avoir un dict pas default
        for doc in clss.iterdir():
            # `stem` et `read_text` c'est de la magie de `pathlib`, check it out
            res[clss.stem].append(featurize(doc.read_text(), lexicon))
    return res

# On r√©utilise le lexique pr√©c√©dent
imdb_features = featurize_dir("../../local/imdb_smol", lexicon)
imdb_features
```

## Visualisation


Comment se r√©partissent les documents du corpus avec la repr√©sentation qu'on a choisi

```python
import matplotlib.pyplot as plt
import seaborn as sns

X = np.array([d[0] for d in (*imdb_features["pos"], *imdb_features["neg"])])
Y = np.array([d[1] for d in (*imdb_features["pos"], *imdb_features["neg"])])
H = np.array([*("pos" for _ in imdb_features["pos"]), *("neg" for _ in imdb_features["neg"])])

fig = plt.figure(dpi=200)
sns.scatterplot(x=X, y=Y, hue=H, s=5)
plt.show()
```

On voit des tendances qui se d√©gagent, mais clairement √ßa va √™tre un peu coton

## Classifieur lin√©aire

On consid√®re des vecteurs de *features* de dimension $n$

$$\mathbf{x} = (x‚ÇÅ, ‚Ä¶, x_n)$$

Un vecteur de poids de dimension $n$

$$\mathbf{w} = (w‚ÇÅ, ‚Ä¶, w_n)$$

et un biais $b$ scalaire (un nombre quoi).

Pour r√©aliser une classification on consid√®re le nombre $z$ (on parle parfois de *logit*)

$$z=w‚ÇÅ√óx‚ÇÅ + ‚Ä¶ + w_n√óx_n + b = \sum_iw_ix_i + b$$

Ce qu'on note aussi

$$z = \mathbf{w}‚ãÖ\mathbf{x}+b$$

$\mathbf{w}‚ãÖ\mathbf{x}$ se lit ¬´‚ÄØw scalaire x‚ÄØ¬ª, on parle de *produit scalaire* en fran√ßais et de
*inner product* en anglais.

(ou pour les math√©maticien‚ãÖne‚ãÖs acharn√©‚ãÖe‚ãÖs $z = \langle w\ |\ x \rangle + b$)

Quelle que soit la fa√ßon dont on le note, on affectera √† $\mathbf{x}$ la classe $0$ si $z < 0$ et la
classe $1$ sinon.

## üò¥ Exo üò¥

### 1. Une fonction affine

√âcrire une fonction qui prend en entr√©e un vecteur de features et un vecteur de poids sous forme de
tableaux numpy $x$ et $w$ de dimensions `(n,)` et un biais $b$ sous forme d'un tableau numpy de
dimensions `(1,)` et renvoie $z=\sum_iw_ix_i + b$.

```python
def affine_combination(x, w, b):
    pass # √Ä vous de jouer‚ÄØ!

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

### üò¥ Correction 1 üò¥


Une version √©l√©mentaire avec des boucles

```python
def affine_combination(x, w, b):
    res = np.zeros(1)
    for wi, xi in zip(w, x):
        res += wi*xi
    res += b
    return res

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

Une version plus courte avec les fonctions natives de numpy

```python
def affine_combination(x, w, b):
    return np.inner(w, x) + b

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

### 2. Un classifieur lin√©aire

√âcrire un classifieur lin√©aire qui prend en entr√©e des vecteurs de features √† deux dimensions
pr√©c√©dents et utilise les poids respectifs $0.6$ et $-0.4$ et un biais de $-0.01$. Appliquez ce
classifieur sur le mini-corpus IMDB qu'on a vectoris√© et calculez son exactitude.

```python
def hardcoded_classifier(x):
    return False  # √Ä vous de jouer

hardcoded_classifier(doc_features)
```


### üò¥ Correction 2 üò¥

On commence par d√©finir le classifieur‚ÄØ: on va renvoyer `True`¬†pour la classe positive et `False`
pour la classe n√©gative.


```python
def hardcoded_classifier(x):
    return affine_combination(x, np.array([0.6, -0.4]), -0.01) > 0.0

hardcoded_classifier(doc_features)
```

Maintenant on le teste

```python
correct_pos = sum(1 for doc in imdb_features["pos"] if hardcoded_classifier(doc))
print(f"Recall for 'pos': {correct_pos}/{len(imdb_features['pos'])}={correct_pos/len(imdb_features['pos']):.02%}")
correct_neg = sum(1 for doc in imdb_features["neg"] if not hardcoded_classifier(doc))
print(f"Recall for 'neg': {correct_neg}/{len(imdb_features['neg'])}={correct_neg/len(imdb_features['neg']):.02%}")
print(f"Accuracy: {correct_pos+correct_neg}/{len(imdb_features['pos'])+len(imdb_features['neg'])}={(correct_pos+correct_neg)/(len(imdb_features['pos'])+len(imdb_features['neg'])):.02%}")
```


## Classifieur lin√©aire‚ÄØ?

Pourquoi lin√©aire‚ÄØ? Regardez la figure suivante qui colore les points $(x,y)$ du plan en fonction de
la valeur de $z$.

```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) - 0.01

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

Ou encore plus clairement, si on repr√©sente la classe assign√©e

```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) -0.01 > 0.0

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On voit bien que la fronti√®re de classification est une droite, *a line*. On a donc un *linear*
classifier‚ÄØ: un classifieur lin√©aire (m√™me si en fran√ßais on dirait qu'il s'agit d'une fonction
*affine*).


Qu'est-ce que √ßa donne si on superpose avec notre corpus‚ÄØ?

```python
fig = plt.figure(dpi=200)

x = np.linspace(0, 0.4, 1000)
y = np.linspace(0, 0.4, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) -0.01 > 0.0

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))

X = np.array([d[0] for d in (*imdb_features["pos"], *imdb_features["neg"])])
Y = np.array([d[1] for d in (*imdb_features["pos"], *imdb_features["neg"])])
H = np.array([*(1 for _ in imdb_features["pos"]), *(0 for _ in imdb_features["neg"])])
plt.scatter(x=X, y=Y, c=H, cmap="viridis", s=5)

plt.show()
```

Pas si surprenant que nos r√©sultats ne soient pas terribles‚Ä¶

## La fonction logistique


$$œÉ(z) = \frac{1}{1 + e^{‚àíz}} = \frac{1}{1 + \exp(‚àíz)}$$

Elle permet de *normaliser* $z$‚ÄØ: $z$ peut √™tre n'importe quel nombre entre $-‚àû$ et $+‚àû$, mais on
aura toujours $0 <¬†œÉ(z) < 1$, ce qui permet de l'interpr√©ter facilement comme une *vraisemblance*.
Autrement dit, $œÉ(z)$ sera proche de $1$ s'il para√Æt vraisemblable que $x$ appartienne √† la classe
$1$ et proche de $0$ sinon.

## üìà Exo üìà

Tracer avec matplotlib la courbe repr√©sentative de la fonction logistique.

### üìà Correction üìà

```python
def logistic(z):
    return 1/(1+np.exp(-z))
```

```python
%matplotlib inline
import matplotlib.pyplot as plt
x = np.linspace(-10, 10, 5000)
y = logistic(x)
plt.plot(x, y)
plt.xlabel("$x$")
plt.ylabel("$œÉ(x)$")
plt.title("Courbe repr√©sentative de la fonction logistique sur $[-10, 10]$")
plt.show()
```

## R√©gression logistique

Formellement‚ÄØ: on suppose qu'il existe une fonction $f$ qui pr√©dit parfaitement les classes, donc
telle que pour tout couple exemple/√©tiquette $(x, y)$ avec $y$ valant $0$ ou $1$, $f(x) = y$. On
approcher cette fonction par une fonction $g$ de la forme

$$g(x) = œÉ(w‚ãÖx+b)$$

Si on choisit les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur
notre ensemble d'apprentissage, on dit que $g$ est la *r√©gression logistique de $f$* sur cet
ensemble.

Un classifieur logistique, c'est simplement un classifieur qui pour un exemple $x$ renvoie $0$ si
$g(x) < 0.5$ et $1$ sinon. Il a exactement les m√™mes capacit√©s de discrimination qu'un classifieur
lin√©aire (sa fronti√®re de d√©cision est la m√™me et il ne sait donc pas prendre de d√©cisions plus
complexes), mais on peut interpr√©ter la confiance qu'il a dans sa d√©cision.


Par exemple voici la confiance que notre classifieur cod√© en dur a en ses d√©cisions

```python
def classifier_confidence(x):
    return logistic(affine_combination(x, np.array([0.6, -0.4]), -0.01))


g_x = classifier_confidence(doc_features)
display(g_x)
display(Markdown(f"Le classifieur est s√ªr √† {g_x:.06%} que ce document est dans la classe $1$."))
display(Markdown(f"Autrement dit, d'apr√®s le classifieur, la classe $1$ a {g_x:.06%} de vraisemblance pour ce document"))
```


Quelle est la vraisemblance de la classe $0$ (review n√©gative)‚ÄØ? Et bien le reste

```python
1.0 - classifier_confidence(doc_features)
```

Comme l'exemple en question appartient bien √† cette classe, √ßa signifie que notre classifieur et
plut√¥t bon **sur cet exemple**. L'est-il sur le reste du corpus‚ÄØ?

```python
pos_confidence = sum(classifier_confidence(doc) for doc in imdb_features["pos"])
print(f"Average confidence for 'pos': {pos_confidence/len(imdb_features['pos']):.02%}")
neg_confidence = sum(1-classifier_confidence(doc) for doc in imdb_features["neg"])
print(f"Average confidence for 'neg': {neg_confidence/len(imdb_features['neg']):.02%}")
print(f"Average confidence for the correct class: {(pos_confidence+neg_confidence)/(len(imdb_features['pos']) + len(imdb_features['neg'])):.02%}")
```

Autrement dit, pour un exemple pris au hasard dans le corpus, la vraisemblance de sa classe telle
que jug√©e par le classifieur sera de $50.49\%$. Un classifieur parfait obtiendrait $100\%$, un
classifieur qui prendrait syst√©matiquement la mauvaise d√©cision $0\%$ et un classifieur al√©atoire
uniforme $50\%$ (puisque notre corpus a autant d'exemples de chaque classe).


Moralit√©‚ÄØ: nos poids ne sont pas tr√®s bien choisis, et notre pr√©occupation dans la suite va √™tre de
chercher comment choisir des poids pour que la confiance moyenne de la classe correcte soit aussi
haute que possible.

## Fonction de co√ªt

On a dit que notre objectif √©tait

> Chercher les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur notre
ensemble d'apprentissage

On formalise ¬´‚ÄØ√™tre le plus proche possible‚ÄØ¬ª de la section pr√©c√©dente comme minimiser une certaine
fonction de co√ªt (*loss*) $L$ qui mesure l'erreur faite par le classifieur sur un exemple.

$$L(g(x), y) = \text{l'√©cart entre la classe pr√©dite par $g$ pour $x$ et la classe correcte $y$}$$

√âtant donn√© un ensemble de test $(x‚ÇÅ, y‚ÇÅ),‚ÄØ‚Ä¶, (x_n, y_n)$, on estime l'erreur faite par le
classifieur logistique $g$ pour chaque exemple $(x_i, y_i)$ comme le co√ªt local $L(g(x·µ¢), y·µ¢)$ et
son erreur sur tout l'ensemble de test par le co√ªt global $\mathcal{L}$‚ÄØ:

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

Plus $\mathcal{L}$ sera bas, meilleur sera notre classifieur.

Dans le cas de la r√©gression logistique, on va s'inspirer de ce qu'on a vu dans la section
pr√©c√©dente et utiliser la *log-vraisemblance n√©gative* (*negative log-likelihood*)‚ÄØ:

On d√©finit la *vraisemblance* $V$ comme pr√©c√©demment par
$$
V(a, y) =
    \begin{cases}
        a & \text{si $y = 1$}\\
        1-a & \text{sinon}
    \end{cases}
$$

Intuitivement, il s'agit de la vraisemblance affect√©e par le mod√®le √† la classe correcte $y$. Il ne
s'agit donc pas d'un co√ªt, mais d'un *gain* (si sa valeur est haute, c'est que le mod√®le est bon)

La *log-vraisemblance n√©gative* $L$ est alors d√©finie par

$$L(a, y) = -\log(V(a, y))$$

Le $\log$ est l√† pour plusieurs raisons, calculatoires et th√©oriques<sup>1</sup> et le $-$ √†
s'assurer qu'on a bien un co√ªt (plus la valeur est basse, meilleur le mod√®le est).

<small>1. Entre autres, comme pour *Na√Øve Bayes*, parce qu'une somme de $\log$-vraisemblance peut
√™tre vue comme le $\log$ de la probabilit√© d'une conjonction d'√©v√©nements ind√©pendants. Mais surtout
parce qu'il rend la fonction de co√ªt **convexe** par rapport √† $w$</small>.

Une interpr√©tation possible‚ÄØ: $L(a, y)$, c'est la
[surprise](https://en.wikipedia.org/wiki/Information_content) de $y$ au sens de la th√©orie de
l'information. Autrement dit‚ÄØ: si j'estime qu'il y a une probabilit√© $a$ d'observer la classe $y$,
$L(a, y)$ mesure √† quel point il serait surprenant d'observer effectivement $y$.


On peut v√©rifier qu'il s'agit bien d'un co√ªt‚ÄØ:

- C'est un nombre positif
- Si le classifieur prend une d√©cision correcte avec une confiance parfaite le co√ªt est nul‚ÄØ:

  $$
    \begin{cases}
        L(1.0, 1) = -\log(1.0) = 0\\
        L(0.0, 0) = -\log(1.0-0.0) = -\log(1.0) = 0
    \end{cases}
  $$
- Si le classifieur prend une d√©cision erron√©e avec une confiance parfaite le co√ªt est infini‚ÄØ:

  $$
    \begin{cases}
        L(0.0, 1) = -\log(0.0) = +\infty\\
        L(1.0, 0) = -\log(1.0-1.0) = \log(0.0) = +\infty
    \end{cases}
  $$

On peut aussi v√©rifier facilement que $L(a, 1)$ est d√©croissant par rapport √† $a$ et que $L(1-a, 0)$
est croissant par rapport √† $a$. Autrement dit, plus le classifieur juge que la classe correcte est
vraisemblable plus le co√ªt $L$ est bas.


Enfin, on peut l'√©crire $L$ en une ligne‚ÄØ: pour un exemple $x$, le co√ªt de l'exemple $(x, y)$ est

$$L(g(x), y) = -\log\left[g(x)√óy + (1-g(x))√ó(1-y)\right]$$

C'est un *trick*, l'astuce c'est que comme $y$ vaut soit $0$ soit $1$, soit $y=0$, soit $1-y=0$ et
donc la somme dans le $\log$ se simplifie dans tous les cas. Rien de transcendant l√†-dedans.

La formule diff√®re un peu de celle de *Speech and Language Processing*, mais les r√©sultats sont les
m√™mes et celle-ci est mieux pour notre probl√®me‚ÄØ!

<small>En fait la leur est la formule g√©n√©rale de l'entropie crois√©e pour des distributions de proba
√† support dans $\{0, 1\}$, ce qui est une autre intuition pour cette fonction de co√ªt, mais ici elle
nous complique la vie.</small>

Une derni√®re fa√ßon de l'√©crire en une ligne‚ÄØ:

$$L(g(x), y) = -\log\left[g(x)\mathbb{1}_{y=1} + (1-g(x))\mathbb{1}_{y=0}\right]$$

## üìâ Exo üìâ

√âcrire une fonction qui prend en entr√©e

- Un vecteur de features $x$ de taille $n$
- Un vecteur de poids $w$ de taille $n$ et un biais $b$ (de taille $1$)
- Une classe cible $y$ ($0$ ou $1$)

Et renvoie la log-vraisemblance n√©gative du classifieur logistique de poids $(w, b)$ pour l'exemple
$(x, y)$.

Servez-vous en pour calculer le co√ªt du classifieur de l'exercise pr√©c√©dent sur le mini-corpus IMDB.

### üìâ Correction üìâ

```python
def logistic_negative_log_likelihood(x, w, b, y):
    g_x = logistic(affine_combination(x, w, b))
    if y == 1:
        correct_likelihood = g_x
    else:
        correct_likelihood = 1-g_x
    loss = -np.log(correct_likelihood)
    return loss
```

```python
def loss_on_imdb(w, b, featurized_corpus):
    loss_on_pos = np.zeros(1)
    for doc_features in featurized_corpus["pos"]:
        loss_on_pos += logistic_negative_log_likelihood(
            doc_features, w, b, 1
        )
    loss_on_neg = np.zeros(1)
    for doc_features in featurized_corpus["neg"]:
        loss_on_neg += logistic_negative_log_likelihood(
            doc_features, w, b, 0
        )
    return loss_on_pos + loss_on_neg
```

Avec des compr√©hensions

```python
def loss_on_imdb(w, b, featurized_corpus):
    loss_on_pos = sum(
        logistic_negative_log_likelihood(doc_features, w, b, 1)
        for doc_features in featurized_corpus["pos"]
    )
    loss_on_neg = sum(
        logistic_negative_log_likelihood(doc_features, w, b, 0)
        for doc_features in featurized_corpus["neg"]
    )
    return loss_on_pos + loss_on_neg
```

En version num√©riquement stable

```python
import math
def loss_on_imdb(w, b, featurized_corpus):
    loss_on_pos = math.fsum(
        logistic_negative_log_likelihood(doc_features, w, b, 1).astype(float)
        for doc_features in featurized_corpus["pos"]
    )
    loss_on_neg = math.fsum(
        logistic_negative_log_likelihood(doc_features, w, b, 0).astype(float)
        for doc_features in featurized_corpus["neg"]
    )
    return np.array([loss_on_pos + loss_on_neg])
```

```python
loss_on_imdb(np.array([0.6, -0.4]), -0.01, imdb_features)
```

<!-- #region -->
## Descente de gradient

### Principe g√©n√©ral

L'**algorithme de descente de gradient** est la cl√© de voute de l'essentiel des travaux en
apprentissage artificiel moderne. Il s'agit d'un algorithme it√©ratif qui √©tant donn√© un mod√®le
param√©tris√© et une fonction de co√ªt (avec des hypoth√®ses de r√©gularit√© assez faibles) permet de
trouver des valeurs des param√®tres pour lesquelles la fonction de co√ªt est minimal.

On ne va pas rentrer dans les d√©tails de l'algorithme de descente de gradient stochastique, mais
juste essayer de se donner quelques id√©es.

L'intuition √† avoir est la suivante‚ÄØ: si vous √™tes dans une vall√©e et que vous voulez trouver
rapidement le point le plus bas, une fa√ßon de faire est de chercher la direction vers laquelle la
pente descend le plus vite, de faire quelques pas dans cette direction puis de recommencer. On parle
aussi pour cette raison d'**algorithme de la plus forte pente**.

Clairement une condition pour que √ßa marche peu importe le point de d√©part, c'est que la vall√©e
n'ait qu'un seul point localement le plus bas. Par exemple √ßa marche avec une vall√©e comme celle-ci
<!-- #endregion -->

```python
%matplotlib inline
import tol_colors as tc
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = R**2 - 1

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

Mais pas pour celle-l√†

```python
%matplotlib inline
import tol_colors as tc
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = -np.cos(R)/(1+0.5*R**2)

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
#ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

OK, mais comment on trouve la plus forte pente en pratique‚ÄØ? En une dimension il suffit de suivre
l'oppos√© du nombre d√©riv√©‚ÄØ: <https://uclaacm.github.io/gradient-descent-visualiser/#playground>


En plus de dimensions, c'est plus compliqu√©, mais on peut s'en sortir en suivant le *gradient* qui
est une g√©n√©ralisation du nombre d√©riv√©‚ÄØ: <https://jackmckew.dev/3d-gradient-descent-in-python.html>


Ce qui fait marcher la machine c'est que **le gradient indique la direction dans laquelle la
fonction cro√Æt le plus vite**. Et que l'oppos√© du gradient indique la direction dans laquelle la
fonction d√©cro√Æt le plus vite.

(localement)

<!-- #region -->
Concr√®tement si on veut trouver $\theta$ tel que $f(\theta)$ soit minimale pour une certaine
fonction $f$ dont le gradient est donn√© par `grad_f` √ßa donne l'algo suivant

```python
def descent(grad_f, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        # On trouve la direction de plus grande pente
        steepest_direction = -grad_f(theta)
        #¬†On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
    return theta
```

Les *hyperparam√®tres* sont

- `theta_0` est notre point de d√©part, notre premi√®re estimation d'o√π se trouvera le minimum, que
  l'algorithme va raffiner. √âvidemment si on a d√©j√† une id√©e de vers o√π on pourrait le trouver, √ßa
  ira plus vite. Si on a aucune id√©e, on peut le prendre al√©atoire.
- `learning_rate` ou ¬´‚ÄØtaux d'apprentissage‚ÄØ¬ª‚ÄØ: de combien on se d√©place √† chaque √©tape. Si on le
  prend grand on arrive vite vers la r√©gion du minimum, on mettra longtemps pour en trouver une
  approximation pr√©cise. Si on le prend petit, √ßa sera l'inverse.
- `n_steps` est le nombre d'√©tapes d'optimisations. Dans un probl√®me d'apprentissage, c'est aussi le
  nombre de fois o√π on aura parcouru l'ensemble d'apprentissage et on parle souvent d'**epoch**

Ici on se donne un nombre fixe¬†d'epochs, une autre possibilit√© serait de s'arr√™ter quand on ne bouge
plus trop, par exemple avec une condition comme

```python
if np.max(grad_f(theta)) < 0.00001:
    break
```

dans la boucle et √©ventuellement avec une boucle infinie `while True`.
<!-- #endregion -->

Point notation‚ÄØ:

- **Le gradient de $f$** est souvent not√© $\nabla f$ ou $\operatorname{grad}f$, voire $\vec\nabla f$
  ou $\overrightarrow{\operatorname{grad}} f$ (pour dire que c'est un vecteur)
- Si $Œ∏=(Œ∏_1, ‚Ä¶, Œ∏_n)$, autrement dit si $f$ est une fonction de $n$ variables, on note
  $\operatorname{grad}f = \left(\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_1}, ‚Ä¶, \frac{‚àÇf(Œ∏)}{‚àÇŒ∏_n}\right)$. Autrement dit
  $\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_i}$, la **d√©riv√©e partielle** de $f(Œ∏)$ par rapport √† $Œ∏_i$ est la $i$-√®me
  coordonn√©es du gradient de $f$.
- **Le taux d'apprentissage** est souvent not√© $Œ±$ ou $Œ∑$


### Descente de gradient stochastique

Rappelez-vous, on a dit que notre fonction de co√ªt, c'√©tait

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

et on cherche la valeur du param√®tre $Œ∏ = (w_1, ‚Ä¶, w_n, b)$ tel que $\mathcal{L}$ soit le plus petit
possible.


On peut utilise la propri√©t√© d'**additivit√©** du gradient‚ÄØ: pour deux fonctions $f$ et $g$, on a

$$\operatorname{grad}(f+g) = \operatorname{grad}f + \operatorname{grad}g$$

Donc ici

$$\operatorname{grad}\mathcal{L} = \sum_i \operatorname{grad}L(g(x·µ¢), y·µ¢)$$

<!-- #region -->
Si on dispose d'une fonction `grad_L` qui, √©tant donn√©s $g(x_i)$ et $y_i$, renvoie
$\operatorname{grad}L(g(x_i), y_i)$, l'algorithme de descente du gradient devient alors

```python
def descent(train_set, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        w = theta[:-1]
        b = theta[-1]
        partial_grads = []
        for (x, y) in train_set:
            #¬†On calcule g(x)
            g_x = logistic(np.inner(w,x)+b)
            #¬†On calcule le gradient de L(g(x), y))
            partial_grads.append(grad_L(g_x, y))
        # On trouve la direction de plus grande pente
        steepest_direction = -np.sum(partial_grads)
        # On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Pour chaque √©tape, on doit calculer tous les $g(x_i)$ et $\operatorname{grad}L(g(x_i), y_i)$. C'est
tr√®s couteux, il doit y avoir moyen de faire mieux.


Si les $L(g(x·µ¢), y·µ¢)$ √©taient ind√©pendants, ce serait plus simple‚ÄØ: on pourrait les optimiser
s√©par√©ment.


Ce n'est √©videmment pas le cas‚ÄØ: si on change $g$ pour que $g(x_0)$ soit plus proche de $y_0$, √ßa
changera aussi la valeur de $g(x_1)$.


**Mais on va faire comme si**

<!-- #region -->
C'est une approximation sauvage, mais apr√®s tout on commence √† avoir l'habitude. On va donc suivre
l'algo suivant

```python
def descent(train_set, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        for (x, y) in train_set:
            w = theta[:-1]
            b = theta[-1]
            # On calcule g(x)
            g_x = logistic(np.inner(w,x)+b)
            # On trouve la direction de plus grande pente
            steepest_direction = -grad_L(g_x, y)
            # On fait quelques pas dans cette direction
            theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Faites bien attention √† la diff√©rence‚ÄØ: au lieu d'attendre d'avoir calcul√© tous les
$\operatorname{grad}L(g(x_i), y_i)$ avant de modifier $Œ∏$, on va le modifier √† chaque fois.


- **Avantage**‚ÄØ: on modifie beaucoup plus souvent le param√®tre, si tout se passe bien, on devrait
  arriver √† une bonne approximation tr√®s vite.
- **Inconv√©nient**‚ÄØ: il se pourrait qu'en essayant de faire baisser $L(g(x_0), y_0)$, on fasse
  augmenter $L(g(x_1), y_1)$.

Notre espoir ici c'est que cette situation n'arrivera pas, et qu'on bon param√®tre pour un certain
couple $(x, y)$ c'est un bon param√®tres pour $tous$ les couples `(exemple, classe)`.


Ce nouvel algorithme s'appelle l'**algorithme de descente de gradient stochastique**, et il est
crucial pour nous, parce qu'on ne pourra en pratique quasiment jamais faire de descente de gradient
globale.


Il ne nous reste plus qu'√† savoir comment on calcule `grad_L`. On ne fera pas la preuve, mais on a


$$\frac{‚àÇL(g(x), y)}{‚àÇw_i} = (g(x)-y)x_i$$


et


$$\frac{‚àÇL(g(x), y)}{‚àÇb} = g(x)-y$$


Autrement dit on mettra √† jour $w$ en calculant

$$w ‚Üê w -Œ∑√ó\operatorname{d}_wL(g(x), y) = w - Œ∑√ó(g(x)-y)x$$


<small>$\operatorname{d}_wL(g(x), y) = \left(\frac{‚àÇL(g(x), y)}{‚àÇw_1}, ‚Ä¶, \frac{‚àÇL(g(x),
y)}{‚àÇw_n}\right)$ est la *diff√©rentielle partielle* de $L(g(x), y)$ par rapport √† $w$.</small>


Et $b$ en calculant

$$b ‚Üê b -Œ∑√ó\frac{‚àÇL(g(x), y)}{‚àÇb} = b - Œ∑√ó(g(x)-y)$$

<!-- #region -->
## üßê Exo üßê

### 1. Calculer le gradient

Reprendre la fonction qui calcule la fonction de co√ªt, et la transformer pour qu'elle renvoie le
gradient par rapport √† $w$ et la d√©riv√©e partielle par rapport √† $b$ en $(x, y)$.
<!-- #endregion -->

```python
def grad_L(x, w, b, y):
    grad = np.zeros(w.size+b.size)  # √Ä vous‚ÄØ!
    return grad

grad_L(np.array([5, 10]), np.array([0.6, -0.4]), np.array([-0.01]), 0)
```

### üßê Correction 1 üßê

```python
def grad_L(x, w, b, y):
    g_x = logistic(np.inner(w, x) + b)
    grad_w = (g_x - y)*x
    grad_b = g_x - y
    return np.append(grad_w, grad_b)
grad_L(np.array([5, 10]), np.array([0.6, -0.4]), np.array([-0.01]), 0)
```

### 2. Descendre le gradient

S'en servir pour apprendre les poids √† donner aux features pr√©c√©dentes √† l'aide du  [mini-corpus
IMDB](../../data/imdb_smol.tar.gz) en utilisant l'algorithme de descente de gradient stochastique.

```python
def descent(featurized_corpus, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        pass  # √Ä vous‚ÄØ!
    return 
descent(imdb_features, np.array([0.6, -0.4, 0.0]), 0.001, 100)
```

### üßê Correction 2 üßê


Avec du feedback pour voir ce qui se passe

```python
import random

def descent_with_logging(featurized_corpus, theta_0, learning_rate, n_steps):
    train_set = [
        *((doc, 1) for doc in featurized_corpus["pos"]),
        *((doc, 0) for doc in featurized_corpus["neg"])
    ]
    theta = theta_0
    w = theta[:-1]
    b = theta[-1]
    print(f"Initial loss: {loss_on_imdb(w, b, featurized_corpus)}")
    
    for i in range(n_steps):
        # On m√©lange le corpus pour s'assurer de ne pas avoir d'abord tous
        # les positifs puis tous les n√©gatifs
        random.shuffle(train_set)
        for j, (x, y) in enumerate(train_set):
            grad = grad_L(x, w, b, y)
            steepest_direction = -grad
            # Purement pour l'affichage
            loss = logistic_negative_log_likelihood(x, w, b, y)
            #print(f"step {i*len(train_set)+j} doc={x}\tw={w}\tb={b}\tloss={loss}\tgrad={grad}")
            theta += learning_rate*steepest_direction
            w = theta[:-1]
            b = theta[-1]
        print(f"Epoch {i} loss: {loss_on_imdb(w, b, featurized_corpus)}\tw={w}\tb={b}")
    return (theta[:-1], theta[-1])

descent_with_logging(imdb_features, np.array([0.6, -0.4, -0.01]), 0.1, 100)
```

Un peu de visu suppl√©mentaire

```python
def make_vector_corpus(featurized_corpus):
    vector_corpus = np.stack([*featurized_corpus["pos"], *featurized_corpus["neg"]])
    vector_target = np.concatenate([np.ones(len(featurized_corpus["pos"])), np.zeros(len(featurized_corpus["neg"]))])
    return vector_corpus, vector_target

vector_corpus, vector_target = make_vector_corpus(imdb_features)
```

```python
w1 = np.linspace(-50, 50, 200)
w2 = np.linspace(-50, 50, 200)
W1, W2 = np.meshgrid(w1, w2)
W = np.stack((W1, W2), axis=-1)
confidence = logistic(
    np.einsum("ijn,kn->ijk", W, vector_corpus)
)
broadcastable_target = vector_target[np.newaxis, np.newaxis, :]
loss = -np.log(confidence * broadcastable_target + (1-confidence)*(1-broadcastable_target)).sum(axis=-1)
fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

surf = ax.plot_surface(W1, W2, loss, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
fig.colorbar(surf, shrink=0.5, aspect=5)
ax.plot_wireframe(W1, W2, loss, color='black')

plt.title("Paysage de la fonction de co√ªt en fonction des valeurs de $w$")

plt.show()
```

## R√©gression multinomiale

Un dernier point‚ÄØ: on a vu dans tout ceci comment utiliser la r√©gression logistique pour un probl√®me
de classification √† deux classes. Comment on l'√©tend √† $n$ classes‚ÄØ?


R√©flechissons d√©j√† √† quoi ressemblerait la sortie d'un tel classifieur‚ÄØ:

Pour un probl√®me √† deux classes, le classifieur $g$ nous donne pour chaque exemple $x$ une
estimation $g(x)$ de la vraisemblance de la classe $1$, et on a vu que la vraisemblance de la classe
$0$ √©tait n√©cessairement $1-g(x)$ pour que la somme des vraisemblances fasse 1.


On peut le pr√©senter autrement‚ÄØ: consid√©rons le classifieur $f$ tel que pour tout exemple $x$

$$f(x) = (1-g(x), g(x))$$

$f$ nous donne un vecteur √† deux coordonn√©es, $f_0(x)$ et $f_1(x)$, qui sont respectivement les
vraisemblances des classes $0$ et $1$.


Pour un probl√®me √† $n$ classes, on va vouloir une vraisemblance par classe, on va donc proc√©der de
la fa√ßon suivante‚ÄØ:

On consid√®re des poids $(w_1, b_1), ‚Ä¶, (w_n, b_n)$. Ils d√©finissent un classifieur lin√©aire.

En effet, si on consid√®re les $z_i$ d√©finis pour tout exemple $x$ par

$$
    \begin{cases}
        z_1 = w_1‚ãÖx + b_1\\
        \vdots\\
        z_n = w_n‚ãÖx + b_1
    \end{cases}
$$

On peut choisir la classe $y$ √† affecter √† $x$ en prenant $y=\operatorname{argmax}\limits_i z_i$


Il reste √† normaliser pour avoir des vraisemblances. Pour √ßa on utilise une fonction **tr√®s**
importante‚ÄØ: la fonction $\operatorname{softmax}$, d√©finie ainsi‚ÄØ:

$$\operatorname{softmax}(z_1, ‚Ä¶, z_n) = \left(\frac{e^{z_1}}{\sum_i e^{z_i}}, ‚Ä¶,
\frac{e^{z_n}}{\sum_i e^{z_i}}\right)$$

Contrairement √† la fonction logistique qui prenait un nombre en entr√©e et renvoyait un nombre,
$\operatorname{softmax}$ prend en entr√©e un **vecteur** non-normalis√© et renvoie un vecteur
normalis√©.


On d√©finit enfin le classifieur logistique multinomial $f$ de la fa√ßon suivante‚ÄØ: pour tout exemple
$x$, on a

$$f(x) = \operatorname{softmax}(w_1‚ãÖx+b_1, ‚Ä¶, w_n‚ãÖx+b_n) = \left(\frac{e^{w_1‚ãÖx+b_1}}{\sum_i
e^{w_i‚ãÖx+b_i}}, ‚Ä¶, \frac{e^{w_n‚ãÖx+b_n}}{\sum_i e^{w_i‚ãÖx+b_i}}\right)$$

et on choisit pour $x$ la classe

$$y = \operatorname{argmax}\limits_i f_i(x) = \operatorname{argmax}\limits_i
\frac{e^{w_i‚ãÖx+b_i}}{\sum_j e^{w_j‚ãÖx+b_j}}$$

Comme la fonction exponentielle est croissante, ce sera la m√™me classe que le classifieur lin√©aire
pr√©c√©dent. Comme pour le cas √† deux classe, la diff√©rence se fera lors de l'apprentissage. Je vous
laisse aller en lire les d√©tails dans *Speech and Language Processing*, mais l'id√©e est la m√™me‚ÄØ: on
utilise la log-vraisemblance n√©gative de la classe correcte comme fonction de co√ªt, et on optimise
les param√®tres avec l'algo de descente de gradient stochastique.


Un dernier d√©tail‚ÄØ?

Qu'est-ce qui se passe si on prend ce qu'on vient de voir pour $n=2$‚ÄØ? Est-ce qu'on retombe sur le
cas √† deux classe vu pr√©c√©demment‚ÄØ?


Oui, regarde‚ÄØ: dans ce cas

$$
    \begin{align}
        f_1(x)
            &= \frac{e^{w_1‚ãÖx+b_1}}{e^{w_0‚ãÖx+b_0}+e^{w_1‚ãÖx+b_1}}\\
            &= \frac{1}{
                \frac{e^{w_0‚ãÖx+b_0}}{e^{w_1‚ãÖx+b_1}} + 1
            }\\
            &= \frac{1}{e^{(w_0‚ãÖx+b_0)-(w_1‚ãÖx+b_1)} + 1}\\
            &= \frac{1}{1 + e^{(w_0-w_1)‚ãÖx+(b_0-b_1)}}\\
            &= œÉ((w_0-w_1)‚ãÖx+(b_0-b_1))
    \end{align}
$$


Autrement dit, appliquer ce qu'on vient de voir pour le cas multinomial, si $n=2$, c'est comme
appliquer ce qu'on a vu pour deux classes, avec $w=w_0-w_1$ et $b=b_0-b_1$.

## La suite

Vous √™tes arriv√©‚ãÖe‚ãÖs au bout de ce cours et vous devriez avoir quelques id√©es de plusieurs concepts
importants‚ÄØ:

- Le concept de classifieur lin√©aire
- Le concept de fonction de co√ªt
- L'algorithme de descente de gradient stochastique
- La fonction softmax

On reparlera de tout √ßa en temps utile. Pour la suite de vos aventures au pays des classifieurs
logistiques, je vous recommande plut√¥t d'utiliser [leur impl√©mentation dans
scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).
Maintenant que vous savez comment √ßa marche, vous pouvez le faire la t√™te haute. Bravo‚ÄØ!

<small>Vous avez aussi d√©couvert les premiers r√©seaux de neurones de ce cours et ce n'est pas
rien‚ÄØ!</small>
