---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 10‚ÄØ: R√©gression logistique
===============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-27
<!-- #endregion -->

```python
from IPython.display import display
```

## Classifieur lin√©aire

On consid√®re des vecteurs de *features* de dimension $n$

$$\mathbf{x} = (x‚ÇÅ, ‚Ä¶, x_n)$$

Un vecteur de poids de dimension $n$

$$\mathbf{w} = (w‚ÇÅ, ‚Ä¶, w_n)$$

et un biais $b$ scalaire (un nombre quoi).

Pour r√©aliser une classification on consid√®re le nombre $z$ (on parle parfois de *logit*)

$$z=w‚ÇÅ√óx‚ÇÅ + ‚Ä¶ + w_n√óx_n + b = \sum_iw_ix_i + b$$

Ce qu'on note aussi

$$z = \mathbf{w}‚ãÖ\mathbf{x}+b$$

$\mathbf{w}‚ãÖ\mathbf{x}$ se lit ¬´‚ÄØw scalaire x‚ÄØ¬ª, on parle de *produit scalaire* en fran√ßais et de *inner product* en anglais.

(ou pour les math√©maticien‚ãÖne‚ãÖs acharn√©‚ãÖe‚ãÖs $z = \langle w\ |\ x \rangle + b$)

Quelle que soit la fa√ßon dont on le note, on affectera √† $\mathbf{x}$ la classe $0$ si $z < 0$ et la classe $1$ sinon.

## üò¥ Exo üò¥

√âcrire une fonction qui prend en entr√©e un vecteur de features et un vecteur de poids sous forme de
tableaux numpy $x$ et $w$ de dimensions `(n,)` et un biais $b$ sous forme d'un tableau numpy de
dimensions `(1,)` et renvoie $z=\sum_iw_ix_i + b$.

```python
import numpy as np
```

```python
def affine_combination(x, w, b):
    pass # √Ä vous de jouer‚ÄØ!

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

```python
def affine_combination(x, w, b):
    res = np.zeros(1)
    for wi, xi in zip(w, x):
        res += wi*xi
    res += b
    return res

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

```python
def affine_combination(x, w, b):
    return np.inner(w, x) + b

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

## üß† Exo üß†

1\. √Ä l'aide d'un lexique de sentiment (par exemple
[VADER](https://github.com/cjhutto/vaderSentiment)), √©crivez une fonction qui prend en entr√©e un
texte en anglais et renvoie sa repr√©sentation sous forme d'un vecteur de features √† deux traits‚ÄØ:
nombre de mots positifs et nombre de mot n√©gatifs.

```python
import re

def poor_mans_tokenizer_and_normalizer(s):
    return [w.lower() for w in re.split(r"\s|\W", s.strip()) if w and all(c.isalpha() for c in w)]

def read_vader(vader_path):
    res = dict()
    with open(vader_path) as in_stream:
        for row in in_stream:
            word, polarity, *_ = row.lstrip().split("\t", maxsplit=2)
            is_positive = float(polarity) > 0
            res[word] = is_positive
    return res

def featurize(text, lexicon):
    words = poor_mans_tokenizer_and_normalizer(text)
    features = np.empty(2)
    features[0] = sum(1 for w in words if lexicon.get(w, False))
    features[1] = sum(1 for w in words if not lexicon.get(w, True))
    return features

lexicon = read_vader("../../data/vader_lexicon.txt")
doc = "I came in in the middle of this film so I had no idea about any credits or even its title till I looked it up here, where I see that it has received a mixed reception by your commentators. I'm on the positive side regarding this film but one thing really caught my attention as I watched: the beautiful and sensitive score written in a Coplandesque Americana style. My surprise was great when I discovered the score to have been written by none other than John Williams himself. True he has written sensitive and poignant scores such as Schindler's List but one usually associates his name with such bombasticities as Star Wars. But in my opinion what Williams has written for this movie surpasses anything I've ever heard of his for tenderness, sensitivity and beauty, fully in keeping with the tender and lovely plot of the movie. And another recent score of his, for Catch Me if You Can, shows still more wit and sophistication. As to Stanley and Iris, I like education movies like How Green was my Valley and Konrack, that one with John Voigt and his young African American charges in South Carolina, and Danny deVito's Renaissance Man, etc. They tell a necessary story of intellectual and spiritual awakening, a story which can't be told often enough. This one is an excellent addition to that genre."
doc_features = featurize(doc, lexicon)
doc_features
```

2\. Appliquer la fonction pr√©c√©dente sur [le mini-corpus IMDB](../../data/imdb_smol.tar.gz)


Commen√ßons par l'extraire

```bash
cd ../../local
tar -xzf ../data/imdb_smol.tar.gz 
ls -lah imdb_smol
```

Maintenant on parcourt le dossier pour construire nos repr√©sentations

```python
from collections import defaultdict
import pathlib  # Manipuler des chemins et des fichiers agr√©ablement

def featurize_dir(corpus_root, lexicon):
    corpus_root = pathlib.Path(corpus_root)
    res = defaultdict(list)
    for clss in corpus_root.iterdir():
        # On peut aussi utiliser une compr√©hension de liste et avoir un dict pas default
        for doc in clss.iterdir():
            # `stem` et `read_text` c'est de la magie de `pathlib`, check it out
            res[clss.stem].append(featurize(doc.read_text(), lexicon))
    return res

# On r√©utilise le lexique pr√©c√©dent
imdb_features = featurize_dir("../../local/imdb_smol", lexicon)
imdb_features
```

3\. √âcrire un classifieur logistique qui prend en entr√©e les vecteurs de features
pr√©c√©dents et utilise les poids respectifs $0.6$ et $-0.4$ et un biais de $0$. Appliquez ce
classifieur sur le mini-corpus IMDB et calculez son exactitude.


On commence par d√©finir le classifieur‚ÄØ: on va renvoyer `True`¬†pour la classe positive et `False`
pour la classe n√©gative.


```python
def hardcoded_classifier(x):
    return (
        np.inner(
            np.array([0.6, -0.4]),
            x,
        )
        > 0.0
    )


hardcoded_classifier(doc_features)
```

Maintenant on le teste

```python
correct_pos = sum(1 for doc in imdb_features["pos"] if hardcoded_classifier(doc))
print(f"Recall for 'pos': {correct_pos}/{len(imdb_features['pos'])}={correct_pos/len(imdb_features['pos']):.02%}")
correct_neg = sum(1 for doc in imdb_features["neg"] if not hardcoded_classifier(doc))
print(f"Recall for 'neg': {correct_neg}/{len(imdb_features['neg'])}={correct_neg/len(imdb_features['neg']):.02%}")
print(f"Accuracy: {correct_pos+correct_neg}/{len(imdb_features['pos'])+len(imdb_features['neg'])}={(correct_pos+correct_neg)/(len(imdb_features['pos'])+len(imdb_features['neg'])):.02%}")
```


## La fonction logistique


$$œÉ(z) = \frac{1}{1 + e^{‚àíz}} = \frac{1}{1 + \exp(‚àíz)}$$

Elle permet de *normaliser* $z$‚ÄØ: $z$ peut √™tre n'importe quel nombre entre $-‚àû$ et $+‚àû$, mais on
aura toujours $0 <¬†œÉ(z) < 1$, ce qui permet de l'interpr√©ter facilement comme une *vraisemblance*.
Autrement dit, $œÉ(z)$ sera proche de $1$ s'il para√Æt vraisemblable que $x$ appartienne √† la classe
$1$ et proche de $0$ sinon.

## üìà Exo üìà

Tracer avec matplotlib la courbe repr√©sentative de la fonction logistique.

```python
def logistic(z):
    return 1/(1+np.exp(-z))
```

```python
%matplotlib inline
import matplotlib.pyplot as plt
x = np.linspace(-10, 10, 5000)
y = logistic(x)
plt.plot(x, y)
plt.xlabel("$x$")
plt.ylabel("$œÉ(x)$")
plt.title("Courbe repr√©sentative de la fonction logistique sur $[-10, 10]$")
plt.show()
```

## R√©gression logistique

Formellement‚ÄØ: on suppose qu'il existe une fonction $f$ qui pr√©dit parfaitement les classes, donc
telle que pour tout couple exemple/√©tiquette $(x, y)$ avec $y$ valant $0$ ou $1$, $f(x) = y$. On
approcher cette fonction par une fonction $g$ de la forme

$$g(x) = œÉ(w‚ãÖx+b)$$

Si on choisit les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur
notre ensemble d'apprentissage, on dit que $g$ est la *r√©gression logistique de $f$* sur cet
ensemble.

Un classifieur logistique, c'est simplement un classifieur qui pour un exemple $x$ renvoie $0$ si
$g(x) < 0.5$ et $1$ sinon. Il a exactement les m√™mes capacit√©s de discrimination qu'un classifieur
lin√©aire (il ne sait pas prendre de d√©cisions plus complexes), mais on peut interpr√©ter la confiance
qu'il a dans sa d√©cision.


Par exemple voici la confiance que notre classifieur cod√© en dur a en ses d√©cisions

```python
def classifier_confidence(x):
    return logistic(
        np.inner(
            np.array([0.6, -0.4]),
            x,
        )
    )


classifier_confidence(doc_features)
```


Autrement dit, d'apr√®s notre classifieur, il y a un peu plus de $99.86\%$ de chances que le document
soit de la classe $1$ (review positive dans ce cas), ou encore, la vraisemblance de la classe $1$
est vraisemblable √† plus de $99.68\%$.


Quelle est la vraisemblance de la classe $0$ (review n√©gative)‚ÄØ? Et bien le reste

```python
1.0 - classifier_confidence(doc_features)
```

Soit un peu plus de $0.1%$.


Comme l'exemple en question appartient bien √† cette classe, √ßa signifie que notre classifieur et
plut√¥t bon **sur cet exemple**. L'est-il sur le reste du corpus‚ÄØ?

```python
pos_confidence = sum(classifier_confidence(doc) for doc in imdb_features["pos"])
print(f"Average confidence for 'pos': {pos_confidence/len(imdb_features['pos']):.02%}")
neg_confidence = sum(1-classifier_confidence(doc) for doc in imdb_features["neg"])
print(f"Average confidence for 'neg': {neg_confidence/len(imdb_features['neg']):.02%}")
print(f"Average confidence for the correct class: {(pos_confidence+neg_confidence)/(len(imdb_features['pos']) + len(imdb_features['neg'])):.02%}")
```

Autrement dit, pour un exemple pris au hasard dans le corpus, la vraisemblance de sa classe telle
que jug√©e par le classifieur sera de $59.47\%$. Un classifieur parfait obtiendrait $100\%$, un
classifieur qui prendrait syst√©matiquement la mauvaise d√©cision $0\%$ et un classifieur al√©atoire
uniforme $50\%$ (puisque notre corpus a autant d'exemples de chaque classe).


Moralit√© nos poids ne sont pas tr√®s bien choisis, et notre pr√©occupation dans la suite va √™tre de
chercher comment choisir des poids pour que la confiance moyenne de la classe correcte soit aussi
haute que possible.

## Fonction de co√ªt

On a dit que notre objectif √©tait

> Chercher les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur
notre ensemble d'apprentissage

On formalise ¬´‚ÄØ√™tre le plus proche possible‚ÄØ¬ª de la section pr√©c√©dente comme minimiser une certaine
fonction de co√ªt (*loss*) $L$ qui mesure l'erreur faite par le classifieur sur un exemple.

$$L(g(x), y) = \text{l'√©cart entre la classe pr√©dite par $g$ pour $x$ et la classe correcte $y$}$$

√âtant donn√© un ensemble de test $(x‚ÇÅ, y‚ÇÅ),‚ÄØ‚Ä¶, (x_n, y_n)$, on estime l'erreur faite par le
classifieur logistique $g$ pour chaque exemple $(x_i, y_i)$ comme le co√ªt local $L(g(x·µ¢), y·µ¢)$ et
son erreur sur tout l'ensemble de test par le co√ªt global $\mathcal{L}$‚ÄØ:

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

Plus $\mathcal{L}$ sera bas, meilleur sera notre classifieur.

Dans le cas de la r√©gression logistique, on va s'inspirer de ce qu'on a vu dans la section
pr√©c√©dente et utiliser la *log-vraisemblance n√©gative* (*negative log-likelihood*)‚ÄØ:

On d√©finit la *vraisemblance* $V$ comme pr√©c√©demment par
$$
V(a, y) =
    \begin{cases}
        a & \text{si $y = 1$}\\
        1-a & \text{sinon}
    \end{cases}
$$

Intuitivement, il s'agit de la vraisemblance affect√©e par le mod√®le √† la classe correcte $y$. Il ne
s'agit donc pas d'un co√ªt, mais d'un *gain* (si sa valeur est haute, c'est que le mod√®le est bon)

La *log-vraisemblance n√©gative* $L$ est alors d√©finie par

$$L(a, y) = -\log(V(a, y))$$

Le $\log$ est l√† pour plusieurs raisons, calculatoires et th√©oriques<sup>1</sup> et le $-$ √†
s'assurer qu'on a bien un co√ªt (plus la valeur est basse, meilleur le mod√®le est).

<small>1. Entre autres, comme pour *Na√Øve Bayes*, parce qu'une somme de $\log$-vraisemblance peut
√™tre vue comme le $\log$ de la probabilit√© d'une conjonction d'√©v√©nements ind√©pendants. Mais surtout
parce qu'il rend la fonction de co√ªt **convexe** par rapport √† $w$</small>.

Une interpr√©tation possible‚ÄØ: $L(a, y)$, c'est la
[surprise](https://en.wikipedia.org/wiki/Information_content) de $y$ au sens de la th√©orie de
l'information. Autrement dit‚ÄØ: si j'estime qu'il y a une probabilit√© $a$ d'observer la classe $y$,
$L(a, y)$ mesure √† quel point il serait surprenant d'observer effectivement $y$.


On peut v√©rifier qu'il s'agit bien d'un co√ªt‚ÄØ:

- C'est un nombre positif
- Si le classifieur prend une d√©cision correcte avec une confiance parfaite le co√ªt est nul‚ÄØ:

  $$
    \begin{cases}
        L(1.0, 1) = -\log(1.0) = 0\\
        L(0.0, 0) = -\log(1.0-0.0) = -\log(1.0) = 0
    \end{cases}
  $$
- Si le classifieur prend une d√©cision erron√©e avec une confiance parfaite le co√ªt est infini‚ÄØ:

  $$
    \begin{cases}
        L(0.0, 1) = -\log(0.0) = +\infty\\
        L(1.0, 0) = -\log(1.0-1.0) = \log(0.0) = +\infty
    \end{cases}
  $$

On peut aussi v√©rifier facilement que $L(a, 1)$ est d√©croissant par rapport √† $a$ et que $L(1-a, 0)$
est croissant par rapport √† $a$. Autrement dit, plus le classifieur juge que la classe correcte est
vraisemblable plus le co√ªt $L$ est bas.


Enfin, on peut l'√©crire $L$ en une ligne‚ÄØ: pour un exemple $x$, le co√ªt de l'exemple $(x, y)$ est

$$L(g(x), y) = -\log\left[g(x)√óy + (1-g(x))√ó(1-y)\right]$$

C'est un *trick*, l'astuce c'est que comme $y$ vaut soit $0$ soit $1$, soit $y=0$, soit $1-y=0$ et
donc la somme dans le $\log$ se simplifie dans tous les cas. Rien de transcendant l√†-dedans.

La formule diff√®re un peu de celle de *Speech and Language Processing*, mais les r√©sultats sont les
m√™mes et celle-ci est mieux pour notre probl√®me‚ÄØ!

<small>En fait la leur est la formule g√©n√©rale de l'entropie crois√©e pour des distributions de proba
√† support dans $\{0, 1\}$, ce qui est une autre intuition pour cette fonction de co√ªt, mais ici elle
nous complique la vie.</small>

Une derni√®re fa√ßon de l'√©crire en une ligne‚ÄØ:

$$L(g(x), y) = -\log\left[g(x)\mathbb{1}_{y=1} + (1-g(x))\mathbb{1}_{y=0}\right]$$

## üìâ Exo üìâ

√âcrire une fonction qui prend en entr√©e

- Un vecteur de features $x$ de taille $n$
- Un vecteur de poids $w$ de taille $n$ et un biais $b$ (de taille $1$)
- Une classe cible $y$ ($0$ ou $1$)

Et renvoie la log-vraisemblance n√©gative du classifieur logistique de poids $(w, b)$ pour l'exemple
$(x, y)$.

Servez-vous en pour calculer le co√ªt du classifieur de l'exercise pr√©c√©dent sur le mini-corpus IMDB.

<!-- #region -->
## Descente de gradient

### Principe g√©n√©ral

L'**algorithme de descente de gradient** est la cl√© de voute de l'essentiel des travaux
en apprentissage artificiel moderne. Il s'agit d'un algorithme it√©ratif qui √©tant donn√© un mod√®le
param√©tris√© et une fonction de co√ªt (avec des hypoth√®ses de r√©gularit√© assez faibles) permet de
trouver des valeurs des param√®tres pour lesquelles la fonction de co√ªt est minimal.

On ne va pas rentrer dans les d√©tails de l'algorithme de descente de gradient stochastique, mais
juste essayer de se donner quelques id√©es.

L'intuition √† avoir est la suivante‚ÄØ: si vous √™tes dans une vall√©e et que vous voulez trouver
rapidement le point le plus bas, une fa√ßon de faire est de chercher la direction vers laquelle la
pente descend le plus vite, de faire quelques pas dans cette direction puis de recommencer. On parle
aussi pour cette raison d'**algorithme de la plus forte pente**.

Clairement une condition pour que √ßa marche peu importe le point de d√©part, c'est que la vall√©e
n'ait qu'un seul point localement le plus bas. Par exemple √ßa marche avec une vall√©e comme celle-ci
<!-- #endregion -->

```python
%matplotlib inline
import tol_colors as tc
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = R**2 - 1

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

Mais pas pour celle-l√†

```python
%matplotlib inline
import tol_colors as tc
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = -np.cos(R)/(1+0.5*R**2)

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
#ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

OK, mais comment on trouve la plus forte pente en pratique‚ÄØ? En une dimension il suffit de suivre
l'oppos√© du nombre d√©riv√©‚ÄØ: <https://uclaacm.github.io/gradient-descent-visualiser/#playground>


En plus de dimensions, c'est plus compliqu√©, mais on peut s'en sortir en suivant le *gradient* qui
est une g√©n√©ralisation du nombre d√©riv√©‚ÄØ: <https://jackmckew.dev/3d-gradient-descent-in-python.html>


Ce qui fait marcher la machine c'est que **le gradient indique la direction dans laquelle la
fonction cro√Æt le plus vite**. Et que l'oppos√© du gradient indique la direction dans laquelle la
fonction d√©cro√Æt le plus vite.

(localement)

<!-- #region -->
Concr√®tement si on veut trouver $\theta$ tel que $f(\theta)$ soit minimale pour une certaine
fonction $f$ dont le gradient est donn√© par `grad_f` √ßa donne l'algo suivant

```python
def descent(grad_f, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        # On trouve la direction de plus grande pente
        steepest_direction = -grad_f(theta)
        #¬†On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
    return theta
```

Les *hyperparam√®tres* sont

- `theta_0` est notre point de d√©part, notre premi√®re estimation d'o√π se trouvera le minimum, que
  l'algorithme va raffiner. √âvidemment si on a d√©j√† une id√©e de vers o√π on pourrait le trouver, √ßa
  ira plus vite. Si on a aucune id√©e, on peut le prendre al√©atoire.
- `learning_rate` ou ¬´‚ÄØtaux d'apprentissage‚ÄØ¬ª‚ÄØ: de combien on se d√©place √† chaque √©tape. Si on le
  prend grand on arrive vite vers la r√©gion du minimum, on mettra longtemps pour en trouver une
  approximation pr√©cise. Si on le prend petit, √ßa sera l'inverse.
- `n_steps` est le nombre d'√©tapes d'optimisations. Dans un probl√®me d'apprentissage, c'est aussi
  le nombre de fois o√π on aura parcouru l'ensemble d'apprentissage et on parle souvent d'**epoch**

Ici on se donne un nombre fixe¬†d'epochs, une autre possibilit√© serait de s'arr√™ter quand on ne bouge
plus trop, par exemple avec une condition comme

```python
if np.max(grad_f(theta)) < 0.00001:
    break
```

dans la boucle et √©ventuellement avec une boucle infinie `while True`.
<!-- #endregion -->

Point notation‚ÄØ:

- **Le gradient de $f$** est souvent not√© $\nabla f$ ou $\operatorname{grad}f$, voire $\vec\nabla f$
  ou $\overrightarrow{\operatorname{grad}} f$ (pour dire que c'est un vecteur)
- Si $Œ∏=(Œ∏_1, ‚Ä¶, Œ∏_n)$, autrement dit si $f$ est une fonction de $n$ variables, on note
  $\operatorname{grad}f = \left(\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_1}, ‚Ä¶, \frac{‚àÇf(Œ∏)}{‚àÇŒ∏_n}\right)$. Autrement dit
  $\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_i}$, la **d√©riv√©e partielle** de $f(Œ∏)$ par rapport √† $Œ∏_i$ est la $i$-√®me
  coordonn√©es du gradient de $f$.
- **Le taux d'apprentissage** est souvent not√© $Œ±$ ou $Œ∑$


### Descente de gradient stochastique

Rappelez-vous, on a dit que notre fonction de co√ªt, c'√©tait

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

et on cherche la valeur du param√®tre $Œ∏ = (w_1, ‚Ä¶, w_n, b)$ tel que $\mathcal{L}$ soit le plus petit
possible.


On peut utilise la propri√©t√© d'**additivit√©** du gradient‚ÄØ: pour deux fonctions $f$ et $g$, on a

$$\operatorname{grad}(f+g) = \operatorname{grad}f + \operatorname{grad}g$$

Donc ici

$$\operatorname{grad}\mathcal{L} = \sum_i \operatorname{grad}L(g(x·µ¢), y·µ¢)$$

<!-- #region -->
Si on dispose d'une fonction `grad_L` qui, √©tant donn√©s $g(x_i)$ et $y_i$, renvoie
$\operatorname{grad}L(g(x_i), y_i)$, l'algorithme de descente du gradient devient alors

```python
def descent(train_set, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        w = theta[:-1]
        b = theta[-1]
        partial_grads = []
        for (x, y) in train_set:
            #¬†On calcule g(x)
            g = sigmoid(np.inner(w*x+b))
            #¬†On calcule le gradient de L(g(x), y))
            partial_grads.append(grad_L(g, y))
        # On trouve la direction de plus grande pente
        steepest_direction = -np.sum(partial_grads)
        # On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Pour chaque √©tape, on doit calculer tous les $g(x_i)$ et $\operatorname{grad}L(g(x_i), y_i)$. C'est
tr√®s couteux, il doit y avoir moyen de faire mieux.


Si les $L(g(x·µ¢), y·µ¢)$ √©taient ind√©pendants, ce serait plus simple‚ÄØ: on pourrait les optimiser
s√©par√©ment.


Ce n'est √©videmment pas le cas‚ÄØ: si on change $g$ pour que $g(x_0)$ soit plus proche de $y_0$, √ßa
changera aussi la valeur de $g(x_1)$.


**Mais on va faire comme si**

<!-- #region -->
C'est une approximation sauvage, mais apr√®s tout on commence √† avoir l'habitude. On va donc suivre
l'algo suivant

```python
def descent(train_set, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        for (x, y) in train_set:
            w = theta[:-1]
            b = theta[-1]
            # On calcule g(x)
            g = sigmoid(np.inner(w*x+b))
            # On trouve la direction de plus grande pente
            steepest_direction = -grad_L(g, y)
            # On fait quelques pas dans cette direction
            theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Faites bien attention √† la diff√©rence‚ÄØ: au lieu d'attendre d'avoir calcul√© tous les
$\operatorname{grad}L(g(x_i), y_i)$ avant de modifier $Œ∏$, on va le modifier √† chaque fois.


- **Avantage**‚ÄØ: on modifie beaucoup plus souvent le param√®tre, si tout se passe bien, on devrait
  arriver √† une bonne approximation tr√®s vite.
- **Inconv√©nient**‚ÄØ: il se pourrait qu'en essayant de faire baisser $L(g(x_0), y_0)$, on fasse
  augmenter $L(g(x_1), y_1)$.

Notre espoir ici c'est que cette situation n'arrivera pas, et qu'on bon param√®tre pour un certain
couple $(x, y)$ c'est un bon param√®tres pour $tous$ les couples `(exemple, classe)`.


Ce nouvel algorithme s'appelle l'**algorithme de descente de gradient stochastique**, et il est
crucial pour nous, parce qu'on ne pourra en pratique quasiment jamais faire de descente de gradient
globale.


Il ne nous reste plus qu'√† savoir comment on calcule `grad_L`. On ne fera pas la preuve, mais on a


$$\frac{‚àÇL(g(x), y)}{‚àÇw_i} = (g(x)-y)x_i$$


et


$$\frac{‚àÇL(g(x), y)}{‚àÇb} = g(x)-y$$


Autrement dit on mettra √† jour $w$ en calculant

$$w ‚Üê w -Œ∑√ó\operatorname{d}_wL(g(x), y) = w - Œ∑√ó(g(x)-y)x$$


<small>$\operatorname{d}_wL(g(x), y) = \left(\frac{‚àÇL(g(x), y)}{‚àÇw_1}, ‚Ä¶, \frac{‚àÇL(g(x),
y)}{‚àÇw_n}\right)$ est la *diff√©rentielle partielle* de $L(g(x), y)$ par rapport √† $w$.</small>


Et $b$ en calculant

$$b ‚Üê b -Œ∑√ó\frac{‚àÇL(g(x), y)}{‚àÇb} = b - Œ∑√ó(g(x)-y)$$

## üßê Exo üßê

1\. Reprendre la fonction qui calcule la fonction de co√ªt, mais faire en sorte qu'elle renvoie √©galement
le gradient par rapport √† $w$ et la d√©riv√©e partielle par rapport √† $b$ en $(x, y)$.


2\. S'en servir pour apprendre les poids √† donner aux features pr√©c√©dentes √† l'aide du  [mini-corpus
IMDB](../../data/imdb_smol.tar.gz) en utilisant l'algorithme de descente de gradient stochastique.

## R√©gression multinomiale

Un dernier point‚ÄØ: on a vu dans tout ceci comment utiliser la r√©gression logistique pour un probl√®me
de classification √† deux classes. Comment on l'√©tend √† $n$ classes‚ÄØ?


R√©flechissons d√©j√† √† quoi ressemblerait la sortie d'un tel classifieur‚ÄØ:

Pour un probl√®me √† deux classes, le classifieur $g$ nous donne pour chaque exemple $x$ une
estimation $g(x)$ de la vraisemblance de la classe $1$, et on a vu que la vraisemblance de la classe
$0$ √©tait n√©cessairement $1-g(x)$ pour que la somme des vraisemblances fasse 1.


On peut le pr√©senter autrement‚ÄØ: consid√©rons le classifieur $f$ tel que pour tout exemple $x$

$$f(x) = (1-g(x), g(x))$$

$f$ nous donne un vecteur √† deux coordonn√©es, $f_0(x)$ et $f_1(x)$, qui sont respectivement les
vraisemblances des classes $0$ et $1$.


Pour un probl√®me √† $n$ classes, on va vouloir une vraisemblance par classe, on va donc proc√©der de
la fa√ßon suivante‚ÄØ:

On consid√®re des poids $(w_1, b_1), ‚Ä¶, (w_n, b_n)$. Ils d√©finissent un classifieur lin√©aire.

En effet, si on consid√®re les $z_i$ d√©finis pour tout exemple $x$ par

$$
    \begin{cases}
        z_1 = w_1‚ãÖx + b_1\\
        \vdots\\
        z_n = w_n‚ãÖx + b_1
    \end{cases}
$$

On peut choisir la classe $y$ √† affecter √† $x$ en prenant $y=\operatorname{argmax}\limits_i z_i$


Il reste √† normaliser pour avoir des vraisemblances. Pour √ßa on utilise une fonction **tr√®s**
importante‚ÄØ: la fonction $\operatorname{softmax}$, d√©finie ainsi‚ÄØ:

$$\operatorname{softmax}(z_1, ‚Ä¶, z_n) = \left(\frac{e^{z_1}}{\sum_i e^{z_i}}, ‚Ä¶,
\frac{e^{z_n}}{\sum_i e^{z_i}}\right)$$

Contrairement √† la fonction logistique qui prenait un nombre en entr√©e et renvoyait un nombre,
$\operatorname{softmax}$ prend en entr√©e un **vecteur** non-normalis√© et renvoie un vecteur
normalis√©.


On d√©finit enfin le classifieur logistique multinomial $f$ de la fa√ßon suivante‚ÄØ: pour tout exemple
$x$, on a

$$f(x) = \operatorname{softmax}(w_1‚ãÖx+b_1, ‚Ä¶, w_n‚ãÖx+b_n) = \left(\frac{e^{w_1‚ãÖx+b_1}}{\sum_i
e^{w_i‚ãÖx+b_i}}, ‚Ä¶, \frac{e^{w_n‚ãÖx+b_n}}{\sum_i e^{w_i‚ãÖx+b_i}}\right)$$

et on choisit pour $x$ la classe

$$y = \operatorname{argmax}\limits_i f_i(x) = \operatorname{argmax}\limits_i
\frac{e^{w_i‚ãÖx+b_i}}{\sum_j e^{w_j‚ãÖx+b_j}}$$

Comme la fonction exponentielle est croissante, ce sera la m√™me classe que le classifieur lin√©aire
pr√©c√©dent. Comme pour le cas √† deux classe, la diff√©rence se fera lors de l'apprentissage. Je vous
laisse aller en lire les d√©tails dans *Speech and Language Processing*, mais l'id√©e est la m√™me‚ÄØ: on
utilise la log-vraisemblance n√©gative de la classe correcte comme fonction de co√ªt, et on optimise
les param√®tres avec l'algo de descente de gradient stochastique.


Un dernier d√©tail‚ÄØ?

Qu'est-ce qui se passe si on prend ce qu'on vient de voir pour $n=2$‚ÄØ? Est-ce qu'on retombe sur le
cas √† deux classe vu pr√©c√©demment‚ÄØ?


Oui, regarde‚ÄØ: dans ce cas

$$
    \begin{align}
        f_1(x)
            &= \frac{e^{w_1‚ãÖx+b_1}}{e^{w_0‚ãÖx+b_0}+e^{w_1‚ãÖx+b_1}}\\
            &= \frac{1}{
                \frac{e^{w_0‚ãÖx+b_0}}{e^{w_1‚ãÖx+b_1}} + 1
            }\\
            &= \frac{1}{e^{(w_0‚ãÖx+b_0)-(w_1‚ãÖx+b_1)} + 1}\\
            &= \frac{1}{1 + e^{(w_0-w_1)‚ãÖx+(b_0-b_1)}}\\
            &= œÉ((w_0-w_1)‚ãÖx+(b_0-b_1))
    \end{align}
$$


Autrement dit, appliquer ce qu'on vient de voir pour le cas multinomial, si $n=2$, c'est comme
appliquer ce qu'on a vu pour deux classes, avec $w=w_0-w_1$ et $b=b_0-b_1$.

## La suite

Vous √™tes arriv√©‚ãÖe‚ãÖs au bout de ce cours et vous devriez avoir quelques id√©es de plusieurs concepts
importants‚ÄØ:

- Le concept de classifieur lin√©aire
- Le concept de fonction de co√ªt
- L'algorithme de descente de gradient stochastique
- La fonction softmax

On reparlera de tout √ßa en temps utile. Pour la suite de vos aventures au pays des classifieurs
logistiques, je vous recommande plut√¥t d'utiliser [leur impl√©mentation dans
scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).
Maintenant que vous savez comment √ßa marche, vous pouvez le faire la t√™te haute. Bravo‚ÄØ!

<small>Vous avez aussi d√©couvert les premiers r√©seaux de neurones de ce cours et ce n'est pas
rien‚ÄØ!</small>