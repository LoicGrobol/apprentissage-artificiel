---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 10â€¯: RÃ©gression logistique
===============================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-20
<!-- #endregion -->

```python
from IPython.display import display
```

## Classifieur linÃ©aire

On considÃ¨re des vecteurs de *features* de dimension $n$

$$\mathbf{x} = (xâ‚, â€¦, x_n)$$

Un vecteur de poids de dimension $n$

$$\mathbf{w} = (wâ‚, â€¦, w_n)$$

et un biais $b$ scalaire (un nombre quoi).

Pour rÃ©aliser une classification on considÃ¨re le nombre $z$ (on parle parfois de *logit*)

$$z=wâ‚Ã—xâ‚ + â€¦ + w_nÃ—x_n + b = \sum_iw_ix_i + b$$

Ce qu'on note aussi

$$z = \mathbf{w}â‹…\mathbf{x}+b$$

$\mathbf{w}â‹…\mathbf{x}$ se lit Â«â€¯w scalaire xâ€¯Â», on parle de *produit scalaire* en franÃ§ais et de *inner product* en anglais.

(ou pour les mathÃ©maticienâ‹…neâ‹…s acharnÃ©â‹…eâ‹…s $z = \langle w\ |\ x \rangle + b$)

Quelle que soit la faÃ§on dont on le note, on affectera Ã  $\mathbf{x}$ la classe $0$ si $z < 0$ et la classe $1$ sinon.

## ğŸ˜´ Exo ğŸ˜´

Ã‰crire une fonction qui prend en entrÃ©e un vecteur de features et un vecteur de poids sous forme de
tableaux numpy $x$ et $w$ de dimensions `(n,)` et un biais $b$ sous forme d'un tableau numpy de
dimensions `(1,)` et renvoie $z=\sum_iw_ix_i + b$.

## La fonction logistique


$$Ïƒ(z) = \frac{1}{1 + e^{âˆ’z}} = \frac{1}{1 + \exp(âˆ’z)}$$

Elle permet de *normaliser* $z$â€¯: $z$ peut Ãªtre n'importe quel nombre entre $-âˆ$ et $+âˆ$, mais on aura toujours $0 <Â Ïƒ(z) < 1$, ce qui permet de l'interprÃ©ter facilement comme une *vraisemblance*. Autrement dit, $Ïƒ(z)$ sera proche de $1$ s'il paraÃ®t vraisemblable que $x$ appartienne Ã  la classe $1$ et proche de $0$ sinon.

## ğŸ“ˆ Exo ğŸ“ˆ

Tracer avec matplotlib la courbe reprÃ©sentative de la fonction logistique.

## RÃ©gression logistique


Formellementâ€¯: on suppose qu'il existe une fonction $f$ qui prÃ©dit parfaitement les classes, donc telle que pour tout couple exemple/Ã©tiquette $(x, y)$ avec $y$ valant $0$ ou $1$, $f(x) = y$. On approcher cette fonction par une fonction $g$ de la forme

$$g(x) = Ïƒ(wâ‹…x+b)$$

Si on choisit les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur notre ensemble d'apprentissage, on dit que $g$ est la *rÃ©gression logistique de $f$* sur cet ensemble.

Un classifieur logistique, c'est simplement un classifieur qui pour un exemple $x$ renvoie $0$ si $g(x) < 0.5$ et $1$ sinon.

## ğŸ§  Exo ğŸ§ 

1\. Ã€ l'aide d'un lexique de sentiment (par exemple
[VADER](https://github.com/cjhutto/vaderSentiment)), Ã©crivez une fonction qui prend en entrÃ©e un
texte en anglais et renvoie sa reprÃ©sentation sous forme d'un vecteur de features Ã  deux traitsâ€¯:
nombre de mots positifs et nombre de mot nÃ©gatifs.

2\. Appliquer la fonction prÃ©cÃ©dente sur le mini-corpus IMDB

3\. Ã‰crire un classifieur logistique (en une fonction) qui prend en entrÃ©e les vecteurs de features
prÃ©cÃ©dents et utilise les poids respectifs $0.6$ et $0.4$ et un biais de $0$. Appliquez ce
classifieur sur le mini-corpus IMDB et calculez son exactitude.

## Fonction de coÃ»t

On formalise Â«â€¯Ãªtre le plus proche possibleâ€¯Â» de la section prÃ©cÃ©dente comme minimiser une certaine fonction de coÃ»t (*loss*) $L$.

Autrement dit, Ã©tant donnÃ© un ensemble de test $(xâ‚, yâ‚),â€¯â€¦, (x_n, y_n)$, on va mesurer la qualitÃ© du classifieur logistique $g$

$$\mathcal{L} = \sum_i L(g(xáµ¢), yáµ¢)$$

Dans le cas de la rÃ©gression logistique, on va utiliser la *log-vraisemblance nÃ©gative* (*negative log-likelihood*)â€¯:

On dÃ©finit la *vraisemblance* $V$ comme
$$
V(a, y) =
    \begin{cases}
        a & \text{si $y = 1$}\\
        1-a & \text{sinon}
    \end{cases}
$$

Intuitivement, il s'agit de la vraisemblance affectÃ©e par le modÃ¨le Ã  la classe correcte $y$. Il ne s'agit donc pas d'un coÃ»t, mais d'un *gain* (si sa valeur est haute, c'est que le modÃ¨le est bon)

La *log-vraisemblance nÃ©gative* $L$ est alors dÃ©finie par

$$L(a, y) = -\log(V(a, y))$$

Le $\log$ permet de rÃ©gulariser la valeur (c'est plus facile Ã  apprendre) et le $-$ Ã  s'assurer qu'on a bien un coÃ»t (plus la valeur est basse, meilleur le modÃ¨le est).

Une autre faÃ§on de le voirâ€¯: $L(a, y)$, c'est la [surprise](https://en.wikipedia.org/wiki/Information_content) de $y$ au sens de la thÃ©orie de l'information. Autrement ditâ€¯: si j'estime qu'il y a une probabilitÃ© $a$ d'observer la classe $y$, $L(a, y)$ mesure Ã  quel point il serait surprenant d'observer effectivement $y$.


On peut l'Ã©crire en une ligneâ€¯: pour un exemple $x$, le coÃ»t de l'exemple $(x, y)$ est

$$L(g(x), y) = -\log\left[g(x)Ã—y + (1-g(x))Ã—(1-y)\right]$$

C'est un *trick*, l'astuce c'est que comme $y$ vaut soit $0$ soit $1$, soit $y=0$, soit $1-y=0$ et donc la somme dans le $\log$ se simplifie dans tous les cas. Rien de transcendant lÃ -dedans.

La formule diffÃ¨re un peu de celle de *Speech and Language Processing* mais les rÃ©sultats sont les mÃªmes et celle-ci est mieux pour notre problÃ¨meâ€¯!

<small>En fait la leur est la formule gÃ©nÃ©rale de l'entropie croisÃ©e pour des distributions de proba Ã  support dans $\{0, 1\}$, ce qui est une autre intuition pour cette fonction de coÃ»t, mais ici elle nous complique la vie.</small>

## ğŸ“‰ Exo ğŸ“‰

Ã‰crire une fonction qui prend en entrÃ©e

- Un vecteur de features $x$ de taille $n$
- Un vecteur de poids $w$ de taille $n$ et un biais $b$ (de taille $1$)
- Une classe cible $c$ ($0$ ou $1$)

Et renvoie la log-vraisemblance nÃ©gative du classifieur logistique de poids $(w, b)$ pour l'exemple
$(x, c)$.

Servez vous-en pour calculer le coÃ»t du classifieur de l'exercise prÃ©cÃ©dent sur le mini-corpus IMDB.

## Descente de gradient

## ğŸ§ Exo ğŸ§

Reprendre la fonction qui calcule la fonction de coÃ»t, mais faire en sorte qu'elle renvoie Ã©galement
le gradient en $(x, c)$.

## ğŸ˜© Exo ğŸ˜©

S'en servir pour apprendre les poids Ã  donner aux features prÃ©cÃ©dentes Ã  l'aide du  [mini-corpus IMDB](../../data/imdb_smol.tar.gz)
