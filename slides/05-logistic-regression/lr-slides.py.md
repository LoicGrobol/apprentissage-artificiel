---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
TP 5‚ÄØ: R√©gression logistique
===============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
```

## Vectorisations arbitraires de documents

On a vu des fa√ßons de traiter des documents vus comme des sacs des mots en les repr√©sentant comme
des vecteurs dont les coordonn√©es correspondaient √† des nombres d'occurrences.

Mais on aimerait ‚Äî‚ÄØentre autres‚ÄØ‚Äî pouvoir travailler avec des repr√©sentations arbitraires, on peut
par exemple imaginer vouloir repr√©senter un document par ≈Äa polarit√© (au sens de l'analyse du
sentiment) de ses mots.

## üß† Exo üß†

### 1. Vectoriser un document

√Ä l'aide du lexique [VADER](https://github.com/cjhutto/vaderSentiment) (vous le trouverez aussi dans
[`data/vader_lexicon.txt`](data/vader_lexicon.txt)), √©crivez une fonction qui prend en entr√©e un
texte en anglais et renvoie sa repr√©sentation sous forme d'un vecteur de features √† deux traits‚ÄØ:
polarit√© positive moyenne (la somme des polarit√©s positives des mots qu'il contient divis√©e par sa
longueur en nombre de mots) et polarit√© n√©gative moyenne.

Le polarit√© d'un mot correspond √† la deuxi√®me colonne (`MEAN-SENTIMENT-RATING`) dans le fichier.

```python
def read_vader(vader_path):
    pass  #¬†√Ä vous de jouer
```

```python
def featurize(doc, lexicon):
    pass # √Ä vous de jouer‚ÄØ!
```

```python
lexicon = read_vader("../../data/vader_lexicon.txt")
doc = "I came in in the middle of this film so I had no idea about any credits or even its title till I looked it up here, where I see that it has received a mixed reception by your commentators. I'm on the positive side regarding this film but one thing really caught my attention as I watched: the beautiful and sensitive score written in a Coplandesque Americana style. My surprise was great when I discovered the score to have been written by none other than John Williams himself. True he has written sensitive and poignant scores such as Schindler's List but one usually associates his name with such bombasticities as Star Wars. But in my opinion what Williams has written for this movie surpasses anything I've ever heard of his for tenderness, sensitivity and beauty, fully in keeping with the tender and lovely plot of the movie. And another recent score of his, for Catch Me if You Can, shows still more wit and sophistication. As to Stanley and Iris, I like education movies like How Green was my Valley and Konrack, that one with John Voigt and his young African American charges in South Carolina, and Danny deVito's Renaissance Man, etc. They tell a necessary story of intellectual and spiritual awakening, a story which can't be told often enough. This one is an excellent addition to that genre."
doc_features = featurize(doc, lexicon)
doc_features
```

### 2. Vectoriser un corpus

Utiliser la fonction pr√©c√©dente pour vectoriser [le mini-corpus IMDB](../../data/imdb_smol.tar.gz)

```python
def featurize_dir(corpus_root, lexicon):
    pass # √Ä vous!

# On r√©utilise le lexique pr√©c√©dent
featurize_dir("data/imdb_smol", lexicon)
```

## Visualisation

Comment se r√©partissent les documents du corpus avec la repr√©sentation qu'on a choisi‚ÄØ?

```python
import matplotlib.pyplot as plt
import seaborn as sns
from corrections import featurize_dir, read_vader

lexicon = read_vader("data/vader_lexicon.txt")
imdb_features = featurize_dir("data/imdb_smol", lexicon)

X = np.array([d[0] for d in (*imdb_features["pos"], *imdb_features["neg"])])
Y = np.array([d[1] for d in (*imdb_features["pos"], *imdb_features["neg"])])
H = np.array([*("pos" for _ in imdb_features["pos"]), *("neg" for _ in imdb_features["neg"])])

fig = plt.figure(dpi=200)
sns.scatterplot(x=X, y=Y, hue=H, s=5)
plt.show()
```

On voit des tendances qui se d√©gagent, mais clairement √ßa va √™tre un peu coton

## Classifieur lin√©aire

On consid√®re des vecteurs de *features* de dimension $n$

$$\mathbf{x} = (x‚ÇÅ, ‚Ä¶, x_n)$$

Un vecteur de poids de dimension $n$

$$\mathbf{w} = (w‚ÇÅ, ‚Ä¶, w_n)$$

et un biais $b$ scalaire (un nombre quoi).

Pour r√©aliser une classification on consid√®re le nombre $z$ (on parle parfois de *logit*)

$$z=w‚ÇÅ√óx‚ÇÅ + ‚Ä¶ + w_n√óx_n + b = \sum_iw_ix_i + b$$

Ce qu'on note aussi

$$z = \mathbf{w}‚ãÖ\mathbf{x}+b$$

$\mathbf{w}‚ãÖ\mathbf{x}$ se lit ¬´‚ÄØw scalaire x‚ÄØ¬ª, on parle de *produit scalaire* en fran√ßais et de
*inner product* en anglais.

(ou pour les math√©maticien‚ãÖne‚ãÖs acharn√©‚ãÖe‚ãÖs $z = \langle w\ |\ x \rangle + b$)

Quelle que soit la fa√ßon dont on le note, on affectera √† $\mathbf{x}$ la classe $0$ si $z < 0$ et la
classe $1$ sinon.

## üò¥ Exo üò¥

### 1. Une fonction affine

√âcrire une fonction qui prend en entr√©e un vecteur de features et un vecteur de poids sous forme de
tableaux numpy $x$ et $w$ de dimensions `(n,)` et un biais $b$ sous forme d'un tableau numpy de
dimensions `(1,)` et renvoie $z=\sum_iw_ix_i + b$.

```python
def affine_combination(x, w, b):
    pass # √Ä vous de jouer‚ÄØ!

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

### 2. Un classifieur lin√©aire

√âcrire un classifieur lin√©aire qui prend en entr√©e des vecteurs de features √† deux dimensions
pr√©c√©dents et utilise les poids respectifs $0.6$ et $-0.4$ et un biais de $-0.01$. Appliquez ce
classifieur sur le mini-corpus IMDB qu'on a vectoris√© et calculez son exactitude.

```python
def hardcoded_classifier(x):
    return 0  # √Ä vous de jouer

hardcoded_classifier(doc_features)
```

Pour l'exactitude, on devrait obtenir quelque chose comme √ßa

```python
from corrections import classifier_accuracy
classifier_accuracy(np.array([0.6, -0.4]), np.array(-0.01), imdb_features)
```

## Classifieur lin√©aire‚ÄØ?

Pourquoi lin√©aire‚ÄØ? Regardez la figure suivante qui colore les points $(x,y)$ du plan en fonction de
la valeur de $z$.

```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) - 0.01

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

Ou encore plus clairement, si on repr√©sente la classe assign√©e

```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) -0.01 > 0.0

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On voit bien que la fronti√®re de classification est une droite, *a line*. On a donc un *linear*
classifier‚ÄØ: un classifieur lin√©aire (m√™me si en fran√ßais on dirait qu'il s'agit d'une fonction
*affine*).


Qu'est-ce que √ßa donne si on superpose avec notre corpus‚ÄØ?

```python
fig = plt.figure(dpi=200)

x = np.linspace(0, 0.4, 1000)
y = np.linspace(0, 0.4, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) -0.01 > 0.0

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))

X = np.array([d[0] for d in (*imdb_features["pos"], *imdb_features["neg"])])
Y = np.array([d[1] for d in (*imdb_features["pos"], *imdb_features["neg"])])
H = np.array([*(1 for _ in imdb_features["pos"]), *(0 for _ in imdb_features["neg"])])
plt.scatter(x=X, y=Y, c=H, cmap="viridis", s=5)

plt.show()
```

Pas si surprenant que nos r√©sultats ne soient pas terribles‚Ä¶

## La fonction logistique


$$œÉ(z) = \frac{1}{1 + e^{‚àíz}} = \frac{1}{1 + \exp(‚àíz)}$$

Elle permet de *normaliser* $z$‚ÄØ: $z$ peut √™tre n'importe quel nombre entre $-‚àû$ et $+‚àû$, mais on
aura toujours $0 <¬†œÉ(z) < 1$, ce qui permet de l'interpr√©ter facilement comme une *vraisemblance*.
Autrement dit, $œÉ(z)$ sera proche de $1$ s'il para√Æt vraisemblable que $x$ appartienne √† la classe
$1$ et proche de $0$ sinon.

## üìà Exo üìà


1\. D√©finir la fonction `logistic` qui prend en entr√©e un tableau numpy $z=[z_1,‚ÄØ‚Ä¶, z_n]$ et
renvoie le tableau $[œÉ(z_1), ‚Ä¶ , œÉ(z_n)]$.

```python
def logistic(z):
    pass  #¬†√Ä vous
```

2\. Tracer avec matplotlib la courbe repr√©sentative de la fonction logistique.

```python

```

## R√©gression logistique

Formellement‚ÄØ: on suppose qu'il existe une fonction $f$ qui pr√©dit parfaitement les classes, donc
telle que pour tout couple exemple/classe $(x, y)$ avec $y$ valant $0$ ou $1$, $f(x) = y$. On
voudrait approcher cette fonction par une fonction $g$ de la forme

$$g(x) = œÉ(w‚ãÖx+b)$$

Si on choisit les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur
notre ensemble d'apprentissage, on dit que $g$ est la *r√©gression logistique de $f$* sur cet
ensemble.

Un classifieur logistique, c'est simplement un classifieur qui pour un exemple $x$ renvoie $0$ si
$g(x) < 0.5$ et $1$ sinon. Il a exactement les m√™mes capacit√©s de discrimination qu'un classifieur
lin√©aire (sa fronti√®re de d√©cision est la m√™me et il ne sait donc pas prendre de d√©cisions plus
complexes), mais on peut interpr√©ter la confiance qu'il a dans sa d√©cision.


Par exemple voici la confiance que notre classifieur cod√© en dur a en ses d√©cisions

```python
from corrections import affine_combination, featurize, logistic


def classifier_confidence(x):
    return logistic(affine_combination(x, np.array([0.6, -0.4]), -0.01))

doc_features = featurize(doc, lexicon)
g_x = classifier_confidence(doc_features)
display(g_x)
display(Markdown(f"Le classifieur est s√ªr √† {g_x:.06%} que ce document est dans la classe $1$."))
display(Markdown(f"Autrement dit, d'apr√®s le classifieur, la classe $1$ a {g_x:.06%} de vraisemblance pour ce document"))
```


Quelle est la vraisemblance de la classe $0$ (*review* n√©gative)‚ÄØ? Et bien le reste

```python
1.0 - classifier_confidence(doc_features)
```

Comme l'exemple en question appartient bien √† cette classe, √ßa signifie que notre classifieur et
plut√¥t bon **sur cet exemple**. L'est-il sur le reste du corpus‚ÄØ?

```python
pos_confidence = sum(classifier_confidence(doc) for doc in imdb_features["pos"])
print(f"Average confidence for 'pos': {pos_confidence/len(imdb_features['pos']):.02%}")
neg_confidence = sum(1-classifier_confidence(doc) for doc in imdb_features["neg"])
print(f"Average confidence for 'neg': {neg_confidence/len(imdb_features['neg']):.02%}")
print(f"Average confidence for the correct class: {(pos_confidence+neg_confidence)/(len(imdb_features['pos']) + len(imdb_features['neg'])):.02%}")
```

Autrement dit, pour un exemple pris au hasard dans le corpus, la vraisemblance de sa classe telle
que jug√©e par le classifieur sera de $50.49\%$. Un classifieur parfait obtiendrait $100\%$, un
classifieur qui prendrait syst√©matiquement la mauvaise d√©cision $0\%$ et un classifieur al√©atoire
uniforme $50\%$ (puisque notre corpus a autant d'exemples de chaque classe).


Moralit√©‚ÄØ: nos poids ne sont pas tr√®s bien choisis, et notre pr√©occupation dans la suite va √™tre de
chercher comment choisir des poids pour que la confiance moyenne de la classe correcte soit aussi
haute que possible.

## Fonction de co√ªt

On a dit que notre objectif √©tait

> Chercher les poids $w$ et le biais $b$ tels que $g$ soit la plus proche possible de $f$ sur notre
ensemble d'apprentissage

On formalise ¬´‚ÄØ√™tre le plus proche possible‚ÄØ¬ª de la section pr√©c√©dente comme minimiser une certaine
fonction de co√ªt (*loss*) $L$ qui mesure l'erreur faite par le classifieur sur un exemple.

$$L(g(x), y) = \text{l'√©cart entre la classe $≈∑$ pr√©dite par $g$ pour $x$ et la classe correcte $y$}$$

√âtant donn√© un ensemble de test $(x‚ÇÅ, y‚ÇÅ),‚ÄØ‚Ä¶, (x_n, y_n)$, on estime l'erreur faite par le
classifieur logistique $g$ pour chaque exemple $(x_i, y_i)$ comme le co√ªt local $L(g(x·µ¢), y·µ¢)$ et
son erreur sur tout l'ensemble de test par le co√ªt global $\mathcal{L}$‚ÄØ:

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

Plus $\mathcal{L}$ sera bas, meilleur sera notre classifieur.

Dans le cas de la r√©gression logistique, on va s'inspirer de ce qu'on a vu dans la section
pr√©c√©dente et utiliser la *log-vraisemblance n√©gative* (*negative log-likelihood*)‚ÄØ:

On d√©finit la *vraisemblance* $V$ comme pr√©c√©demment par
$$
V(a, y) =
    \begin{cases}
        a & \text{si $y = 1$}\\
        1-a & \text{sinon}
    \end{cases}
$$

Intuitivement, il s'agit de la vraisemblance affect√©e par le mod√®le √† la classe correcte $y$. Il ne
s'agit donc pas d'un co√ªt, mais d'un *gain* (si sa valeur est haute, c'est que le mod√®le est bon)

La *log-vraisemblance n√©gative* $L$ est alors d√©finie par

$$L(a, y) = -\log(V(a, y))$$

Le $\log$ est l√† pour plusieurs raisons, calculatoires et th√©oriques<sup>1</sup> et le $-$ √†
s'assurer qu'on a bien un co√ªt (plus la valeur est basse, meilleur le mod√®le est).

<small>1. Entre autres, parce qu'une somme de $\log$-vraisemblance peut
√™tre vue comme le $\log$ de la probabilit√© d'une conjonction d'√©v√©nements ind√©pendants. Mais surtout
parce qu'il rend la fonction de co√ªt **convexe** par rapport √† $w$</small>.

Une interpr√©tation possible‚ÄØ: $L(a, y)$, c'est la
[surprise](https://en.wikipedia.org/wiki/Information_content) de $y$ au sens de la th√©orie de
l'information. Autrement dit‚ÄØ: si j'estime qu'il y a une probabilit√© $a$ d'observer la classe $y$,
$L(a, y)$ mesure √† quel point il serait surprenant d'observer effectivement $y$.


On peut v√©rifier qu'il s'agit bien d'un co√ªt‚ÄØ:

- C'est un nombre positif
- Si le classifieur prend une d√©cision correcte avec une confiance parfaite le co√ªt est nul‚ÄØ:

  $$
    \begin{cases}
        L(1.0, 1) = -\log(1.0) = 0\\
        L(0.0, 0) = -\log(1.0-0.0) = -\log(1.0) = 0
    \end{cases}
  $$
- Si le classifieur prend une d√©cision erron√©e avec une confiance parfaite le co√ªt est infini‚ÄØ:

  $$
    \begin{cases}
        L(0.0, 1) = -\log(0.0) = +\infty\\
        L(1.0, 0) = -\log(1.0-1.0) = \log(0.0) = +\infty
    \end{cases}
  $$

On peut aussi v√©rifier facilement que $L(a, 1)$ est d√©croissant par rapport √† $a$ et que $L(1-a, 0)$
est croissant par rapport √† $a$. Autrement dit, plus le classifieur juge que la classe correcte est
vraisemblable plus le co√ªt $L$ est bas.


Enfin, on peut l'√©crire $L$ en une ligne‚ÄØ: pour un exemple $x$, le co√ªt de l'exemple $(x, y)$ est

$$L(g(x), y) = -\log\left[g(x)√óy + (1-g(x))√ó(1-y)\right]$$

C'est une astucs‚ÄØ: comme $y$ vaut soit $0$ soit $1$, on a  soit $y=0$, soit $1-y=0$, et donc la
somme dans le $\log$ se simplifie dans tous les cas. Rien de transcendant l√†-dedans.

La formule diff√®re un peu de celle de *Speech and Language Processing*, mais les r√©sultats sont les
m√™mes et celle-ci est mieux pour notre probl√®me‚ÄØ!

<small>En fait la leur est la formule g√©n√©rale de l'entropie crois√©e pour des distributions de proba
√† support dans $\{0, 1\}$, ce qui est une autre intuition pour cette fonction de co√ªt, mais ici elle
nous complique la vie.</small>

Une derni√®re fa√ßon de l'√©crire en une ligne‚ÄØ:

$$L(g(x), y) = -\log\left[g(x)\mathbb{1}_{y=1} + (1-g(x))\mathbb{1}_{y=0}\right]$$

## üìâ Exo üìâ

√âcrire une fonction qui prend en entr√©e

- Un vecteur de features $x$ de taille $n$
- Un vecteur de poids $w$ de taille $n$ et un biais $b$ (de taille $1$)
- Une classe cible $y$ ($0$ ou $1$)

Et renvoie la log-vraisemblance n√©gative du classifieur logistique de poids $(w, b)$ pour l'exemple
$(x, y)$.

Servez-vous en pour calculer le co√ªt du classifieur de l'exercise pr√©c√©dent sur le mini-corpus IMDB.

Le r√©sultat devrait ressembler √† √ßa

```python
from corrections import loss_on_imdb
loss_on_imdb(np.array([0.6, -0.4]), -0.01, imdb_features)
```

<!-- #region -->
## Descente de gradient

### Principe g√©n√©ral

L'**algorithme de descente de gradient** est la cl√© de voute de l'essentiel des travaux en
apprentissage artificiel moderne. Il s'agit d'un algorithme it√©ratif qui √©tant donn√© un mod√®le
param√©tris√© et une fonction de co√ªt (avec des hypoth√®ses de r√©gularit√© assez faibles) permet de
trouver des valeurs des param√®tres pour lesquelles la fonction de co√ªt est minimal.

On ne va pas rentrer dans les d√©tails de l'algorithme de descente de gradient stochastique, mais
juste essayer de se donner quelques id√©es.

L'intuition √† avoir est la suivante‚ÄØ: si vous √™tes dans une vall√©e et que vous voulez trouver
rapidement le point le plus bas, une fa√ßon de faire est de chercher la direction vers laquelle la
pente descend le plus vite, de faire quelques pas dans cette direction puis de recommencer. On parle
aussi pour cette raison d'**algorithme de la plus forte pente**.

Clairement une condition pour que √ßa marche peu importe le point de d√©part, c'est que la vall√©e
n'ait qu'un seul point localement le plus bas. Par exemple √ßa marche avec une vall√©e comme celle-ci
<!-- #endregion -->

```python
%matplotlib inline
import tol_colors as tc
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = R**2 - 1

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

Mais pas pour celle-l√†

```python
%matplotlib inline
import tol_colors as tc
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = -np.cos(R)/(1+0.5*R**2)

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
# ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

OK, mais comment on trouve la plus forte pente en pratique‚ÄØ? En une dimension il suffit de suivre
l'oppos√© du nombre d√©riv√©‚ÄØ: <https://uclaacm.github.io/gradient-descent-visualiser/#playground>


En plus de dimensions, c'est plus compliqu√©, mais on peut s'en sortir en suivant le *gradient* qui
est une g√©n√©ralisation du nombre d√©riv√©‚ÄØ: <https://jackmckew.dev/3d-gradient-descent-in-python.html>


Ce qui fait marcher la machine c'est que **le gradient indique la direction dans laquelle la
fonction cro√Æt le plus vite**. Et que l'oppos√© du gradient indique la direction dans laquelle la
fonction d√©cro√Æt le plus vite.

(localement)

<!-- #region -->
Concr√®tement si on veut trouver $\theta$ tel que $f(\theta)$ soit minimale pour une certaine
fonction $f$ dont le gradient est donn√© par `grad_f` √ßa donne l'algo suivant

```python
def descent(grad_f, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        # On trouve la direction de plus grande pente
        steepest_direction = -grad_f(theta)
        #¬†On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
    return theta
```

Les *hyperparam√®tres* sont

- `theta_0` est notre point de d√©part, notre premi√®re estimation d'o√π se trouvera le minimum, que
  l'algorithme va raffiner. √âvidemment si on a d√©j√† une id√©e de vers o√π on pourrait le trouver, √ßa
  ira plus vite. Si on a aucune id√©e, on peut le prendre al√©atoire.
- `learning_rate` ou ¬´‚ÄØtaux d'apprentissage‚ÄØ¬ª‚ÄØ: de combien on se d√©place √† chaque √©tape. Si on le
  prend grand on arrive vite vers la r√©gion du minimum, on mettra longtemps pour en trouver une
  approximation pr√©cise. Si on le prend petit, √ßa sera l'inverse.
- `n_steps` est le nombre d'√©tapes d'optimisations. Dans un probl√®me d'apprentissage, c'est aussi le
  nombre de fois o√π on aura parcouru l'ensemble d'apprentissage et on parle souvent d'**epoch**

Ici on se donne un nombre fixe¬†d'epochs, une autre possibilit√© serait de s'arr√™ter quand on ne bouge
plus trop, par exemple avec une condition comme

```python
if np.max(grad_f(theta)) < 0.00001:
    break
```

dans la boucle et √©ventuellement avec une boucle infinie `while True`.
<!-- #endregion -->

Point notation‚ÄØ:

- **Le gradient de $f$** est souvent not√© $\nabla f$ ou $\operatorname{grad}f$, voire $\vec\nabla f$
  ou $\overrightarrow{\operatorname{grad}} f$ (pour dire que c'est un vecteur)
- Si $Œ∏=(Œ∏_1, ‚Ä¶, Œ∏_n)$, autrement dit si $f$ est une fonction de $n$ variables, on note
  $\operatorname{grad}f = \left(\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_1}, ‚Ä¶, \frac{‚àÇf(Œ∏)}{‚àÇŒ∏_n}\right)$. Autrement dit
  $\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_i}$, la **d√©riv√©e partielle** de $f(Œ∏)$ par rapport √† $Œ∏_i$, est la $i$-√®me
  coordonn√©es du gradient de $f$.
- **Le taux d'apprentissage** est souvent not√© $Œ±$ ou $Œ∑$


### Descente de gradient stochastique

Rappelez-vous, on a dit que notre fonction de co√ªt, c'√©tait

$$\mathcal{L} = \sum_i L(g(x·µ¢), y·µ¢)$$

et on cherche la valeur du param√®tre $Œ∏ = (w_1, ‚Ä¶, w_n, b)$ tel que $\mathcal{L}$ soit le plus petit
possible.


On peut utilise la propri√©t√© d'**additivit√©** du gradient‚ÄØ: pour deux fonctions $f$ et $g$, on a

$$\operatorname{grad}(f+g) = \operatorname{grad}f + \operatorname{grad}g$$

Donc ici

$$\operatorname{grad}\mathcal{L} = \sum_i \operatorname{grad}L(g(x·µ¢), y·µ¢)$$

<!-- #region -->
Si on dispose d'une fonction `grad_L` qui, √©tant donn√©s $g(x_i)$ et $y_i$, renvoie
$\operatorname{grad}L(g(x_i), y_i)$, l'algorithme de descente du gradient devient alors

```python
def descent(train_set, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        w = theta[:-1]
        b = theta[-1]
        partial_grads = []
        for (x, y) in train_set:
            #¬†On calcule g(x)
            g_x = logistic(np.inner(w,x)+b)
            #¬†On calcule le gradient de L(g(x), y))
            partial_grads.append(grad_L(g_x, y))
        # On trouve la direction de plus grande pente
        steepest_direction = -np.sum(partial_grads)
        # On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Pour chaque √©tape, on doit calculer tous les $g(x_i)$ et $\operatorname{grad}L(g(x_i), y_i)$. C'est
tr√®s couteux, il doit y avoir moyen de faire mieux.


Si les $L(g(x·µ¢), y·µ¢)$ √©taient ind√©pendants, ce serait plus simple‚ÄØ: on pourrait les optimiser
s√©par√©ment.


Ce n'est √©videmment pas le cas‚ÄØ: si on change $g$ pour que $g(x_0)$ soit plus proche de $y_0$, √ßa
changera aussi la valeur de $g(x_1)$.


**Mais on va faire comme si**

<!-- #region -->
C'est une approximation sauvage, mais apr√®s tout on commence √† avoir l'habitude. On va donc suivre
l'algo suivant

```python
def descent(train_set, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        for (x, y) in train_set:
            w = theta[:-1]
            b = theta[-1]
            # On calcule g(x)
            g_x = logistic(np.inner(w,x)+b)
            # On trouve la direction de plus grande pente
            steepest_direction = -grad_L(g_x, y)
            # On fait quelques pas dans cette direction
            theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Faites bien attention √† la diff√©rence‚ÄØ: au lieu d'attendre d'avoir calcul√© tous les
$\operatorname{grad}L(g(x_i), y_i)$ avant de modifier $Œ∏$, on va le modifier √† chaque fois.


- **Avantage**‚ÄØ: on modifie beaucoup plus souvent le param√®tre, si tout se passe bien, on devrait
  arriver √† une bonne approximation tr√®s vite.
- **Inconv√©nient**‚ÄØ: il se pourrait qu'en essayant de faire baisser $L(g(x_0), y_0)$, on fasse
  augmenter $L(g(x_1), y_1)$.

Notre espoir ici, c'est que cette situation n'arrivera pas, et qu'un bon param√®tre pour un certain
couple $(x, y)$ soit aussi un bon param√®tre pour $tous$ les couples `(exemple, classe)`.


Ce nouvel algorithme s'appelle l'**algorithme de descente de gradient stochastique**, et il est
crucial pour nous, parce qu'on ne pourra en pratique quasiment jamais faire de descente de gradient
globale.


Il ne nous reste plus qu'√† savoir comment on calcule `grad_L`. On ne fera pas la preuve, mais on a


$$\frac{‚àÇL(g(x), y)}{‚àÇw_i} = (g(x)-y)x_i$$


et


$$\frac{‚àÇL(g(x), y)}{‚àÇb} = g(x)-y$$


Autrement dit on mettra √† jour $w$ en calculant

$$w ‚Üê w -Œ∑√ó\operatorname{d}_wL(g(x), y) = w - Œ∑√ó(g(x)-y)x$$


<small>$\operatorname{d}_wL(g(x), y) = \left(\frac{‚àÇL(g(x), y)}{‚àÇw_1}, ‚Ä¶, \frac{‚àÇL(g(x),
y)}{‚àÇw_n}\right)$ est la *diff√©rentielle partielle* de $L(g(x), y)$ par rapport √† $w$.</small>


Et $b$ en calculant

$$b ‚Üê b -Œ∑√ó\frac{‚àÇL(g(x), y)}{‚àÇb} = b - Œ∑√ó(g(x)-y)$$

<!-- #region -->
## üßê Exo üßê

### 1. Calculer le gradient

Reprendre la fonction qui calcule la fonction de co√ªt, et la transformer pour qu'elle renvoie le
gradient par rapport √† $w$ et la d√©riv√©e partielle par rapport √† $b$ en $(x, y)$.
<!-- #endregion -->

```python
def grad_L(x, w, b, y):
    grad = np.zeros(w.size+b.size)  # √Ä vous‚ÄØ!
    return grad

grad_L(np.array([5, 10]), np.array([0.6, -0.4]), np.array([-0.01]), 0)
```

### 2. Descendre le gradient

S'en servir pour apprendre les poids √† donner aux *features* pr√©c√©dentes √† l'aide du [mini-corpus
IMDB](../../data/imdb_smol.tar.gz) en utilisant l'algorithme de descente de gradient stochastique.

```python
def descent(featurized_corpus, theta_0, learning_rate, n_steps):
    theta = theta_0
    for _ in range(n_steps):
        pass  # √Ä vous‚ÄØ!
    return 
descent(imdb_features, np.array([0.6, -0.4, 0.0]), 0.001, 100)
```

Dans une version o√π on affiche et on garde trace de l'historique

```python
from corrections import descent_with_logging
theta, theta_history = descent_with_logging(imdb_features, np.array([0.6, -0.4, -0.01]), 0.1, 100)
```

Un peu de visu suppl√©mentaire‚ÄØ:


Le trajet fait par $Œ∏$ au cours de l'apprentissage

```python
import numpy as np
import matplotlib.pyplot as plt


fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

x, y, z = np.hsplit(np.array(theta_history), 3)

ax.plot(x.squeeze(), y.squeeze(), z.squeeze(), label="Trajet de $Œ∏$ au cours de l'apprentissage")
ax.legend()

plt.show()
```

Ici comme on a peu de donn√©es, on peut m√™me se permettre le luxe de regarder la loss qu'on aurait
pour toutes leurs valeurs, par exemple si on fixe $b=0$, voil√† la t√™te qu'√† la loss globale
(l'abscisse et l'ordonn√©es sont les coordonn√©es de $w$, l'altitude/la couleur est la valeur de la
loss).

```python
def make_vector_corpus(featurized_corpus):
    vector_corpus = np.stack([*featurized_corpus["pos"], *featurized_corpus["neg"]])
    vector_target = np.concatenate([np.ones(len(featurized_corpus["pos"])), np.zeros(len(featurized_corpus["neg"]))])
    return vector_corpus, vector_target

vector_corpus, vector_target = make_vector_corpus(imdb_features)
```

```python
w1 = np.linspace(-50, 100, 200)
w2 = np.linspace(-100, 50, 200)
W1, W2 = np.meshgrid(w1, w2)
W = np.stack((W1, W2), axis=-1)
# Un peu de magie pour acc√©l√©rer le calcul
confidence = logistic(
    np.einsum("ijn,kn->ijk", W, vector_corpus)
)
broadcastable_target = vector_target[np.newaxis, np.newaxis, :]
loss = -np.log(confidence * broadcastable_target + (1-confidence)*(1-broadcastable_target)).sum(axis=-1)
fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')
ax.set_xlim(-50, 100)
ax.set_ylim(-100, 50)
ax.set_zlim(0, 3000)

surf = ax.plot_surface(W1, W2, loss, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1, alpha=0.8)
fig.colorbar(surf, shrink=0.5, aspect=5)
ax.plot_wireframe(W1, W2, loss, color='black')

heatmap = ax.contourf(W1, W2, loss, offset=-30, cmap=tc.tol_cmap("sunset"))

plt.title("Paysage de la fonction de co√ªt en fonction des valeurs de $w$ pour $b=0$")

plt.show()
```

## R√©gression multinomiale

Un dernier point‚ÄØ: on a vu dans tout ceci comment utiliser la r√©gression logistique pour un probl√®me
de classification √† deux classes. Comment on l'√©tend √† $n$ classes‚ÄØ?


R√©fl√©chissons d√©j√† √† quoi ressemblerait la sortie d'un tel classifieur‚ÄØ:

Pour un probl√®me √† deux classes, le classifieur $g$ nous donne pour chaque exemple $x$ une
estimation $g(x)$ de la vraisemblance de la classe $1$, et on a vu que la vraisemblance de la classe
$0$ √©tait n√©cessairement $1-g(x)$ pour que la somme des vraisemblances fasse 1.


On peut le pr√©senter autrement‚ÄØ: consid√©rons le classifieur $f$ tel que pour tout exemple $x$

$$f(x) = (1-g(x), g(x))$$

$f$ nous donne un vecteur √† deux coordonn√©es, $f_0(x)$ et $f_1(x)$, qui sont respectivement les
vraisemblances des classes $0$ et $1$.


Pour un probl√®me √† $n$ classes, on va vouloir une vraisemblance par classe, on va donc proc√©der de
la fa√ßon suivante‚ÄØ:

On consid√®re des poids $(w_1, b_1), ‚Ä¶, (w_n, b_n)$. Ils d√©finissent un classifieur lin√©aire.

En effet, si on consid√®re les $z_i$ d√©finis pour tout exemple $x$ par

$$
    \begin{cases}
        z_1 = w_1‚ãÖx + b_1\\
        \vdots\\
        z_n = w_n‚ãÖx + b_n
    \end{cases}
$$

On peut choisir la classe $y$ √† affecter √† $x$ en prenant $y=\operatorname{argmax}\limits_i z_i$


Il reste √† normaliser pour avoir des vraisemblances. Pour √ßa on utilise une fonction **tr√®s**
importante‚ÄØ: la fonction $\operatorname{softmax}$, d√©finie ainsi‚ÄØ:

$$\operatorname{softmax}(z_1, ‚Ä¶, z_n) = \left(\frac{e^{z_1}}{\sum_i e^{z_i}}, ‚Ä¶,
\frac{e^{z_n}}{\sum_i e^{z_i}}\right)$$

Contrairement √† la fonction logistique qui prenait un nombre en entr√©e et renvoyait un nombre,
$\operatorname{softmax}$ prend en entr√©e un **vecteur** non-normalis√© et renvoie un vecteur
normalis√©.


Pourquoi elle s'appelle *softmax*‚ÄØ? Consid√©rez le vecteur $v = (0.1, -0.5, 2.1, 2, 1.6)$. Son
maximum $\max(v)$, c'est $2.1$, et ce qu'on appelle $\operatorname{argmax}(v)$, la position du
maximum, c'est $3$.

Pour *argmax* √† la place d'une position, on peut aussi le voir comme un masque‚ÄØ: $(0, 0, 1, 0, 0)$,
autrement dit un vecteur dont les valeurs sont $0$ pour chaque position, sauf la position du
maximum, qui contient un $1$ (on parle de repr√©sentation *one-hot*). Visualisons ces vecteur‚ÄØ:

```python
v = np.array([0.1, -0.5, 2.1, 1.8, 0.6])
```

```python
plt.bar(np.arange(v.shape[0]), v)
plt.title("Coordonn√©es de $v$")
plt.show()
```


```python
plt.bar(np.arange(v.shape[0]), v == v.max())
plt.title("Coordonn√©es de $\operatorname{argmax}(v)$")
plt.show()
```

Et softmax‚ÄØ? Et bien regardez (on l'importe depuis
[SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html), un des
adelphes de NumPy, pour ne pas avoir √† le recoder nous-m√™me).

```python
from scipy.special import softmax
plt.bar(np.arange(v.shape[0]), softmax(v))
plt.title("Coordonn√©es de $\operatorname{softmax}(v)$")
plt.show()
```

On voit que c'est un genre d'entre-deux entre $v$ et $\operatorname{argmax}(v)$‚ÄØ:

- La distribution des coordonn√©es ressemble √† celle de $v$‚Ä¶
- mais les coordonn√©es sont toutes entre $0$ et $1$.
- Le max est tir√© vers $1$ et toutes les autres coordonn√©es vers $0$.
- La somme des coordonn√©es fait $1$.

Si, si, je vous assure‚ÄØ:

```python
softmax(v).sum()
```

Ok, √† l'erreur d'arrondi en virgule flottante pr√®s‚Ä¶


Autrement dit, on a, comme pour la fonction logistique, une fonction qui *normalise* les valeurs
tout en pr√©servant certaines propri√©t√©s.


Revenons √† nos moutons‚ÄØ: on d√©finit enfin le classifieur logistique multinomial $f$ de la fa√ßon
suivante‚ÄØ: pour tout exemple $x$, on a

$$f(x) = \operatorname{softmax}(w_1‚ãÖx+b_1, ‚Ä¶, w_n‚ãÖx+b_n) = \left(\frac{e^{w_1‚ãÖx+b_1}}{\sum_i
e^{w_i‚ãÖx+b_i}}, ‚Ä¶, \frac{e^{w_n‚ãÖx+b_n}}{\sum_i e^{w_i‚ãÖx+b_i}}\right)$$

et on choisit pour $x$ la classe

$$y = \operatorname{argmax}\limits_i f_i(x) = \operatorname{argmax}\limits_i
\frac{e^{w_i‚ãÖx+b_i}}{\sum_j e^{w_j‚ãÖx+b_j}}$$

Comme la fonction exponentielle est croissante, ce sera la m√™me classe que le classifieur lin√©aire
pr√©c√©dent. Comme pour le cas √† deux classe, la diff√©rence se fera lors de l'apprentissage. Je vous
laisse aller en lire les d√©tails dans *Speech and Language Processing*, mais l'id√©e est la m√™me‚ÄØ: on
utilise la log-vraisemblance n√©gative de la classe correcte comme fonction de co√ªt, et on optimise
les param√®tres avec l'algo de descente de gradient stochastique.


Un dernier d√©tail‚ÄØ?

Qu'est-ce qui se passe si on prend ce qu'on vient de voir pour $n=2$‚ÄØ? Est-ce qu'on retombe sur le
cas √† deux classe vu pr√©c√©demment‚ÄØ?


Oui, regarde‚ÄØ: dans ce cas

$$
    \begin{align}
        f_1(x)
            &= \frac{e^{w_1‚ãÖx+b_1}}{e^{w_0‚ãÖx+b_0}+e^{w_1‚ãÖx+b_1}}\\
            &= \frac{1}{
                \frac{e^{w_0‚ãÖx+b_0}}{e^{w_1‚ãÖx+b_1}} + 1
            }\\
            &= \frac{1}{e^{(w_0‚ãÖx+b_0)-(w_1‚ãÖx+b_1)} + 1}\\
            &= \frac{1}{1 + e^{(w_0-w_1)‚ãÖx+(b_0-b_1)}}\\
            &= œÉ((w_0-w_1)‚ãÖx+(b_0-b_1))
    \end{align}
$$


Autrement dit, appliquer ce qu'on vient de voir pour le cas multinomial, si $n=2$, c'est comme
appliquer ce qu'on a vu pour deux classes, avec $w=w_0-w_1$ et $b=b_0-b_1$.

## La suite

Vous √™tes arriv√©‚ãÖes au bout de ce cours et vous devriez avoir quelques id√©es de plusieurs concepts
importants‚ÄØ:

- Le concept de classifieur lin√©aire
- Le concept de fonction de co√ªt
- L'algorithme de descente de gradient stochastique
- La fonction softmax

On reparlera de tout √ßa en temps utile. Pour la suite de vos aventures au pays des classifieurs
logistiques, je vous recommande plut√¥t d'utiliser [leur impl√©mentation dans
scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).
Maintenant que vous savez comment √ßa marche, vous pouvez le faire la t√™te haute. Bravo‚ÄØ!
