---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 12‚ÄØ: R√©seaux de neurones
==============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-11-09
<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

## Le perceptron simple

[![Sch√©ma d'un neurone avec des l√©gendes pour les organelles et les connexions importantes pour la
communication entre
neurones.](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)](https://commons.wikimedia.org/w/index.php?curid=28761830)

Un mod√®le de neurone biologique (plut√¥t sensoriel)‚ÄØ: une unit√© qui re√ßoit plusieurs entr√©es $x_i$
scalaires (des nombres quoi), en calcule une somme pond√©r√©e $z$ (avec des poids $w_i$ pr√©d√©finis) et
renvoie une sortie binaire $y$ ($1$ si $z$ est positif, $0$ sinon).


Autrement dit

$$\begin{align}
z &= \sum_i w_ix_i\\
y &=
    \begin{cases}
        1 & \text{si $z > 0$}\\
        0 & \text{sinon}
    \end{cases}
\end{align}$$

Formul√© c√©l√®brement par McCulloch et Pitts (1943) avec des notations diff√©rentes

**Attention** selon les auteurices, le cas $z=0$ est trait√© diff√©remment, pour *Speech and Language Processing*, on renvoie $0$ dans ce cas, c'est donc la convention qu'on suivra, mais v√©rifiez √† chaque fois.


On peut ajouter un terme de *biais* en fixant $x_0=1$ et $w_0=b$, ce qui donne

$$\begin{equation}
    z = \sum_{i=0}^n w_ix_i = \sum_{i=1}^n w_ix_i + b
\end{equation}$$

Ou sch√©matiquement


![](figures/perceptron/perceptron.svg)


Ou avec du code

```python
def perceptron(inpt, weights):
    """Calcule la sortie du perceptron dont les poids sont `weights` pour l'entr√©e `inpt`
    
    Entr√©es‚ÄØ:
    
    - `inpt` un tableau numpy de dimension $n$
    - `weights` un tableau numpy de dimention $n+1$
    
    Sortie: un tableau numpy de type bool√©en et de dimentions $0$
    """
    return (np.inner(weights[1:], inpt) + weights[0]) > 0
```

Impl√©ment√© comme une machine, le perceptron Mark I, par Rosenblatt (1958)‚ÄØ:

[![Une photographie en noir et blanc d'une machine ressemblant √† une grande armoire pleine de fils
√©lectriques](https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg)](https://en.wikipedia.org/wiki/File:Mark_I_perceptron.jpeg)


**Est-ce que √ßa vous rappelle quelque chose‚ÄØ?**


ü§î


C'est un **classifieur lin√©aire** dont on a d√©j√† parl√© [pr√©c√©demment](../lecture10/lecture10.md).


Les ambitions initiales √©taient grandes

> *the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.*  
> New York Times, rappport√© par Olazaran (1996)


C'est par exemple assez facile de construire des neurones qui r√©alisent les op√©rations logiques √©l√©mentaires $\operatorname{ET}$, $\operatorname{OU}$ et $\operatorname{NON}$¬†:

```python
and_weights = np.array([-0.6, 0.5, 0.5])
print("x\ty\tx ET y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], and_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

<!-- TODO: ceci pourrait √™tre un exo -->

```python
or_weights = np.array([-0.5, 1, 1])
print("x\ty\tx OU y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], or_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

```python
not_weights = np.array([1, -1])
print("x\tNON x")
for x_i in [0, 1]:
    out = perceptron([x_i], not_weights).astype(int)
    print(f"{x_i}\t{out}")
```

Mais on se heurte vite √† des probl√®mes, m√™me pour repr√©senter les fonctions logiques les plus
basiques, comme $\operatorname{XOR}$ d√©finie pour $x ‚àà \{0, 1\}$ et  $y ‚àà \{0, 1\}$ par‚ÄØ:


$$\begin{equation}
    \operatorname{XOR}(x, y) = 
        \begin{cases}
            1 & \text{si $x ‚â† y$}\\
            0 & \text{si $x = y$}
        \end{cases}
\end{equation}$$



Autrement dit, $\operatorname{XOR}(x, y)$ c'est vrai si $x$ est vrai ou si $y$ est vrai, mais pas si
les deux sont vrais en m√™me temps.

```python
import tol_colors as tc

x = np.array([0, 1])
y = np.array([0, 1])
X, Y = np.meshgrid(x, y)
Z = np.logical_xor(X, Y)

fig = plt.figure(dpi=200)

heatmap = plt.scatter(X, Y, c=Z, cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```


Si on l'√©tend √† tout le plan pour mieux voir


```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = np.logical_xor(X > 0.5, Y > 0.5)

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On voit clairement le hic‚ÄØ: ce n'est pas un probl√®me lin√©airement s√©parable, donc un classifieur lin√©aire ne sera jamais capable de le r√©soudre.


## R√©seaux de neurones

Comment on peut s'en sortir‚ÄØ? En combinant des neurones‚ÄØ!


On sait faire les portes logiques √©l√©mentaires $\operatorname{ET}$, $\operatorname{OU}$ et $\operatorname{NON}$, or on a

$$\begin{equation}
    x \operatorname{XOR} y = (x \operatorname{OU} y)\quad\operatorname{ET}\quad\operatorname{NON}(x \operatorname{ET} y)
\end{equation}$$

<small>
Ou en notation fonctionnelle

$$\begin{equation}
    \operatorname{XOR}(x, y) = \operatorname{ET}\left[\operatorname{OU}(x, y), \operatorname{NON}(\operatorname{ET}(x,y))\right]
\end{equation}$$
</small>


On peut donc avoir $\operatorname{XOR}$ non pas avec un seul neurone, mais avec plusieurs neurones mis en **r√©seau**

![](figures/xor/xor.svg)

Ou, en √©crivant les termes de biais dans les neurones et en ajoutant un neurone pour servir de relai

![](figures/xor_ffnn/xor_ffnn.svg)

On voit ici appra√Ætre une structure en plusieurs couches (une d'entr√©e, une de sortie et trois interm√©diaires) o√π chaque neurone prend en entr√©e les sorties de tous les neurones de la couche pr√©c√©dente.

On appelle cette structure un r√©seau de neurones **compl√®tement connect√©** ou **dense**. On parle aussi un peu abusivement de *perceptron multicouches*. En anglais _**multilayer perceptron**_ ou _**feedforward neural network**_.


Voyons sa fronti√®re de d√©cision

```python
import tol_colors as tc

def and_net(X, Y):
    return 0.5*X + 0.5*Y - 0.6 > 0

def or_net(X, Y):
    return X + Y - 0.5 > 0

def not_net(X):
    return -1*X + 1 > 0

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = and_net(or_net(X, Y), not_net(and_net(X, Y)))

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

√áa marche‚ÄØ!


Enfin √ßa marche pour les coins, mais c'est tout ce qui nous int√©ressait‚ÄØ!

## Les couches

Une autre fa√ßon de voir ces couches neuronales qui va √™tre bien pratique pour la suite, c'est de voir chaque couche comme une fonction qui renvoie autant de sorties qu'elle a de neurones et prend autant d'entr√©es qu'il y a de neurones dans la couche pr√©c√©dente. Par exemple la premi√®re couche notre r√©seau $\operatorname{XOR}$ peut s'√©crire comme‚ÄØ:

```python
def layer1(inpt):
    output_1 = ((np.inner(inpt, np.array([0.5, 0.5])) + np.array(-0.6)) > 0).astype(int)
    output_2 = ((np.inner(inpt, np.array([1, 1])) + np.array(-0.5)) > 0).astype(int)
    return np.hstack([output_1, output_2])

display(layer1([1, 0]))
display(layer1([1, 1]))
```

La deuxi√®me couche comme‚ÄØ:

```python
def layer2(inpt):
    output_1 = ((np.inner(inpt, np.array([-1, 0])) + np.array(1)) > 0).astype(int)
    output_2 = ((np.inner(inpt, np.array([0, 1])) + np.array(0)) > 0).astype(int)
    return np.hstack([output_1, output_2])

display(layer2([0, 1]))
display(layer2([1, 1]))
```

Et la troisi√®me couche comme

```python
def layer3(inpt):
    return ((np.inner(inpt, np.array([0.5, 0.5])) + np.array(-0.6)) > 0).astype(int)

display(layer3([1, 1]))
display(layer3([0, 1]))
```

Le r√©seau c'est donc

```python
def xor_ffnn(inpt):
    return layer3(layer2(layer1(inpt)))

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i]).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

Maintenant, si on regarde, ces fonctions ont toutes la m√™me t√™te, on pourrait le faire en une seule

```python
def layer(inpt, weight1, bias1, weight2, bias2):
    output_1 = ((np.inner(inpt, weight1) + bias1) > 0).astype(int)
    output_2 = ((np.inner(inpt, weight2) + bias2) > 0).astype(int)
    return np.hstack([output_1, output_2])
                
layer([1, 0], [0.5, 0.5], -0.6, [1, 1], -0.5)
```

On peut rassembler ensemble les poids

```python
def layer(inpt, weight, bias):
    output_1 = ((np.inner(inpt, weight[0]) + bias[0]) > 0).astype(int)
    output_2 = ((np.inner(inpt, weight[1]) + bias[1]) > 0).astype(int)
    return np.hstack([output_1, output_2])
                
layer([0, 1], [[0.5, 0.5], [1, 1]], [-0.5, -0.6])
```

Et m√™me, en l'√©crivant comme des op√©rations matricielles

```python
def layer(inpt, weight, bias):
    output = ((np.matmul(weight, inpt) + bias) > 0).astype(int)
    return output
                
layer([0, 1], [[0.5, 0.5], [1, 1]], [-0.5, -0.6])
```

Cette derni√®re formulation est celle qu'on utilise en g√©n√©ral, elle a le gros avantage de tr√®s bien se parall√©liser, et m√™me, si on dispose de mat√©riel sp√©cialis√© (comme des cartes graphiques) de b√©n√©ficier d'acc√©l√©rations suppl√©mentaires (voir par exemple Vuduc et Choi ([2013](https://jeewhanchoi.github.io/publication/pdf/brief_history.pdf)) pour la culture).


Elle permet aussi de facilement manipuler les tailles des couches‚ÄØ: une couche √† $n$ entr√©es et $m$ sorties correspond √† une matrice de poids de taille $m√ón$ et un vecteur de biais de taille $m$.


Une derni√®re subtilit√©‚ÄØ? Pour mat√©rialiser le concept de couche et √©viter d'avoir √† passer en permanence des poids, on utilise en g√©n√©ral des classes. Voici notre r√©seau $\operatorname{XOR}$ r√©√©crit en objet‚ÄØ:

```python
class Layer:
    """Une couche neuronale compl√®tement connect√©e"""
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias

    def __call__(self, inpt):
        return ((np.matmul(self.weight, inpt) + self.bias) > 0).astype(int)
    
layer1 = Layer(np.array([[0.5, 0.5], [1, 1]]), np.array([-0.6, -0.5]))
layer2 = Layer(np.array([[-1, 0], [0, 1]]), np.array([1, 0]))
layer3 = Layer(np.array([0.5, 0.5]), np.array(-0.6))

def xor_ffnn(inpt):
    return layer3(layer2(layer1(inpt)))

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

(admirez au passage l'usage de `__call__`)


On peut aussi imaginer un conteneur pour une r√©seau

```python
class Network:
    def __init__(self, layers):
        self.layers = layers
    
    def __call__(self, inpt):
        res = inpt
        for l in self.layers:
            res = l(res)
        return res

xor_ffnn = Network([layer1, layer2, layer3])

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

√áa fait propre, non‚ÄØ?

## Non-linearit√©s

Comme pour les classifieurs logistiques, on aime bien en g√©n√©ral avoir une d√©cision qui ne soit pas tout ou rien mais puisse pr√©dire des nombres, pour √ßa on peut remplacer le `> 0` dans ce qui pr√©c√®de par la fonction logistique

```python
def sigmoid(x):
    return 1/(1+np.exp(-x))

class SigmoidLayer:
    """Une couche neuronale compl√®tement connect√©e suivie de la fonction logistique"""
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias

    def __call__(self, inpt):
        return sigmoid(np.matmul(self.weight, inpt) + self.bias)
    
soft_and = SigmoidLayer(np.array([0.5, 0.5]), np.array(-0.6))

print("x\ty\tx soft_and y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = soft_and([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

On peut aussi l'imaginer comme la succession d'une couche purement lin√©aire et d'une couche qui applique la fonction logistique sur ses entr√©es coordonn√©e par coordonn√©e‚ÄØ:

```python
class LinearLayer:
    """Une couche neuronale lin√©aire compl√®tement connect√©e"""
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias

    def __call__(self, inpt):
        return np.matmul(self.weight, inpt) + self.bias
    
class SigmoidLayer:
    """Une couche neuronale qui applique la fonction logistique aux coordonn√©es de son entr√©e"""
    # Pas besoin d'un `__init__` particulier ici, on a pas de param√®tres √† initialiser
    def __call__(self, inpt):
        return 1/(1+np.exp(-inpt))
    
soft_and = Network(
    [
        LinearLayer(np.array([0.5, 0.5]), np.array(-0.6)),
        SigmoidLayer(),
    ],
)

print("x\ty\tx soft_and y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = soft_and([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

Dans le cas g√©n√©ral, on dit que la fonction logistique dans ce r√©seau est une **non-lin√©arit√©** ou **activation**, c'est-√†-dire une fonction non-lin√©aire appliqu√©e coordonn√©e par coordonn√©e aux sorties d'une couche neuronale. On peut en choisir une autre, selon ce qu'on veut obtenir.

Pour les couches de sorties, c'est souvent l'application cibl√©e qui va conditionner ce choix, pour les couches internes, dites **couches cach√©es**, elle conditionnent la capacit√© d'apprentissage du r√©seau. Voici quelques uns des exemples les plus courants‚ÄØ:

```python
x = np.linspace(-5, 5, 1000)

fig = plt.figure(dpi=200, constrained_layout=True)
axs = fig.subplots(3, 2)

axs[0, 0].plot(x, 1/(1+np.exp(-x)))
axs[0, 0].set_title("Fonction logistique")
axs[0, 1].plot(x, x > 0)
axs[0, 1].set_title("Fonction de Heavyside/√©chelon (unit step)")
axs[1, 0].plot(x, np.maximum(x, 0))
axs[1, 0].set_title("Rectifieur (ReLU)")
axs[1, 1].plot(x, np.tanh(x))
axs[1, 1].set_title("Tangente hyperbolique")
axs[1, 1].spines['left'].set_position('center')
axs[1, 1].spines['bottom'].set_position('zero')
axs[2, 0].plot(x, np.maximum(x, 0.1*x))
axs[2, 0].set_title("Leaky ReLU")
axs[2, 1].plot(x, np.maximum(x, 0.5*x*(1+np.tanh(np.sqrt(2*np.pi)*(x+0.044715*x**3)))))
axs[2, 1].set_title("GELU")

for ax in fig.get_axes():
    ax.spines["right"].set_color("none")
    ax.spines[bbb"top"].set_color("none")

plt.show()
```

Vous reconnaissez celle qu'on a utilis√© dans notre r√©seau $\operatorname{XOR}$‚ÄØ?


Et il y en a [plein](https://mlfromscratch.com/activation-functions-explained) d'autres.


En pratique, le choix de la bonne non-lin√©arit√© pour un r√©seau n'est pas encore bien compris‚ÄØ: c'est un hyperparam√®tre √† optimiser parmi d'autres. Ces derni√®res ann√©es on choisit plut√¥t par d√©faut la fonction rectifieur (dite un peu abusivement ReLU).


Mais au fait, pourquoi on s'emb√™te avec √ßa‚ÄØ? √áa ne suffit pas des couches lin√©aires‚ÄØ?


Non.


Si on se limite √† des couches lin√©aires, nos r√©seaux ne peuvent exprimer que des fonctions lin√©aires. M√™me si on peut faire beaucoup de choses avec, on a souvent besoin de plus. Voyez par exemple ce que √ßa donne dans [le bac √† sable de Tensorflow](https://playground.tensorflow.org).


Si on utilise des non-lin√©arit√©s, en revanche, nos r√©seaux deviennent beaucoup plus puissants. Beaucoup, **beaucoup** plus.


Le [**Th√©or√®me d'approximation universelle**](https://en.wikipedia.org/wiki/Universal_approximation_theorem), dont il existe de nombreuses versions (Pinkus ([1999](https://pinkus.net.technion.ac.il/files/2021/02/acta.pdf)) en fait une tr√®s bonne revue) dit en substance qu'√† condition d'avoir assez de couches, ou des couches suffisament larges et d'utiliser des non-lin√©arit√©s continues qui ne soient pas des polyn√¥mes, √©tant donn√©e une fonction continue $f$, on peut toujours trouver un r√©seau de neurones qui soit aussi pr√®s qu'on veut de $f$.


Bien que √ßa ne dise rien de la capacit√© des r√©seaux de neurones √† *apprendre* des fonctions arbitraires, c'est une des motivations th√©oriques principales √† leur utilisation‚ÄØ: au moins, contrairement √† un classifieur logistique par exemple, ils sont capables de repr√©senter les fonctions qui nous int√©ressent.


Derni√®re pr√©cision‚ÄØ: les couches lin√©aires et les non-lin√©arit√©s par coordonn√©es ne sont pas les seules types de couches qu'on utilise en pratique. Notablement, pour construire des classifieurs multiclasses on utilise souvent la fonction $\operatorname{softmax}$ comme derni√®re couche.

$$\begin{equation}
    \operatorname{softmax}(z_1, ‚Ä¶, z_n)
    = \left(
        \frac{e^{z_1}}{\sum_i e^{z_i}},
        ‚Ä¶,
        \frac{e^{z_n}}{\sum_i e^{z_i}}
    \right)
\end{equation}$$


Au final, voici √† quoi ressemble un classifieur neuronal classique.

```python
from scipy.special import softmax

class ReluLayer:
    """Une couche neuronale qui applique la fonction rectifieur"""
    def __call__(self, inpt):
        return np.maximum(inpt, 0)

class SoftmaxLayer:
    """Une couche neuronale qui applique la fonction softmax √† son entr√©e"""
    def __call__(self, inpt):
        return softmax(inpt)
    
# Un r√©seau a une couche cach√©e de taille 32 qui prend en entr√©e des vecteurs
# de traits de dimension 16 et renvoie les vraisemblances de 8 classes.
# Les poids sont al√©atoires
classifier = Network(
    [
        LinearLayer(np.random.normal(size=(32, 16)), np.random.normal(size=(32))),
        ReluLayer(),
        LinearLayer(np.random.normal(size=(8, 32)), np.random.normal(size=(8))),
        SoftmaxLayer(),
    ],
)

classifier([0, 2, 1, 3, 7, 0.1, 0.5, -12, 2, 1, -0.5, -1, 10
            , -2, 0.128, -8])
```

<small>[En pratique](https://ogunlao.github.io/2020/04/26/you_dont_really_know_softmax.html) comme $\operatorname{softmax}$ est toujours plut√¥t instable, on utilise plut√¥t $\log\operatorname{softmax}$, c'est toujours la m√™me histoire.</small>

## Apprendre un r√©seau de neurones

Tout √ßa c'est bien gentil, mais encore une fois, on a choisi des poids √† la main. Or notre objectif c'est d'**apprendre**.


Comment on apprend un r√©seau de neurone‚ÄØ? Comment on d√©termine les poids √† partir de donn√©es‚ÄØ?


Et bien c'est toujours la m√™me recette pour l'apprentissage supervis√©‚ÄØ:

- D√©terminer une fonction de co√ªt
- Apprendre par descente de gradient


Les fonctions de co√ªt ressemblent tr√®s fort √† celles d'autres techniques d'apprentissage. En TAL, comme on s'en sort toujours plus ou moins pour se rammener √† de la classification, on va en g√©n√©ral utiliser la $\log$-vraisemblance n√©gative, comme pour les classifieurs logistiques.


Concr√®tement, qu'est-ce que √ßa donne‚ÄØ? Et bien si on a un r√©seau de neurones $f$ pour un probl√®me √† $n$ classes (donc qui renvoie en sortie des vecteurs normalis√©s de dimension $n$) et un exemple $(x, y)$ o√π $x$ est une entr√©e adapt√©e √† $f$ et $1‚©Ωy‚©Ωn$ est la classe √† pr√©dire pour $x$, la loss de $f$ pour $(x, y)$ sera

$$\begin{equation}
    L(f, x, y) = -\log\left(f(x)_y\right)
\end{equation}$$

o√π $f(x)_y$ est la $y$-i√®me coordonn√©e de $f(x)$.

<small>C'est aussi pour √ßa qu'on aime bien utiliser le $\log\operatorname{softmax}$‚ÄØ: de toute fa√ßon on va vouloir calculer un $\log$ apr√®s.</small>


Ok, et le gradient‚ÄØ?


√áa se corse un peu mais pas trop.


On va utiliser les m√™mes id√©es que celles qu'on a vu pour les classifieurs logistiques‚ÄØ: on va consid√©rer un param√®tre $Œ∏$ qui sera une concat√©nation de tous les poids de toutes les couches du r√©seau dans un gros vecteur et les les $L(f, x, y)$ comme des fonctions de $Œ∏$.


Par bonheur, si les non-lin√©arit√©s qu'on a choisi sont gentilles (et elles le sont, on les choisit pour), ces fonctions seront diff√©rentiables, c'est-√†-dire qu'elles ont un gradient pour tout $(x, y)$ et on peut donc leur appliquer l'algorithme de descente de gradient stochastique.


Alors quel est le probl√®me‚ÄØ?


Il y en a deux‚ÄØ:

1. Comment on calcule ces gradients‚ÄØ?
2. Est-ce que l'algorithme fonctionne toujours‚ÄØ?


Le point 1. n'est pas un probl√®me, les fonctions en questions peuvent √™tre compliqu√©es, surtout si le r√©seau est profond, et caculer leur gradients √† la main √ßa peut √™tre p√©nible, mais heureusement on a des programmes de calcul symbolique qui ont la gentillesse de le faire pour nous. C'est ce qu'on appelle de la **diff√©rentiation automatique** dont on va voir un exemple juste apr√®s.


Le point 2. est plus d√©licat en th√©orie‚ÄØ: on a pas de garantie th√©orique que l'algo fonctionne toujours, ni m√™me r√©ellement d'estimation de son comportement. Mais **en pratique** √ßa a tendance √† marcher la plupart du temps‚ÄØ: si on applique l'algo de descente de gradient avec des hyperparam√®tres raisonnables et suffisament de donn√©es, on arrive √† trouver des bons poids.

Un [certain](https://ruder.io/optimizing-gradient-descent/) nombre de raffinement de cet algo (que vous trouverez souvent sous le nom *SGD* pour _**S**tochastic **G**radient **D**escent_) ont √©t√© d√©velopp√© pour essayer que √ßa marche le mieux possible le plus souvent possible. Deux particuli√®rement notables sont l'accel√©ration de Nesterov et l'estimation adaptative des moments ([Adam](https://arxiv.org/abs/1412.6980)).

## En pratique üî•

En pratique, comme on ne va certainement pas impl√©menter tout √ßa √† la main ici (m√™me si je vous recommande de le faire une fois de votre c√¥t√© pour bien comprendre comment √ßa marche), on va se reposer sur la biblioth√®que de r√©seaux de neurones la plus utilis√©e pour le TAL ces derni√®res (et probablement aussi ces prochaines) ann√©es‚ÄØ: [Pytorch](pytorch.org).
