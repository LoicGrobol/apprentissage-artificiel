---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 12‚ÄØ: R√©seaux de neurones
==============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-11-09
<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

## Le perceptron simple

[![Sch√©ma d'un neurone avec des l√©gendes pour les organelles et les connexions importantes pour la
communication entre
neurones.](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)](https://commons.wikimedia.org/w/index.php?curid=28761830)

Un mod√®le de neurone biologique (plut√¥t sensoriel)‚ÄØ: une unit√© qui re√ßoit plusieurs entr√©es $x_i$
scalaires (des nombres quoi), en calcule une somme pond√©r√©e $z$ (avec des poids $w_i$ pr√©d√©finis) et
renvoie une sortie binaire $y$ ($1$ si $z$ est positif, $0$ sinon).


Autrement dit

$$\begin{align}
z &= \sum_i w_ix_i\\
y &=
    \begin{cases}
        1 & \text{si $z > 0$}\\
        0 & \text{sinon}
    \end{cases}
\end{align}$$

Formul√© c√©l√®brement par McCulloch et Pitts (1943) avec des notations diff√©rentes

**Attention** selon les auteurices, le cas $z=0$ est trait√© diff√©remment, pour *Speech and Language Processing*, on renvoie $0$ dans ce cas, c'est donc la convention qu'on suivra, mais v√©rifiez √† chaque fois.


On peut ajouter un terme de *biais* en fixant $x_0=1$ et $w_0=b$, ce qui donne

$$\begin{equation}
    z = \sum_{i=0}^n w_ix_i = \sum_{i=1}^n w_ix_i + b
\end{equation}$$

Ou sch√©matiquement


![](figures/perceptron/perceptron.svg)


Ou avec du code

```python
def perceptron(inpt, weights):
    """Calcule la sortie du perceptron dont les poids sont `weights` pour l'entr√©e `inpt`
    
    Entr√©es‚ÄØ:
    
    - `inpt` un tableau numpy de dimension $n$
    - `weights` un tableau numpy de dimention $n+1$
    
    Sortie: un tableau numpy de type bool√©en et de dimentions $0$
    """
    return (np.inner(weights[1:], inpt) + weights[0]) > 0
```

Impl√©ment√© comme une machine, le perceptron Mark I, par Rosenblatt (1958)‚ÄØ:

[![Une photographie en noir et blanc d'une machine ressemblant √† une grande armoire pleine de fils
√©lectriques](https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg)](https://en.wikipedia.org/wiki/File:Mark_I_perceptron.jpeg)


**Est-ce que √ßa vous rappelle quelque chose‚ÄØ?**


ü§î


C'est un **classifieur lin√©aire** dont on a d√©j√† parl√© [pr√©c√©demment](../lecture10/lecture10.md).


Les ambitions initiales √©taient grandes

> *the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.*  
> New York Times, rappport√© par Olazaran (1996)


C'est par exemple assez facile de construire des neurones qui r√©alisent les op√©rations logiques √©l√©mentaires $\operatorname{ET}$, $\operatorname{OU}$ et $\operatorname{NON}$¬†:

```python
and_weights = np.array([-0.6, 0.5, 0.5])
print("x\ty\tx ET y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], and_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

<!-- TODO: ceci pourrait √™tre un exo -->

```python
or_weights = np.array([-0.5, 1, 1])
print("x\ty\tx OU y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], or_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

```python
not_weights = np.array([1, -1])
print("x\tNON x")
for x_i in [0, 1]:
    out = perceptron([x_i], not_weights).astype(int)
    print(f"{x_i}\t{out}")
```

Mais on se heurte vite √† des probl√®mes, m√™me pour repr√©senter les fonctions logiques les plus
basiques, comme $\operatorname{XOR}$ d√©finie pour $x ‚àà \{0, 1\}$ et  $y ‚àà \{0, 1\}$ par‚ÄØ:


$$\begin{equation}
    \operatorname{XOR}(x, y) = 
        \begin{cases}
            1 & \text{si $x ‚â† y$}\\
            0 & \text{si $x = y$}
        \end{cases}
\end{equation}$$



Autrement dit, $\operatorname{XOR}(x, y)$ c'est vrai si $x$ est vrai ou si $y$ est vrai, mais pas si
les deux sont vrais en m√™me temps.

```python
import tol_colors as tc

x = np.array([0, 1])
y = np.array([0, 1])
X, Y = np.meshgrid(x, y)
Z = np.logical_xor(X, Y)

fig = plt.figure(dpi=200)

heatmap = plt.scatter(X, Y, c=Z, cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```


Si on l'√©tend √† tout le plan pour mieux voir


```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = np.logical_xor(X > 0.5, Y > 0.5)

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On voit clairement le hic‚ÄØ: ce n'est pas un probl√®me lin√©airement s√©parable, donc un classifieur lin√©aire ne sera jamais capable de le r√©soudre.


## R√©seaux de neurones

Comment on peut s'en sortir‚ÄØ? En combinant des neurones‚ÄØ!


On sait faire les portes logiques √©l√©mentaires $\operatorname{ET}$, $\operatorname{OU}$ et $\operatorname{NON}$, or on a

$$\begin{equation}
    x \operatorname{XOR} y = (x \operatorname{OU} y)\quad\operatorname{ET}\quad\operatorname{NON}(x \operatorname{ET} y)
\end{equation}$$

<small>
Ou en notation fonctionnelle

$$\begin{equation}
    \operatorname{XOR}(x, y) = \operatorname{ET}\left[\operatorname{OU}(x, y), \operatorname{NON}(\operatorname{ET}(x,y))\right]
\end{equation}$$
</small>


On peut donc avoir $\operatorname{XOR}$ non pas avec un seul neurone, mais avec plusieurs neurones mis en **r√©seau**

![](figures/xor/xor.svg)

Ou, en √©crivant les termes de biais dans les neurones et en ajoutant un neurone pour servir de relai

![](figures/xor_ffnn/xor_ffnn.svg)

On voit ici appra√Ætre une structure en plusieurs couches (une d'entr√©e, une de sortie et trois interm√©diaires) o√π chaque neurone prend en entr√©e les sorties de tous les neurones de la couche pr√©c√©dente.

On appelle cette structure un r√©seau de neurones **compl√®tement connect√©** ou **dense**. On parle aussi un peu abusivement de *perceptron multicouches*. En anglais _**multilayer perceptron**_ ou _**feedforward neural network**_.


Voyons sa fronti√®re de d√©cision

```python
import tol_colors as tc

def and_net(X, Y):
    return 0.5*X + 0.5*Y - 0.6 > 0

def or_net(X, Y):
    return X + Y - 0.5 > 0

def not_net(X):
    return -1*X + 1 > 0

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = and_net(or_net(X, Y), not_net(and_net(X, Y)))

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

√áa marche‚ÄØ!

## Les couches

Une autre fa√ßon de voir ces couches neuronales qui va √™tre bien pratique pour la suite, c'est de voir chaque couche comme une fonction qui renvoie autant de sorties qu'elle a de neurones et prend autant d'entr√©es qu'il y a de neurones dans la couche pr√©c√©dente. Par exemple la premi√®re couche notre r√©seau $\operatorname{XOR}$ peut s'√©crire comme‚ÄØ:

```python
def layer1(inpt):
    output_1 = ((np.inner(inpt, np.array([0.5, 0.5])) + np.array(-0.6)) > 0).astype(int)
    output_2 = ((np.inner(inpt, np.array([1, 1])) + np.array(-0.5)) > 0).astype(int)
    return np.hstack([output_1, output_2])

display(layer1([1, 0]))
display(layer1([1, 1]))
```

La deuxi√®me couche comme‚ÄØ:

```python
def layer2(inpt):
    output_1 = ((np.inner(inpt, np.array([-1, 0])) + np.array(1)) > 0).astype(int)
    output_2 = ((np.inner(inpt, np.array([0, 1])) + np.array(0)) > 0).astype(int)
    return np.hstack([output_1, output_2])

display(layer2([0, 1]))
display(layer2([1, 1]))
```

Et la troisi√®me couchec comme

```python
def layer3(inpt):
    return ((np.inner(inpt, np.array([0.5, 0.5])) + np.array(-0.6)) > 0).astype(int)

display(layer3([1, 1]))
display(layer3([0, 1]))
```

Le r√©seau c'est donc

```python
def xor_ffnn(inpt):
    return layer3(layer2(layer1(inpt)))

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i]).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```
