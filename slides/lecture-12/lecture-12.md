---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 12‚ÄØ: R√©seaux de neurones
==============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-11-10
<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

## Le perceptron simple

[![Sch√©ma d'un neurone avec des l√©gendes pour les organelles et les connexions importantes pour la
communication entre
neurones.](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)](https://commons.wikimedia.org/w/index.php?curid=28761830)

Un mod√®le de neurone biologique (plut√¥t sensoriel)‚ÄØ: une unit√© qui re√ßoit plusieurs entr√©es $x_i$
scalaires (des nombres quoi), en calcule une somme pond√©r√©e $z$ (avec des poids $w_i$ pr√©d√©finis) et
renvoie une sortie binaire $y$ ($1$ si $z$ est positif, $0$ sinon).


Autrement dit

$$\begin{align}
z &= \sum_i w_ix_i\\
y &=
    \begin{cases}
        1 & \text{si $z > 0$}\\
        0 & \text{sinon}
    \end{cases}
\end{align}$$

Formul√© c√©l√®brement par McCulloch et Pitts (1943) avec des notations diff√©rentes

**Attention** selon les auteurices, le cas $z=0$ est trait√© diff√©remment, pour *Speech and Language Processing*, on renvoie $0$ dans ce cas, c'est donc la convention qu'on suivra, mais v√©rifiez √† chaque fois.


On peut ajouter un terme de *biais* en fixant $x_0=1$ et $w_0=b$, ce qui donne

$$\begin{equation}
    z = \sum_{i=0}^n w_ix_i = \sum_{i=1}^n w_ix_i + b
\end{equation}$$

Ou sch√©matiquement


![](figures/perceptron/perceptron.svg)


Ou avec du code

```python
def perceptron(inpt, weights):
    """Calcule la sortie du perceptron dont les poids sont `weights` pour l'entr√©e `inpt`
    
    Entr√©es‚ÄØ:
    
    - `inpt` un tableau numpy de dimension $n$
    - `weights` un tableau numpy de dimention $n+1$
    
    Sortie: un tableau numpy de type bool√©en et de dimensions $0$
    """
    return (np.inner(weights[1:], inpt) + weights[0]) > 0
```

Impl√©ment√© comme une machine, le perceptron Mark I, par Rosenblatt (1958)‚ÄØ:

[![Une photographie en noir et blanc d'une machine ressemblant √† une grande armoire pleine de fils
√©lectriques](https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg)](https://en.wikipedia.org/wiki/File:Mark_I_perceptron.jpeg)


**Est-ce que √ßa vous rappelle quelque chose‚ÄØ?**


ü§î


C'est un **classifieur lin√©aire** dont on a d√©j√† parl√© [pr√©c√©demment](../lecture10/lecture10.md).


Les ambitions initiales √©taient grandes

> *the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.*  
> New York Times, rappport√© par Olazaran (1996)


C'est par exemple assez facile de construire un qui r√©alis l'op√©ration logique √©l√©mentaire $\operatorname{ET}$¬†:

```python
and_weights = np.array([-0.6, 0.5, 0.5])
print("x\ty\tx ET y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], and_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

√áa marche bien parce que c'est un probl√®me **lin√©airement s√©parable**‚ÄØ: si on repr√©sente $x$ et $y$ dans le plan, on peut tracer une droite qui s√©pare la parties o√π $x\operatorname{ET}y$ vaut $1$ et la partie o√π √ßa vaut $0$‚ÄØ:

```python
import tol_colors as tc

x = np.array([0, 1])
y = np.array([0, 1])
X, Y = np.meshgrid(x, y)
Z = np.logical_and(X, Y)

fig = plt.figure(dpi=200)

heatmap = plt.scatter(X, Y, c=Z, cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

Ici voil√† les valeurs que renvoie notre neurone‚ÄØ:

```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = 0.5*X + 0.5*Y - 0.6 > 0

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On confirme‚ÄØ: √ßa marche‚ÄØ!


√áa marche aussi tr√®s bien pour $\operatorname{OU}$ et $\operatorname{NON}$


<!-- TODO: ceci pourrait √™tre un exo -->

```python
or_weights = np.array([-0.5, 1, 1])
print("x\ty\tx OU y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], or_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

```python
not_weights = np.array([1, -1])
print("x\tNON x")
for x_i in [0, 1]:
    out = perceptron([x_i], not_weights).astype(int)
    print(f"{x_i}\t{out}")
```

Mais on se heurte vite √† des probl√®mes, m√™me pour repr√©senter les fonctions logiques les plus
basiques, comme $\operatorname{XOR}$ d√©finie pour $x ‚àà \{0, 1\}$ et  $y ‚àà \{0, 1\}$ par‚ÄØ:


$$\begin{equation}
    \operatorname{XOR}(x, y) = 
        \begin{cases}
            1 & \text{si $x ‚â† y$}\\
            0 & \text{si $x = y$}
        \end{cases}
\end{equation}$$



Autrement dit, $\operatorname{XOR}(x, y)$ c'est vrai si $x$ est vrai ou si $y$ est vrai, mais pas si
les deux sont vrais en m√™me temps.

```python
import tol_colors as tc

x = np.array([0, 1])
y = np.array([0, 1])
X, Y = np.meshgrid(x, y)
Z = np.logical_xor(X, Y)

fig = plt.figure(dpi=200)

heatmap = plt.scatter(X, Y, c=Z, cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```


Si on l'√©tend √† tout le plan pour mieux voir en prenant $0.5$ comme fronti√®re pour *vrai*‚ÄØ:


```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = np.logical_xor(X > 0.5, Y > 0.5)

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On voit clairement le hic‚ÄØ: ce n'est pas un probl√®me lin√©airement s√©parable, donc un classifieur
lin√©aire ne sera jamais capable de le r√©soudre.


## R√©seaux de neurones

Comment on peut s'en sortir‚ÄØ? En combinant des neurones‚ÄØ!


On sait faire les portes logiques √©l√©mentaires $\operatorname{ET}$, $\operatorname{OU}$ et
$\operatorname{NON}$, or on a

$$\begin{equation}
    x \operatorname{XOR} y = (x \operatorname{OU} y)\quad\operatorname{ET}\quad\operatorname{NON}(x \operatorname{ET} y)
\end{equation}$$

<small>
Ou en notation fonctionnelle

$$\begin{equation}
    \operatorname{XOR}(x, y) = \operatorname{ET}\left[\operatorname{OU}(x, y), \operatorname{NON}(\operatorname{ET}(x,y))\right]
\end{equation}$$
</small>


On peut donc avoir $\operatorname{XOR}$ non pas avec un seul neurone, mais avec plusieurs neurones
mis en **r√©seau**

![](figures/xor/xor.svg)

Ou, en √©crivant les termes de biais dans les neurones et en ajoutant un neurone pour servir de relai

![](figures/xor_ffnn/xor_ffnn.svg)

On voit ici appra√Ætre une structure en plusieurs couches (une d'entr√©e, une de sortie et trois
interm√©diaires) o√π chaque neurone prend en entr√©e les sorties de tous les neurones de la couche
pr√©c√©dente.

On appelle cette structure un r√©seau de neurones **compl√®tement connect√©** ou **dense**. On parle
aussi un peu abusivement de *perceptron multicouches*. En anglais _**multilayer perceptron**_ ou
_**feedforward neural network**_.


Voyons sa fronti√®re de d√©cision

```python
import tol_colors as tc

def and_net(X, Y):
    return 0.5*X + 0.5*Y - 0.6 > 0

def or_net(X, Y):
    return X + Y - 0.5 > 0

def not_net(X):
    return -1*X + 1 > 0

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = and_net(or_net(X, Y), not_net(and_net(X, Y)))

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

√áa marche‚ÄØ!


Enfin √ßa marche pour les coins, mais c'est tout ce qui nous int√©ressait‚ÄØ!

## Les couches

Une autre fa√ßon de voir ces couches neuronales qui va √™tre bien pratique pour la suite, c'est de
voir chaque couche comme une fonction qui renvoie autant de sorties qu'elle a de neurones et prend
autant d'entr√©es qu'il y a de neurones dans la couche pr√©c√©dente. Par exemple la premi√®re couche
notre r√©seau $\operatorname{XOR}$ peut s'√©crire comme‚ÄØ:

```python
def layer1(inpt):
    output_1 = ((np.inner(inpt, np.array([0.5, 0.5])) + np.array(-0.6)) > 0).astype(int)
    output_2 = ((np.inner(inpt, np.array([1, 1])) + np.array(-0.5)) > 0).astype(int)
    return np.hstack([output_1, output_2])

display(layer1([1, 0]))
display(layer1([1, 1]))
```

La deuxi√®me couche comme‚ÄØ:

```python
def layer2(inpt):
    output_1 = ((np.inner(inpt, np.array([-1, 0])) + np.array(1)) > 0).astype(int)
    output_2 = ((np.inner(inpt, np.array([0, 1])) + np.array(0)) > 0).astype(int)
    return np.hstack([output_1, output_2])

display(layer2([0, 1]))
display(layer2([1, 1]))
```

Et la troisi√®me couche comme

```python
def layer3(inpt):
    return ((np.inner(inpt, np.array([0.5, 0.5])) + np.array(-0.6)) > 0).astype(int)

display(layer3([1, 1]))
display(layer3([0, 1]))
```

Le r√©seau c'est donc

```python
def xor_ffnn(inpt):
    return layer3(layer2(layer1(inpt)))

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i]).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

Maintenant, si on regarde, ces fonctions ont toutes la m√™me t√™te, on pourrait le faire en une seule

```python
def layer(inpt, weight1, bias1, weight2, bias2):
    output_1 = ((np.inner(inpt, weight1) + bias1) > 0).astype(int)
    output_2 = ((np.inner(inpt, weight2) + bias2) > 0).astype(int)
    return np.hstack([output_1, output_2])
                
layer([1, 0], [0.5, 0.5], -0.6, [1, 1], -0.5)
```

On peut rassembler ensemble les poids

```python
def layer(inpt, weight, bias):
    output_1 = ((np.inner(inpt, weight[0]) + bias[0]) > 0).astype(int)
    output_2 = ((np.inner(inpt, weight[1]) + bias[1]) > 0).astype(int)
    return np.hstack([output_1, output_2])
                
layer([0, 1], [[0.5, 0.5], [1, 1]], [-0.5, -0.6])
```

Et m√™me, en l'√©crivant comme des op√©rations matricielles

```python
def layer(inpt, weight, bias):
    output = ((np.matmul(weight, inpt) + bias) > 0).astype(int)
    return output
                
layer([0, 1], [[0.5, 0.5], [1, 1]], [-0.5, -0.6])
```

Cette derni√®re formulation est celle qu'on utilise en g√©n√©ral, elle a le gros avantage de tr√®s bien
se parall√©liser, et m√™me, si on dispose de mat√©riel sp√©cialis√©, comme des cartes graphiques) de
b√©n√©ficier d'acc√©l√©rations suppl√©mentaires (voir par exemple Vuduc et Choi
([2013](https://jeewhanchoi.github.io/publication/pdf/brief_history.pdf)) pour la culture).


Elle permet aussi de facilement manipuler les tailles des couches‚ÄØ: une couche √† $n$ entr√©es et $m$
sorties correspond √† une matrice de poids de taille $m√ón$ et un vecteur de biais de taille $m$.


Une derni√®re subtilit√©‚ÄØ? Pour mat√©rialiser le concept de couche et √©viter d'avoir √† passer en
permanence des poids, on utilise en g√©n√©ral des classes. Voici notre r√©seau $\operatorname{XOR}$
r√©√©crit en objet‚ÄØ:

```python
class Layer:
    """Une couche neuronale compl√®tement connect√©e"""
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias

    def __call__(self, inpt):
        return ((np.matmul(self.weight, inpt) + self.bias) > 0).astype(int)
    
layer1 = Layer(
    np.array([[0.5, 0.5], [1, 1]]),
    np.array([-0.6, -0.5]),
)
display(layer1([0,1]))
layer2 = Layer(np.array([[-1, 0], [0, 1]]), np.array([1, 0]))
layer3 = Layer(np.array([0.5, 0.5]), np.array(-0.6))

def xor_ffnn(inpt):
    return layer3(layer2(layer1(inpt)))

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

(admirez au passage l'usage de `__call__`)


On peut aussi imaginer un conteneur pour une r√©seau

```python
class Network:
    def __init__(self, layers):
        self.layers = layers
    
    def __call__(self, inpt):
        res = inpt
        for l in self.layers:
            res = l(res)
        return res

xor_ffnn = Network([layer1, layer2, layer3])

print("x\ty\tx XOR y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = xor_ffnn([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

√áa fait propre, non‚ÄØ?

## Non-linearit√©s

Comme pour les classifieurs logistiques, on aime bien en g√©n√©ral avoir une d√©cision qui ne soit pas
tout ou rien mais puisse pr√©dire des nombres, pour √ßa on peut remplacer le `> 0` dans ce qui pr√©c√®de
par la fonction logistique

```python
def sigmoid(x):
    return 1/(1+np.exp(-x))

class SigmoidLayer:
    """Une couche neuronale compl√®tement connect√©e suivie de la fonction logistique"""
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias

    def __call__(self, inpt):
        return sigmoid(np.matmul(self.weight, inpt) + self.bias)
    
soft_and = SigmoidLayer(np.array([0.5, 0.5]), np.array(-0.6))

print("x\ty\tx soft_and y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = soft_and([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

On peut aussi l'imaginer comme la succession d'une couche purement lin√©aire et d'une couche qui
applique la fonction logistique sur ses entr√©es coordonn√©e par coordonn√©e‚ÄØ:

```python
class LinearLayer:
    """Une couche neuronale lin√©aire compl√®tement connect√©e"""
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias

    def __call__(self, inpt):
        return np.matmul(self.weight, inpt) + self.bias
    
class SigmoidLayer:
    """Une couche neuronale qui applique la fonction logistique aux coordonn√©es de son entr√©e"""
    # Pas besoin d'un `__init__` particulier ici, on a pas de param√®tres √† initialiser
    def __call__(self, inpt):
        return 1/(1+np.exp(-inpt))
    
soft_and = Network(
    [
        LinearLayer(np.array([0.5, 0.5]), np.array(-0.6)),
        SigmoidLayer(),
    ],
)

print("x\ty\tx soft_and y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = soft_and([x_i, y_i])
        print(f"{x_i}\t{y_i}\t{out}")
```

Dans le cas g√©n√©ral, on dit que la fonction logistique dans ce r√©seau est une **non-lin√©arit√©** ou
**activation**, c'est-√†-dire une fonction non-lin√©aire appliqu√©e coordonn√©e par coordonn√©e aux
sorties d'une couche neuronale. On peut en choisir une autre, selon ce qu'on veut obtenir.

Pour les couches de sorties, c'est souvent l'application cibl√©e qui va conditionner ce choix, pour
les couches internes, dites **couches cach√©es**, elle conditionnent la capacit√© d'apprentissage du
r√©seau. Voici quelques uns des exemples les plus courants‚ÄØ:

```python
x = np.linspace(-5, 5, 1000)

fig = plt.figure(dpi=200, constrained_layout=True)
axs = fig.subplots(3, 2)

axs[0, 0].plot(x, 1/(1+np.exp(-x)))
axs[0, 0].set_title("Fonction logistique")
axs[0, 1].plot(x, x > 0)
axs[0, 1].set_title("Fonction de Heaviside/√©chelon (unit step)")
axs[1, 0].plot(x, np.maximum(x, 0))
axs[1, 0].set_title("Rectifieur (ReLU)")
axs[1, 1].plot(x, np.tanh(x))
axs[1, 1].set_title("Tangente hyperbolique")
axs[1, 1].spines['left'].set_position('center')
axs[1, 1].spines['bottom'].set_position('zero')
axs[2, 0].plot(x, np.maximum(x, 0.1*x))
axs[2, 0].set_title("Leaky ReLU")
axs[2, 1].plot(x, np.maximum(x, 0.5*x*(1+np.tanh(np.sqrt(2*np.pi)*(x+0.044715*x**3)))))
axs[2, 1].set_title("GELU")

for ax in fig.get_axes():
    ax.spines["right"].set_color("none")
    ax.spines["top"].set_color("none")

plt.show()
```

Vous reconnaissez celle qu'on a utilis√© dans notre r√©seau $\operatorname{XOR}$‚ÄØ?


Et il y en a [plein](https://mlfromscratch.com/activation-functions-explained) d'autres.


En pratique, le choix de la bonne non-lin√©arit√© pour un r√©seau n'est pas encore bien compris‚ÄØ: c'est
un hyperparam√®tre √† optimiser parmi d'autres. Ces derni√®res ann√©es on choisit plut√¥t par d√©faut la
fonction rectifieur (dite un peu abusivement ReLU).


Mais au fait, pourquoi on s'emb√™te avec √ßa‚ÄØ? √áa ne suffit pas des couches lin√©aires‚ÄØ?


Non.


Si on se limite √† des couches lin√©aires, nos r√©seaux ne peuvent exprimer que des fonctions
lin√©aires. M√™me si on peut faire beaucoup de choses avec, on a souvent besoin de plus. Voyez par
exemple ce que √ßa donne dans [le bac √† sable de Tensorflow](https://playground.tensorflow.org).


Si on utilise des non-lin√©arit√©s, en revanche, nos r√©seaux deviennent beaucoup plus puissants.
Beaucoup, **beaucoup** plus.


Le [**Th√©or√®me d'approximation
universelle**](https://en.wikipedia.org/wiki/Universal_approximation_theorem), dont il existe de
nombreuses versions (Pinkus, [1999](https://pinkus.net.technion.ac.il/files/2021/02/acta.pdf)) en
fait une tr√®s bonne revue) dit en substance qu'√† condition d'avoir assez de couches, ou des couches
suffisament larges et d'utiliser des non-lin√©arit√©s continues qui ne soient pas des polyn√¥mes, √©tant
donn√©e une fonction continue $f$, on peut toujours trouver un r√©seau de neurones qui soit aussi pr√®s
qu'on veut de $f$.


Bien que √ßa ne dise rien de la capacit√© des r√©seaux de neurones √† *apprendre* des fonctions
arbitraires, c'est une des motivations th√©oriques principales √† leur utilisation‚ÄØ: au moins,
contrairement √† un classifieur logistique par exemple, ils sont capables de repr√©senter les
fonctions qui nous int√©ressent.


Derni√®re pr√©cision‚ÄØ: les couches lin√©aires et les non-lin√©arit√©s par coordonn√©es ne sont pas les
seules types de couches qu'on utilise en pratique. Notablement, pour construire des classifieurs
multiclasses on utilise souvent la fonction $\operatorname{softmax}$ comme derni√®re couche.

$$\begin{equation}
    \operatorname{softmax}(z_1, ‚Ä¶, z_n)
    = \left(
        \frac{e^{z_1}}{\sum_i e^{z_i}},
        ‚Ä¶,
        \frac{e^{z_n}}{\sum_i e^{z_i}}
    \right)
\end{equation}$$


Au final, voici √† quoi ressemble un classifieur neuronal classique.

```python
from scipy.special import softmax

class ReluLayer:
    """Une couche neuronale qui applique la fonction rectifieur"""
    def __call__(self, inpt):
        return np.maximum(inpt, 0)

class SoftmaxLayer:
    """Une couche neuronale qui applique la fonction softmax √† son entr√©e"""
    def __call__(self, inpt):
        return softmax(inpt)
    
# Un r√©seau a une couche cach√©e de taille 32 qui prend en entr√©e des vecteurs
# de traits de dimension 16 et renvoie les vraisemblances de 8 classes.
# Les poids sont al√©atoires
classifier = Network(
    [
        LinearLayer(np.random.normal(size=(32, 16)), np.random.normal(size=(32))),
        ReluLayer(),
        LinearLayer(np.random.normal(size=(8, 32)), np.random.normal(size=(8))),
        SoftmaxLayer(),
    ],
)

classifier([0, 2, 1, 3, 7, 0.1, 0.5, -12, 2, 1, -0.5, -1, 10
            , -2, 0.128, -8])
```

<small>[En pratique](https://ogunlao.github.io/2020/04/26/you_dont_really_know_softmax.html) comme
$\operatorname{softmax}$ est toujours plut√¥t instable, on utilise plut√¥t
$\log\operatorname{softmax}$, c'est toujours la m√™me histoire.</small>

## Apprendre un r√©seau de neurones

Tout √ßa c'est bien gentil, mais encore une fois, on a choisi des poids √† la main. Or notre objectif
c'est d'**apprendre**.


Comment on apprend un r√©seau de neurone‚ÄØ? Comment on d√©termine les poids √† partir de donn√©es‚ÄØ?


Et bien c'est toujours la m√™me recette pour l'apprentissage supervis√©‚ÄØ:

- D√©terminer une fonction de co√ªt
- Apprendre par descente de gradient


Les fonctions de co√ªt ressemblent tr√®s fort √† celles d'autres techniques d'apprentissage. En TAL,
comme on s'en sort toujours plus ou moins pour se rammener √† de la classification, on va en g√©n√©ral
utiliser la $\log$-vraisemblance n√©gative, comme pour les classifieurs logistiques.


Concr√®tement, qu'est-ce que √ßa donne‚ÄØ? Et bien si on a un r√©seau de neurones $f$ pour un probl√®me √†
$n$ classes (donc qui renvoie en sortie des vecteurs normalis√©s de dimension $n$) et un exemple $(x,
y)$ o√π $x$ est une entr√©e adapt√©e √† $f$ et $1‚©Ωy‚©Ωn$ est la classe √† pr√©dire pour $x$, la loss de $f$
pour $(x, y)$ sera

$$\begin{equation}
    L(f, x, y) = -\log\left(f(x)_y\right)
\end{equation}$$

o√π $f(x)_y$ est la $y$-i√®me coordonn√©e de $f(x)$.

<small>C'est aussi pour √ßa qu'on aime bien utiliser le $\log\operatorname{softmax}$‚ÄØ: de toute fa√ßon
on va vouloir calculer un $\log$ apr√®s.</small>


Ok, et le gradient‚ÄØ?


√áa se corse un peu mais pas trop.


On va utiliser les m√™mes id√©es que celles qu'on a vu pour les classifieurs logistiques‚ÄØ: on va
consid√©rer un param√®tre $Œ∏$ qui sera une concat√©nation de tous les poids de toutes les couches du
r√©seau dans un gros vecteur et les les $L(f, x, y)$ comme des fonctions de $Œ∏$.


Par bonheur, si les non-lin√©arit√©s qu'on a choisi sont gentilles (et elles le sont, on les choisit
pour), ces fonctions seront diff√©rentiables, c'est-√†-dire qu'elles ont un gradient pour tout $(x,
y)$ et on peut donc leur appliquer l'algorithme de descente de gradient stochastique.


Alors quel est le probl√®me‚ÄØ?


Il y en a deux‚ÄØ:

1. Comment on calcule ces gradients‚ÄØ?
2. Est-ce que l'algorithme fonctionne toujours‚ÄØ?


Le point 1. n'est pas un probl√®me, les fonctions en questions peuvent √™tre compliqu√©es, surtout si
le r√©seau est profond, et caculer leur gradients √† la main √ßa peut √™tre p√©nible, mais heureusement
on a des programmes de calcul symbolique qui ont la gentillesse de le faire pour nous. C'est ce
qu'on appelle de la **diff√©rentiation automatique** dont on va voir un exemple juste apr√®s.


Le point 2. est plus d√©licat en th√©orie‚ÄØ: on a pas de garantie th√©orique que l'algo fonctionne
toujours, ni m√™me r√©ellement d'estimation de son comportement. Mais **en pratique** √ßa a tendance √†
marcher la plupart du temps‚ÄØ: si on applique l'algo de descente de gradient avec des hyperparam√®tres
raisonnables et suffisament de donn√©es, on arrive √† trouver des bons poids.

Un [certain](https://ruder.io/optimizing-gradient-descent/) nombre de raffinement de cet algo (que
vous trouverez souvent sous le nom *SGD* pour _**S**tochastic **G**radient **D**escent_) ont √©t√©
d√©velopp√© pour essayer que √ßa marche le mieux possible le plus souvent possible. Deux
particuli√®rement notables sont l'accel√©ration de Nesterov et l'estimation adaptative des moments
([Adam](https://arxiv.org/abs/1412.6980)).

## En pratique üî•

En pratique, comme on ne va certainement pas impl√©menter tout √ßa √† la main ici (m√™me si je vous
recommande de le faire une fois de votre c√¥t√© pour bien comprendre comment √ßa marche), on va se
reposer sur la biblioth√®que de r√©seaux de neurones la plus utilis√©e pour le TAL ces derni√®res (et
probablement aussi ces prochaines) ann√©es‚ÄØ: [Pytorch](pytorch.org).

```python
import torch
```

Pytorch fait plein de choses (allez voir la [doc](https://pytorch.org/docs)), mais pour commencer,
on va l'utiliser comme une collection de couches neuronales et une biblioth√®que de calcul vectoriel
(comme numpy).

### Les tenseurs


L'objet de base dans Pytorch est le **tenseur** `torch.tensor`, qui est un autre nom pour ce que
numpy appelle un `array`.

```python
t = torch.tensor([1,2,3,4])
t
```

```python
t = torch.tensor(
    [
        [1,2,3,4],
        [5,6,7,8],
    ]
)
t
```

D'ailleurs on peut facilement faire des allers-retours entre Pytorch et Numpy

```python
torch.tensor([1,2,3,4]).numpy()
```

```python
torch.from_numpy(np.array([1,2,3,4]))
```

Comme les tableaux numpy, on peut leur appliquer des op√©rations

```python
torch.tensor([1,2,3,4]) + torch.tensor([1,5,-2,-1])
```

Et la plupart des op√©rations d√©finies dans numpy sont disponible ici aussi (Pytorch essaie autant
que possible d'√™tre compatible)

```python
torch.sum(torch.tensor([1,2,3,4]))
```

M√™me si en g√©n√©ral, on y pr√©f√®re un style d'op√©rations en cha√Ænes

```python
torch.tensor([1,2,3,4]).mul(torch.tensor(2)).sum()
```

Vous trouverez dans la doc [la liste des fonctions natives](https://pytorch.org/docs/stable/torch.html) et celle des [m√©thodes des tenseurs](https://pytorch.org/docs/stable/tensors.html), n'h√©sitez pas √† vous y pencher souvent, surtout avant de vouloir recoder des trucs vous m√™mes.

### Les couches neuronales


Les couches neuronales sont d√©finies dans le module [`torch.nn`](https://pytorch.org/docs/stable/nn.html).

```python
import torch.nn
```

Il y en a beaucoup

```python
len(dir(torch.nn))
```

En pratique, Pytorch ne fait pas la diff√©rence entre un r√©seau et une couche‚ÄØ: tout √ßa sera un `torch.nn.Module`. L'avantage c'est que √ßa permet facilement d'interconnecter des r√©seaux entre eux.


La couche la plus importante pour nous ici c'est la couche
[`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) qui est
√©videmment la couche lin√©aire compl√®tement connect√©e qu'on a appell√© `LinearLayer` plus haut.


Voici une r√©impl√©mentation du r√©seau $\operatorname{XOR}$ en Pytorch (c'est un peu laborieux parce
que Pytorch n'est pas vraiment pr√©vu pour coder des poids en dur, mais c'est possible‚ÄØ!)

```python
layer1 = torch.nn.Linear(2, 2)
layer1.weight = torch.nn.Parameter(torch.tensor([[0.5, 0.5], [1, 1]]))
layer1.bias = torch.nn.Parameter(torch.tensor([-0.6, -0.5]))
layer2 = torch.nn.Linear(2, 2)
layer2.weight = torch.nn.Parameter(torch.tensor([[-1.0, 0.0], [0.0, 1.0]]))
layer2.bias = torch.nn.Parameter(torch.tensor([1.0, 0.0]))
layer3 = torch.nn.Linear(2, 2)
layer3.weight = torch.nn.Parameter(torch.tensor([[0.5, 0.5]]))
layer3.bias = torch.nn.Parameter(torch.tensor([-0.6]))

# Pas de couche correspondant √† la fonction de Heaviside en Pytorch, il faut la coder nous m√™me‚ÄØ!
class StepLayer(torch.nn.Module):
    def forward(self, inpt):
        return torch.heaviside(inpt, torch.tensor(0.0))

xor_ffnn = torch.nn.Sequential(
    layer1, StepLayer(), layer2, StepLayer(), layer3, StepLayer()
)

print("x\ty\tx XOR y")
for x_i in [0.0, 1.0]:
    for y_i in [0.0, 1.0]:
        with torch.no_grad():
            out = xor_ffnn(torch.tensor([x_i, y_i]))
        print(f"{x_i}\t{y_i}\t{out}")
```

On peut remarque que la d√©finition du calcul fait par une couche ne se fait pas directement en impl√©mentant `__call__` mais `forward` (le nom vient de l'id√©e que dans un r√©seau les donn√©es **avancent** √† travers les diff√©rentes couches). Pytorch fait plein de magie pour que l'utilisation des algos d'apprentissage soit aussi laconique que possible, et une de ses astuces c'est qu'il d√©finit lui-m√™me `__call__` en prenant le `forward` d√©fini par vous et en faisant d'autres trucs autour.

## Entra√Ænement üî•

Pour entra√Æner un r√©seau en Pytorch, on peut presque directement traduire l'algo de descente de gradient stochastique. Voici par exemple comment on peut entra√Æner un r√©seau √† trois couches logistiques de deux neurones √† apprendre la fonction $\operatorname{XOR}$


On commence par d√©finir un jeu de donn√©es d'apprentissage

```python
train_set = [
    (torch.tensor([0.0, 0.0]), torch.tensor([0.])),
    (torch.tensor([0.0, 1.0]), torch.tensor([1.0])),
    (torch.tensor([1.0, 0.0]), torch.tensor([1.0])),
    (torch.tensor([1.0, 1.0]), torch.tensor([0.0])),
]
```

Le r√©seau

```python
# Dans une fonction comme √ßa c'est facile de le r√©initialiser pour relancer un apprentissage
def get_xor_net():
    return torch.nn.Sequential(
        torch.nn.Linear(2, 2),
        torch.nn.Sigmoid(),
        torch.nn.Linear(2, 2),
        torch.nn.Sigmoid(),
        torch.nn.Linear(2, 1),
        torch.nn.Sigmoid(),
    )
get_xor_net()
```

Et on traduit l'algo

```python
import torch.optim

xor_net = get_xor_net()
# SGD est d√©j√† impl√©ment√©, sous la forme d'un objet auquel on
# passe les param√®tres √† optimiser‚ÄØ: ici les poids du r√©seau
optim = torch.optim.SGD(xor_net.parameters(), lr=0.03)

print("Epoch\tLoss")

# Apprendre XOR n'est pas si rapide, on va faire 50‚ÄØ000 epochs
loss_history = []
for epoch in range(50000):
    # Pour l'affichage
    epoch_loss = 0.0
    # On parcourt le dataset
    for inpt, target in train_set:
        # Le r√©seau pr√©dit une classe
        output = xor_net(inpt)
        # On mesure son erreur avec la log-vraisemblance n√©gative
        loss = torch.nn.functional.binary_cross_entropy(output, target)
        # On calcule le gradient de la loss par rapport √† chacun des
        #¬†param√®tres en ce point.
        # `backward` parce qu'on utilise l'algo de r√©troprogation du gradient
        loss.backward()
        # On applique un pas de l'algo de descente de gradient
        # C'est ici qu'on modifie les poids
        optim.step()
        # On doit remettre les gradients des param√®tres √† z√©ro, sinon ils
        # s'accumulent quand on appelle `backward`
        optim.zero_grad()
        # Pour l'affichage toujours
        epoch_loss += loss.item()
    loss_history.append(epoch_loss)
    if not epoch % 1000:
        print(f"{epoch}\t{epoch_loss}")

print("x\ty\tx XOR y")
for x_i in [0.0, 1.0]:
    for y_i in [0.0, 1.0]:
        with torch.no_grad():
            out = xor_net(torch.tensor([x_i, y_i]))
        print(f"{x_i}\t{y_i}\t{out}")
```

F√©licitations, vous venez d'apprendre un r√©seau de neurones et de vous adonner √† la c√©l√®bre tradition dite ¬´‚ÄØregarder avec anxi√©t√© la loss en esp√©rant qu'elle descende‚ÄØ¬ª.


Et elle est bien descendue, n'est-ce pas‚ÄØ?

```python
fig = plt.figure(dpi=200)
plt.plot(loss_history)
plt.show()
```

Qu'est-ce que √ßa donne comme poids‚ÄØ? Regardons‚ÄØ:

```python
for p in xor_net.parameters():
    print(p.data)
```

Est-ce qu'on peut en tirer des conclusions‚ÄØ? Pas s√ªr‚ÄØ!


On peut au moins regarder la heatmap

```python
x = np.linspace(0, 1, 1000, dtype=np.float64)
y = np.linspace(0, 1, 1000, dtype=np.float64)
X, Y = np.meshgrid(x, y)
inpt = torch.stack((torch.from_numpy(X), torch.from_numpy(Y)), dim=-1).to(torch.float)
with torch.no_grad():
    Z = xor_net(inpt).squeeze().numpy()

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

Pas si mal‚ÄØ! Et voici la fronti√®re de d√©cision

```python
x = np.linspace(0, 1, 1000, dtype=np.float64)
y = np.linspace(0, 1, 1000, dtype=np.float64)
X, Y = np.meshgrid(x, y)
inpt = torch.stack((torch.from_numpy(X), torch.from_numpy(Y)), dim=-1).to(torch.float)
with torch.no_grad():
    Z = xor_net(inpt).gt(0.5).squeeze().numpy()

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

## Aller plus loin

La tradition veut qu'on commence par entra√Æner un mod√®le sur le jeu de donn√©es MNIST‚ÄØ: suivez [le
tutoriel de towards
datascience](https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627) (une
source pas toujours excellente mais dans ce cas pr√©cis √ßa va.

On fait du TAL ici‚ÄØ! Et langage‚ÄØ? Et bien en pratique c'est un peu plus compliqu√© √† traiter que les
images ou les nombres. On se penchera davantage dessus la prochaine fois, mais pour l'instant vous
pouvez faire un peu de classification de documents avec [le tutoriel de
torchtext](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) (qui n'est
pas une biblioth√®que tr√®s souvent populaire, mais elle est bien utile ici. Microsoft propose [un
tutorial
similaire](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch).

Un peu de lecture‚ÄØ: [*Natural Language Processing (almost) from
scratch](https://dl.acm.org/doi/10.5555/1953048.2078186) (Collobert et al., 2011).

[Une super s√©rie de vid√©o](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
avec de belles visus sur [la cha√Æne YouTube 3blue1brown](https://www.youtube.com/c/3blue1brown).
