---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
√âvaluer les mod√®les de langue √† n-grammes‚ÄØ: corrections
===================================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2022-10-05
<!-- #endregion -->

```python
import math
import random
import re
from collections import Counter, defaultdict
```

## Pr√©c√©dently

```python
def crude_tokenizer_and_normalizer(s):
    tokenizer_re = re.compile(
        r"""
        (?:                   # Dans ce groupe, on d√©tecte les mots
            \b\w+?\b          # Un mot c'est des caract√®res du groupe \w, entre deux fronti√®res de mot
            (?:               # √âventuellement suivi de
                '             # Une apostrophe
            |
                (?:-\w+?\b)*  # Ou d'autres mots, s√©par√©s par des traits d'union
            )?
        )
        |\S        # Si on a pas d√©tect√© de mot, on veut bien attraper un truc ici sera forc√©ment une ponctuation
        """,
        re.VERBOSE,
    )
    return tokenizer_re.findall(s.lower())

crude_tokenizer_and_normalizer("La lune et les Pl√©iades sont d√©j√† couch√©es : la nuit a fourni la moiti√© de sa carri√®re, et moi, malheureuse, je suis seule dans mon lit, accabl√©e sous le chagrin.")
```

```python
def extract_ngrams(words, n):
    res = []
    for i in range(len(words)-n+1):
        # Tuple pour pouvoir s'en servir comme cl√© de dictionnaire et donc OK avec `Counter`
        res.append(tuple(words[i:i+n]))
    return res
```

```python
def read_corpus_for_ngrams(file_path, n):
    unigrams = Counter()
    ngrams = Counter()
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            # Il nous faut bien `n-1` symboles de d√©but de phrase 
            words = ["<s>"]*(n-1) + words
            words.append("</s>")
            unigrams.update(words)
            ngrams.update(extract_ngrams(words, n))
    
    return unigrams, ngrams
```

```python
def get_ngram_probs(unigram_counts, ngram_counts):
    probs = defaultdict(dict)
    for (*previous_words, target_word), c in ngram_counts.items():
        probs[tuple(previous_words)][target_word] = c/unigram_counts[target_word]

    return dict(probs)
```

```python
def generate_from_ngrams(ngram_probs):
    # J'avais dit qu'on pouvait. Ici on le fait salement
    n = len(next(iter(ngram_probs.keys()))) + 1
    sent = ["<s>"] * (n-1)
    while sent[-1] != "</s>":
        # Essayer de bien r√©fl√©chir pour comprendre le `1-n`
        previous_words = tuple(sent[1-n:])
        candidates = list(ngram_probs[previous_words].keys())
        weights = [ngram_probs[previous_words][c] for c in candidates]
        sent.append(random.choices(candidates, weights)[0])
    # Pas la peine de renvoyer les tokens <s>
    return sent[n-1:-1]
```

```python
# Unpacking dans un appel de fonction, on va voir qui‚ÄØ? Oui, la doc.
bigram_probs = get_ngram_probs(*read_corpus_for_ngrams("data/zola_ventre-de-paris.txt", 2))

for _ in range(8):
    print(" ".join(generate_from_ngrams(bigram_probs)))
```

```python
# Unpacking dans un appel de fonction, on va voir qui‚ÄØ? Oui, la doc.
trigram_probs = get_ngram_probs(*read_corpus_for_ngrams("data/zola_ventre-de-paris.txt", 3))

for _ in range(8):
    print(" ".join(generate_from_ngrams(trigram_probs)))
```

```python
pentagram_probs = get_ngram_probs(*read_corpus_for_ngrams("data/zola_ventre-de-paris.txt", 5))

for _ in range(8):
    print(" ".join(generate_from_ngrams(pentagram_probs)))
```

## üé≤ Vraisemblance d'une phrase üé≤


√âcrire une fonction `sent_likelihood`, qui prend en argument des probas de n-grammes, une phrase
sous forme de liste de cha√Ænes de caract√®res (et √©ventuellement `n` si vous ne voulez pas utiliser
mon astuce sale du d√©but) et renvoie la vraisemblance de cette phrase.


```python
def sent_likelihood(ngram_probs, sent, n):
    sent = ["<s>"] * (n-1) + sent + ["</s>"]
    total_likelihood = 1.0
    for i in range(len(sent)-n+1):
        total_likelihood = total_likelihood * ngram_probs[tuple(sent[i:i+n-1])][sent[i+n-1]]
    return total_likelihood

assert sent_likelihood(trigram_probs, ["p√©nitentes", ",", "que", "prenez-vous", "?"], 3) == 3.9225257586711874e-14
```

Avec de la syntaxe plus Pythonique

```python
def sent_likelihood(ngram_probs, sent, n):
    # Unpacking, encore et toujours
    sent = [*(["<s>"] * (n-1)), *sent, "</s>"]
    total_likelihood = 1.0
    for i in range(len(sent)-n+1):
        total_likelihood *= ngram_probs[tuple(sent[i:i+n-1])][sent[i+n-1]]
    return total_likelihood

assert sent_likelihood(trigram_probs, ["p√©nitentes", ",", "que", "prenez-vous", "?"], 3) == 3.9225257586711874e-14
```

## ü§òüèª Calculer la perplexit√© ü§òüèª

1\. √âcrire une fonction `sent_loglikelihood`, qui les m√™mes arguments que `sent_likelihood` et
renvoie la **log-vraisemblance** de cette phrase.


```python
def sent_loglikelihood(ngram_probs, sent, n):
    sent = ["<s>"] * (n-1) + sent + ["</s>"]
    result = 0.0
    for i in range(len(sent)-n+1):
        # Notez que du coup, on pourrait d√®s le d√©but calculer les log-probas de n-grammes
        result += math.log(ngram_probs[tuple(sent[i:i+n-1])][sent[i+n-1]])
    return result

assert sent_loglikelihood(trigram_probs, ["p√©nitentes", ",", "que", "prenez-vous", "?"], 3) == -30.86945552941164
```


2\. √âcrire une fonction `avg_log_likelihood`, qui prend qui prend en argument des probas de
n-grammes, un chemin vers un corpus et `n` et renvoie la vraisemblance moyenne de ce corpus estim√©e
par le mod√®le √† n-grammes correspondant √† ces probas (autrement dit, la moyenne des
log-vraisemblances de phrases). Testez sur *Le Ventre de Paris* (non, on l'a dit, c'est pas une
√©valuation juste, mais √ßa va nous permettre de voir si √ßa marche).

```python
def avg_log_likelihood(ngram_probs, file_path, n):
    log_likelihood_sum = 0.0
    n_sents = 0
    with open(file_path) as in_stream:
        for line in in_stream:
            words = crude_tokenizer_and_normalizer(line.strip())
            log_likelihood_sum += sent_loglikelihood(ngram_probs, words, n)
            n_sents += 1
    
    return log_likelihood_sum/n_sents

assert avg_log_likelihood(bigram_probs, "data/zola_ventre-de-paris.txt", 2) == -56.321217776181875
assert avg_log_likelihood(trigram_probs, "data/zola_ventre-de-paris.txt", 3) == -81.20968449380536
assert avg_log_likelihood(pentagram_probs, "data/zola_ventre-de-paris.txt", 5) == -88.25016939038316
```

3\. En pratique, on √©value pas vraiment les mod√®les de langues avec les log-vraisemblances, mais
avec une quantit√© appel√©e **perplexit√©**, d√©finie comme¬†$\exp(-\text{avg-log-l})$, o√π
$\text{avg-log-l}$ est la log-vraisemblance moyenne et $\exp$ est la fonction exponentielle
`math.exp`. √âcrire une fonction `perplexity` qui calcule la perplexit√© d'un mod√®le de langue √†
n-grammes sur un corpus donn√©. L√† encore, testez avec les mod√®les pr√©c√©dents et *Le Ventre de
Paris*.

```python
def perplexity(ngram_probs, file_path, n):
    return math.exp(-avg_log_likelihood(ngram_probs, file_path, n))
```
