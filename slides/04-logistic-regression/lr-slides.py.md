---
jupyter:
  jupytext:
    custom_cell_magics: kql
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: cours-ml
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
TP 4‚ÄØ: R√©gression logistique
===============================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import matplotlib.pyplot as plt
import numpy as np
import polars as pl
import seaborn as sns
import tol_colors as tc
```

## üß† Exo üß†

### 1. Vectoriser un document

Regardez la t√™te du lexique [VADER](https://github.com/cjhutto/vaderSentiment) (vous le
trouverez aussi dans [`data/vader_lexicon.txt`](data/vader_lexicon.txt)). Il contient une liste de
mots (premi√®re colonne) avec pour chaque mot $w$ une polarit√© (deuxi√®me colonne) $p(w)$, positive ou
n√©gative.

1\. √âcrivez une fonction qui lit le lexique VADER et renvoie un `dict` associant √† chaque mot sa
polarit√©.

```python
def read_vader(vader_path: str) -> dict[str, float]:
    pass  #¬†√Ä vous de jouer
```

(Il y a des annotations de types dans la cellule pr√©c√©dente, je vais essayer d'en mettre autant que
possible dans les TPSs futurs. Il est vivement conseill√© de lire
<https://realpython.com/lessons/type-hinting/> et <https://realpython.com/python-type-checking/> et
de parcourir [la doc](https://docs.python.org/3/library/typing.html) √† ce sujet).

√âtant donn√© un document $d$, vu comme un sac de mots de longueur $N$, on peut utiliser VADER pour
donner une repr√©sentation tr√®s (trop) simple de $d$‚ÄØ: un vecteur de deux *features*‚ÄØ: sa polarit√©
positive moyenne $g$ et sa polarit√© n√©gative moyenne $b$, d√©finies par‚ÄØ:

$$
\begin{equation}
    \begin{aligned}
        g &= \frac{1}{N}\sum_{w‚ààd} \max(p(w), 0)\\
        b &= \frac{1}{N}\sum_{w‚ààd} (-\min(p(w), 0))
    \end{aligned}
\end{equation}
$$

(notez bien que $g$ et $b$ sont tous les deux des nombres **positifs**)


2\. √âcrire une fonction qui prend en entr√©e un texte en anglais et le dictionnaire pr√©c√©dent et
renvoie cette repr√©sentation. On pourra supposer que les mots inconnus ont une polarit√© de $0$.

```python
def featurize(doc: str, lexicon: dict[str, float]) -> np.ndarray:
    pass # √Ä vous de jouer‚ÄØ!
```

Vous aurez besoin d'extraire les mots du texte, vous pouvez le faire avec nltk, spacy, ou comme
scikit-learn, sauvagement avec une regex comme `\b\w+\b'`.

```python
lexicon = read_vader("data/vader_lexicon.txt")
doc = "I came in in the middle of this film so I had no idea about any credits or even its title till I looked it up here, where I see that it has received a mixed reception by your commentators. I'm on the positive side regarding this film but one thing really caught my attention as I watched: the beautiful and sensitive score written in a Coplandesque Americana style. My surprise was great when I discovered the score to have been written by none other than John Williams himself. True he has written sensitive and poignant scores such as Schindler's List but one usually associates his name with such bombasticities as Star Wars. But in my opinion what Williams has written for this movie surpasses anything I've ever heard of his for tenderness, sensitivity and beauty, fully in keeping with the tender and lovely plot of the movie. And another recent score of his, for Catch Me if You Can, shows still more wit and sophistication. As to Stanley and Iris, I like education movies like How Green was my Valley and Konrack, that one with John Voigt and his young African American charges in South Carolina, and Danny deVito's Renaissance Man, etc. They tell a necessary story of intellectual and spiritual awakening, a story which can't be told often enough. This one is an excellent addition to that genre."
doc_features = featurize(doc, lexicon)
doc_features
```

### 2. Vectoriser un corpus

Utiliser la fonction pr√©c√©dente pour vectoriser [le mini-corpus IMDB](data/imdb_smol.tar.gz)‚ÄØ:
g√©n√©rez un `DataFrame` polars o√π chaque document est repr√©sent√© par une ligne, les colonnes √©tant
`g`, `b` et `cls`, la classe (`"pos"` ou `"neg"`) du document (qui vous est donn√©e par son
sous-dossier).

Rappel‚ÄØ: vous pouvez bien s√ªr commencer par vectoriser chaque document et ensuite construire un
DataFrame, mais en ce qui me concerne, je pr√©f√®re faire l'inverse‚ÄØ: construire d'abord un DataFrame
avec les contenus des documents et leurs classes et construire les colonnes `g` et `b` en utilisant
[`map_elements()`](https://docs.pola.rs/user-guide/expressions/user-defined-functions/).

(R√©√©crire `featurize` comme une expression polars, c'est *possible*, c'est pas tr√®s agr√©able, mais
c'est plus rapide. √Ä vous de voir selon vos go√ªts.)

```python
def featurize_dir(corpus_root_path: str, lexicon: dict[str, float]) -> pl.DataFrame:
    pass # √Ä vous!

# On r√©utilise le lexique pr√©c√©dent
featurize_dir("data/imdb_smol", lexicon)
```

## Visualisation

Comment se r√©partissent les documents du corpus avec la repr√©sentation qu'on a choisi‚ÄØ?

```python
lexicon = read_vader("data/vader_lexicon.txt")
imdb_features = featurize_dir("data/imdb_smol", lexicon)

X = imdb_features.get_column("g").to_numpy()
Y = imdb_features.get_column("b").to_numpy()
H = imdb_features.get_column("cls").to_numpy()

fig = plt.figure(dpi=200)
sns.scatterplot(x=X, y=Y, hue=H, s=5)
plt.show()
```

On voit des tendances qui se d√©gagent, mais clairement √ßa va √™tre un peu coton

## Classifieur lin√©aire

On consid√®re des vecteurs de *features* de dimension $n$

$$\mathbf{x} = (x‚ÇÅ, ‚Ä¶, x_n)$$

Un vecteur de poids de dimension $n$

$$\mathbf{Œ±} = (Œ±‚ÇÅ, ‚Ä¶, Œ±_n)$$

et un biais $Œ≤$ scalaire (un nombre quoi).

Pour r√©aliser une classification on consid√®re le nombre $z$

$$z=Œ±‚ÇÅ√óx‚ÇÅ + ‚Ä¶ + Œ±_n√óx_n + Œ≤ = \sum_iŒ±_ix_i + Œ≤$$

Ce qu'on note aussi

$$z = \mathbf{Œ±}‚ãÖ\mathbf{x}+Œ≤$$

$\mathbf{Œ±}‚ãÖ\mathbf{x}$ se lit ¬´‚ÄØalpha scalaire x‚ÄØ¬ª, on parle de *produit scalaire* en fran√ßais et de
*inner product* en anglais.

(ou pour les math√©maticien‚ãÖnes acharn√©‚ãÖes $z = \langle Œ±\ |\ x \rangle + Œ≤$)

Quelle que soit la fa√ßon dont on le note, on affectera √† $\mathbf{x}$ la classe $0$ si $z < 0$ et la
classe $1$ sinon.

## üò¥ Exo üò¥

### 1. Une fonction affine

√âcrire une fonction qui prend en entr√©e un vecteur de features et un vecteur de poids sous forme de
tableaux numpy $x$ et $Œ±$ de dimensions `(n,)` et un biais $Œ≤$ sous forme d'un tableau numpy de
dimensions `(1,)` et renvoie $z=\sum_iŒ±_ix_i + Œ≤$.

```python
def affine_combination(x: np.ndarray, alpha: np.ndarray, beta: np.ndarray) -> np.ndarray:
    # Attention, j'ai typ√© la valeur de retour comme un ndarray, mais il devra √™tre
    # de forme (1,)
    pass # √Ä vous de jouer‚ÄØ!

affine_combination(
    np.array([2, 0, 2, 1]),
    np.array([-0.2, 999.1, 0.5, 2]),
    np.array([1]),
)
```

### 2. Un classifieur lin√©aire

1\. √âcrire un classifieur lin√©aire qui prend en entr√©e des vecteurs de features √† deux dimensions
pr√©c√©dents et utilise les poids respectifs $0.6$ et $-0.4$ et un biais de $-0.01$.

```python
lexicon = read_vader("data/vader_lexicon.txt")
imdb_features = featurize_dir("data/imdb_smol", lexicon)
```

```python
def hardcoded_classifier(x: np.ndarray) -> str:
    return "pos"  # √Ä vous de jouer

hardcoded_classifier(np.array([2.7, 1.3]))
```

2\. Appliquez ce classifieur sur le mini-corpus IMDB qu'on a vectoris√© et calculez son exactitude.

```python
# Faites marcher cette cellule. Il faudra sans doute √©crire une fonction.

classifier_accuracy(np.array([0.6, -0.4]), np.array(-0.01), imdb_features)
```

## Classifieur lin√©aire‚ÄØ?

Pourquoi lin√©aire‚ÄØ? Regardez la figure suivante qui colore les points $(x,y)$ du plan en fonction de
la valeur de $z$.

```python
x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) - 0.01

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

Ou encore plus clairement, si on repr√©sente la classe assign√©e

```python

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) -0.01 > 0.0

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On voit bien que la fronti√®re de classification est une droite, *a line*. On a donc un *linear*
classifier‚ÄØ: un classifieur lin√©aire (m√™me si en fran√ßais on dirait qu'il s'agit d'une fonction
*affine*).


Qu'est-ce que √ßa donne si on superpose avec notre corpus‚ÄØ?

```python
fig = plt.figure(dpi=200)

x = np.linspace(0, 0.4, 1000)
y = np.linspace(0, 0.4, 1000)
X, Y = np.meshgrid(x, y)
Z = (0.6*X - 0.4*Y) -0.01 > 0.0

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))

X = imdb_features.get_column("g").to_numpy()
Y = imdb_features.get_column("b").to_numpy()
H = imdb_features.get_column("cls").to_numpy()

plt.scatter(x=X, y=Y, c=H, cmap="viridis", s=5)

plt.show()
```

Pas si surprenant que nos r√©sultats ne soient pas terribles‚Ä¶

## La fonction logistique


$$œÉ(z) = \frac{1}{1 + e^{‚àíz}} = \frac{1}{1 + \exp(‚àíz)}$$

Elle permet de *normaliser* $z$‚ÄØ: $z$ peut √™tre n'importe quel nombre entre $-‚àû$ et $+‚àû$, mais on
aura toujours $0 <¬†œÉ(z) < 1$, ce qui permet de l'interpr√©ter facilement comme une *vraisemblance*.
Autrement dit, $œÉ(z)$ sera proche de $1$ s'il para√Æt vraisemblable que $x$ appartienne √† la classe
$1$ et proche de $0$ sinon.

## üìà Exo üìà


1\. D√©finir la fonction `logistic` qui prend en entr√©e un tableau numpy $z=[z_1,‚ÄØ‚Ä¶, z_n]$ et
renvoie le tableau $[œÉ(z_1), ‚Ä¶ , œÉ(z_n)]$.

```python
def logistic(z: np.ndarray) -> np.ndarray:
    pass  #¬†√Ä vous
```

2\. Tracer avec matplotlib la courbe repr√©sentative de la fonction logistique.

```python

```

## R√©gression logistique

Formellement‚ÄØ: on suppose qu'il existe une fonction $f$ qui pr√©dit parfaitement les classes, donc
telle que pour tout couple exemple/classe $(x, y)$ avec $y$ valant $0$ ou $1$, $f(x) = y$. On
voudrait approcher cette fonction par une fonction $M$ de la forme

$$M(x) = œÉ(\langle Œ±\ |\ x \rangle + Œ≤)$$

Si on choisit les poids $Œ±$ et le biais $Œ≤$ tels que $M$ soit la plus proche possible de $f$ sur
notre ensemble d'apprentissage, on dit que $M$ est la *r√©gression logistique de $f$* sur cet
ensemble.

Un classifieur logistique, c'est simplement un classifieur qui pour un exemple $x$ renvoie $0$ si
$M(x) < 0.5$ et $1$ sinon. Il a exactement les m√™mes capacit√©s de discrimination qu'un classifieur
lin√©aire (sa fronti√®re de d√©cision est la m√™me et il ne sait donc pas prendre de d√©cisions plus
complexes), mais on a acc√®s √† un score qui peut √™tre interpr√©t√© comme la confiance qu'il a dans sa
d√©cision.


Par exemple voici la confiance que notre classifieur cod√© en dur a en ses d√©cisions

```python
def classifier_confidence(x: np.ndarray) -> np.ndarray:
    return logistic(affine_combination(x, np.array([0.6, -0.4]), -0.01))

doc_features = featurize(doc, lexicon)
M_x = classifier_confidence(doc_features)
display(M_x)
display(Markdown(f"Le classifieur est s√ªr √† {M_x:.06%} que ce document est dans la classe $1$."))
display(Markdown(f"Autrement dit, d'apr√®s le classifieur, la classe $1$ a {M_x:.06%} de vraisemblance pour ce document"))
```


Quelle est la vraisemblance de la classe $0$ (*review* n√©gative)‚ÄØ? Et bien le reste

```python
1.0 - classifier_confidence(doc_features)
```

Comme l'exemple en question appartient bien √† cette classe, √ßa signifie que notre classifieur et
plut√¥t bon **sur cet exemple**. L'est-il sur le reste du corpus‚ÄØ?

```python
pos_confidence = sum(classifier_confidence(doc) for doc in imdb_features["pos"])
print(f"Average confidence for 'pos': {pos_confidence/len(imdb_features['pos']):.02%}")
neg_confidence = sum(1-classifier_confidence(doc) for doc in imdb_features["neg"])
print(f"Average confidence for 'neg': {neg_confidence/len(imdb_features['neg']):.02%}")
print(f"Average confidence for the correct class: {(pos_confidence+neg_confidence)/(len(imdb_features['pos']) + len(imdb_features['neg'])):.02%}")
```

Autrement dit, pour un exemple pris au hasard dans le corpus, la vraisemblance de sa classe telle
que jug√©e par le classifieur sera de $50.49\%$. Un classifieur parfait obtiendrait $100\%$, un
classifieur qui prendrait syst√©matiquement la mauvaise d√©cision $0\%$ et un classifieur al√©atoire
uniforme $50\%$ (puisque notre corpus a autant d'exemples de chaque classe).


Moralit√©‚ÄØ: nos poids ne sont pas tr√®s bien choisis, et notre pr√©occupation dans la suite va √™tre de
chercher comment choisir des poids pour que la confiance moyenne de la classe correcte soit aussi
haute que possible.


**Attention** le score que nous donne le classifieur peut √™tre vu sa confiance dans la d√©cision prise **mais** √ßa ne veut pas dire que ce score a beaucoup de valeur pour √©tudier le mod√®le. De fait, la confiance en question n'est que tr√®s rarement corr√©l√©e avec l'exactitude des pr√©cisions. 

Autrement dit‚ÄØ: **quand un classifieur logistique se trompe, il a tendance √† le faire avec beaucoup de confiance mal plac√©e.**

## Fonction de co√ªt

On a dit que notre objectif √©tait

> Chercher les poids $Œ±$ et le biais $Œ≤$ tels que $M$ soit la plus proche possible de $f$ sur notre
ensemble d'apprentissage

On formalise ¬´‚ÄØ√™tre le plus proche possible‚ÄØ¬ª de la section pr√©c√©dente comme **minimiser** une
certaine fonction de **co√ªt** (*loss*) $\mathcal{L}$ qui mesure l'erreur faite par le classifieur.

On d√©ifinit souvent $\mathcal{L}$ en √©valuant d'abord un co√ªt *local* $L$ pour chaque exemple¬†:

$$L(M(x), y) = \text{l'√©cart entre la classe $≈∑=M(x)$ pr√©dite par $M$ pour $x$ et la classe correcte $y$}$$

Puis, √©tant donn√© un ensemble de test $\mathcal{D}_t = \{(x‚ÇÅ, y‚ÇÅ),‚ÄØ‚Ä¶, (x_n, y_n)\}$, on d√©finit $\mathcal{L}$ comme le co√ªt total‚ÄØ:

$$\mathcal{L}(M, \mathcal{D}_t) = \sum_i L(M(x·µ¢), y·µ¢)$$

Plus $\mathcal{L}$ sera bas, meilleur sera notre classifieur.




Dans le cas de la r√©gression logistique, on va s'inspirer de ce qu'on a vu dans la section
pr√©c√©dente et utiliser la *log-vraisemblance n√©gative* (*negative log-likelihood*)‚ÄØ:

On d√©finit la *vraisemblance* $V$ comme pr√©c√©demment par
$$
V(M(x), y) =
    \begin{cases}
        M(x) & \text{si $y = 1$}\\
        1-M(x) & \text{sinon}
    \end{cases}
$$

Intuitivement, il s'agit du score attribu√© par le mod√®le √† la classe correcte $y$. Il ne
s'agit donc pas d'un co√ªt, mais d'un *gain* (si sa valeur est haute, c'est que le mod√®le est bon)

La *log-vraisemblance n√©gative* $L$ est alors d√©finie par

$$L(M(x), y) = -\log(V(M(x), y))$$

Le $\log$ est l√† pour plusieurs raisons, calculatoires et th√©oriques et le $-$ √†
s'assurer qu'on a bien un co√ªt (plus la valeur est basse, meilleur le mod√®le est).

<details>
<summary>Details</summary>
Entre autres, parce qu'une somme de $\log$-vraisemblances peut
√™tre vue comme le $\log$ de la probabilit√© d'une conjonction d'√©v√©nements ind√©pendants. Mais surtout
parce qu'il rend la fonction de co√ªt **convexe** par rapport √† $(Œ±, Œ≤)$.

Une interpr√©tation possible‚ÄØ: $L(M(x), y)$, c'est la
[surprise](https://en.wikipedia.org/wiki/Information_content) de $y$ au sens de la th√©orie de
l'information. Autrement dit‚ÄØ: si j'estime qu'il y a une probabilit√© $a$ d'observer la classe $y$,
$L(a, y)$ mesure √† quel point il serait surprenant d'observer effectivement $y$.
</details>


On peut v√©rifier qu'il s'agit bien d'un co√ªt‚ÄØ:

- C'est un nombre positif
- Si le classifieur prend une d√©cision correcte avec une confiance parfaite le co√ªt est nul‚ÄØ:

  $$
    \begin{cases}
        L(1.0, 1) = -\log(1.0) = 0\\
        L(0.0, 0) = -\log(1.0-0.0) = -\log(1.0) = 0
    \end{cases}
  $$
- Si le classifieur prend une d√©cision erron√©e avec une confiance parfaite le co√ªt est infini‚ÄØ:

  $$
    \begin{cases}
        L(0.0, 1) = -\log(0.0) = +\infty\\
        L(1.0, 0) = -\log(1.0-1.0) = \log(0.0) = +\infty
    \end{cases}
  $$

On peut aussi v√©rifier facilement que $L(M(x), 1)$ est d√©croissant par rapport √† $M(x)$ et que $L(1-M(x), 0)$
est croissant par rapport √† $M(x)$. Autrement dit, plus le classifieur juge que la classe correcte est
vraisemblable plus le co√ªt $L$ est bas.


Enfin, on peut √©crire $L$ en une ligne‚ÄØ: pour un exemple $x$, le co√ªt de l'exemple $(x, y)$ est

$$L(M(x), y) = -\log\left[M(x)√óy + (1-M(x))√ó(1-y)\right]$$

C'est une astuce‚ÄØ: comme $y$ vaut soit $0$ soit $1$, on a  soit $y=0$, soit $1-y=0$, et donc la
somme dans le $\log$ se simplifie dans tous les cas.

Une derni√®re fa√ßon de l'√©crire en une ligne‚ÄØ:

$$L(M(x), y) = -\log\left[M(x)\mathbb{1}_{y=1} + (1-M(x))\mathbb{1}_{y=0}\right]$$

<details>
<summary>Si vous lisez SLP</summary>
La formule diff√®re un peu de celle de *Speech and Language Processing*, mais les r√©sultats sont les
m√™mes et celle-ci est mieux pour notre probl√®me‚ÄØ!

En fait la leur est la formule g√©n√©rale de l'entropie crois√©e pour des distributions de proba
√† support dans $\{0, 1\}$, ce qui est une autre intuition pour cette fonction de co√ªt, mais ici elle
nous complique la vie.
</details>

## üìâ Exo üìâ

√âcrire une fonction qui prend en entr√©e

- Un vecteur de features $x$ de taille $n$
- Un vecteur de poids $Œ±$ de taille $n$ et un biais $Œ≤$ (de taille $1$)
- Une classe cible $y$ ($0$ ou $1$)

Et renvoie la log-vraisemblance n√©gative du classifieur logistique de poids $(Œ±, Œ≤)$ pour l'exemple
$(x, y)$.

Servez-vous en pour calculer le co√ªt du classifieur de l'exercise pr√©c√©dent sur le mini-corpus IMDB.

Le r√©sultat devrait ressembler √† √ßa

```python
loss_on_imdb(np.array([0.6, -0.4]), -0.01, imdb_features)
```

<!-- #region -->
## Descente de gradient

### Principe g√©n√©ral

L'**algorithme de descente de gradient** est la cl√© de voute de l'essentiel des travaux en
apprentissage artificiel moderne. Il s'agit d'un algorithme it√©ratif qui √©tant donn√© un mod√®le
param√©tris√© et une fonction de co√ªt (avec des hypoth√®ses de r√©gularit√© assez faibles) permet de
trouver des valeurs des param√®tres pour lesquelles la fonction de co√ªt est minimal.

On ne va pas rentrer dans les d√©tails de l'algorithme de descente de gradient stochastique, mais
juste essayer de se donner quelques id√©es.

L'intuition √† avoir est la suivante‚ÄØ: si vous √™tes dans une vall√©e et que vous voulez trouver
rapidement le point le plus bas, une fa√ßon de faire est de chercher la direction vers laquelle la
pente descend le plus vite, de faire quelques pas dans cette direction puis de recommencer. On parle
aussi pour cette raison d'**algorithme de la plus forte pente**.

Clairement une condition pour que √ßa marche peu importe le point de d√©part, c'est que la vall√©e
n'ait qu'un seul point localement le plus bas. Par exemple √ßa marche avec une vall√©e comme celle-ci
<!-- #endregion -->

```python
fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = R**2 - 1

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

Mais pas pour celle-l√†

```python
fig = plt.figure(figsize=(20, 20), dpi=200)
ax = plt.axes(projection='3d')

r = np.linspace(0, 8, 100)
p = np.linspace(0, 2*np.pi, 100)
R, P = np.meshgrid(r, p)
Z = -np.cos(R)/(1+0.5*R**2)

X, Y = R*np.cos(P), R*np.sin(P)

ax.plot_surface(X, Y, Z, cmap=tc.tol_cmap("sunset"), edgecolor="none", rstride=1, cstride=1)
# ax.plot_wireframe(X, Y, Z, color='black')

plt.show()
```

OK, mais comment on trouve la plus forte pente en pratique‚ÄØ? En une dimension il suffit de suivre
l'oppos√© du nombre d√©riv√©‚ÄØ: <https://uclaacm.github.io/gradient-descent-visualiser/#playground>


En plus de dimensions, c'est plus compliqu√©, mais on peut s'en sortir en suivant le *gradient* qui
est une g√©n√©ralisation du nombre d√©riv√©‚ÄØ: <https://jackmckew.dev/3d-gradient-descent-in-python.html>


Ce qui fait marcher la machine c'est que **le gradient indique la direction dans laquelle la
fonction cro√Æt le plus vite**. Et que l'oppos√© du gradient indique la direction dans laquelle la
fonction d√©cro√Æt le plus vite.

(localement)

<!-- #region -->
Concr√®tement si on veut trouver $\theta$ tel que $f(\theta)$ soit minimale pour une certaine
fonction $f$ dont le gradient est donn√© par `grad_f`, √ßa donne l'algo suivant

```python
def descent(grad_f: np.ndarray, theta_0: np.ndarray, learning_rate: float, n_steps: int) -> np.ndarray:
    theta = theta_0
    for _ in range(n_steps):
        # On trouve la direction de plus grande pente
        steepest_direction = -grad_f(theta)
        #¬†On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
    return theta
```

Les *hyperparam√®tres* sont

- `theta_0` est notre point de d√©part, notre premi√®re estimation d'o√π se trouvera le minimum, que
  l'algorithme va raffiner. √âvidemment si on a d√©j√† une id√©e de vers o√π on pourrait le trouver, √ßa
  ira plus vite. Si on a aucune id√©e, on peut le prendre al√©atoire.
- `learning_rate` ou ¬´‚ÄØtaux d'apprentissage‚ÄØ¬ª‚ÄØ: de combien on se d√©place √† chaque √©tape. Si on le
  prend grand on arrive vite vers la r√©gion du minimum, on mettra longtemps pour en trouver une
  approximation pr√©cise. Si on le prend petit, √ßa sera l'inverse.
- `n_steps` est le nombre d'√©tapes d'optimisations. Dans un probl√®me d'apprentissage, c'est aussi le
  nombre de fois o√π on aura parcouru l'ensemble d'apprentissage et on parle souvent d'**epoch**

Ici on se donne un nombre fixe¬†d'epochs, une autre possibilit√© serait de s'arr√™ter quand on ne bouge
plus trop, par exemple avec une condition comme

```python
if np.max(grad_f(theta)) < 0.00001:
    break
```

dans la boucle et √©ventuellement avec une boucle infinie `while True`.
<!-- #endregion -->

Point notation‚ÄØ:

- **Le gradient de $f$** est souvent not√© $\nabla f$ ou $\operatorname{grad}f$, voire $\vec\nabla f$
  ou $\overrightarrow{\operatorname{grad}} f$ (pour dire que c'est un vecteur)
- Si $Œ∏=(Œ∏_1, ‚Ä¶, Œ∏_n)$, autrement dit si $f$ est une fonction de $n$ variables, on note
  $\operatorname{grad}f = \left(\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_1}, ‚Ä¶, \frac{‚àÇf(Œ∏)}{‚àÇŒ∏_n}\right)$. Autrement dit
  $\frac{‚àÇf(Œ∏)}{‚àÇŒ∏_i}$, la **d√©riv√©e partielle** de $f(Œ∏)$ par rapport √† $Œ∏_i$, est la $i$-√®me
  coordonn√©es du gradient de $f$.
- **Le taux d'apprentissage** est souvent not√© $Œ∑$ (ou $Œ±$ mais ici c'est d√©j√† quelque chose)

L'√©tape de mise √† jour de $Œ∏$ peut donc se noter

$$Œ∏ ‚Üê Œ∏ - Œ∑ \operatorname{grad}f


### Descente de gradient stochastique

Rappelez-vous, on a dit que notre fonction de co√ªt, c'√©tait

$$\mathcal{L} = \sum_i L(M(x·µ¢), y·µ¢)$$

et on cherche la valeur du param√®tre $Œ∏ = (Œ±_1, ‚Ä¶, Œ±_n, Œ≤)$ tel que $\mathcal{L}$ soit le plus petit
possible.


On peut utilise la propri√©t√© d'**additivit√©** du gradient‚ÄØ: pour deux fonctions $f$ et $g$, on a

$$\operatorname{grad}(f+g) = \operatorname{grad}f + \operatorname{grad}g$$

Donc ici

$$\operatorname{grad}_{Œ∏}\mathcal{L} = \sum_i \operatorname{grad}_{Œ∏}L(M(x·µ¢), y·µ¢)$$

<!-- #region -->
Si on dispose d'une fonction `grad_L` qui, √©tant donn√©s $M(x_i)$ et $y_i$, renvoie
$\operatorname{grad}_{Œ∏}L(M(x_i), y_i)$, l'algorithme de descente du gradient devient alors

```python
def descent(train_set: np.ndarray, theta_0: np.ndarray, learning_rate: float, n_steps: int) -> np.ndarray:
    theta = theta_0
    for _ in range(n_steps):
        alpha = theta[:-1]
        beta = theta[-1]
        partial_grads = []
        for (x, y) in train_set:
            #¬†On calcule g(x)
            M_x = logistic(np.inner(alpha, x)+beta)
            #¬†On calcule le gradient de L(g(x), y))
            partial_grads.append(grad_L(M_x, y))
        # On trouve la direction de plus grande pente
        steepest_direction = -np.sum(partial_grads)
        # On fait quelques pas dans cette direction
        theta += learning_rate*steepest_direction
        
    return theta
```
<!-- #endregion -->

Pour chaque √©tape, on doit calculer tous les $M(x_i)$ et $\operatorname{grad}_{Œ∏}L(M(x_i), y_i)$. C'est
tr√®s couteux, il doit y avoir moyen de faire mieux.


Si les $L(M(x·µ¢), y·µ¢)$ √©taient ind√©pendants, ce serait plus simple‚ÄØ: on pourrait les optimiser
s√©par√©ment.


Ce n'est √©videmment pas le cas‚ÄØ: si on change $M$ pour que $M(x_0)$ soit plus proche de $y_0$, √ßa
changera aussi la valeur de $M(x_1)$.


**Mais on va faire comme si**

<!-- #region -->
C'est une approximation sauvage, mais apr√®s tout on commence √† avoir l'habitude. On va donc suivre
l'algo suivant

```python
def descent(train_set: np.ndarray, theta_0: np.ndarray, learning_rate: float, n_steps: int) -> np.ndarray:
    theta = theta_0
    for _ in range(n_steps):
        for (x, y) in train_set:
            alpha = theta[:-1]
            beta = theta[-1]
            # On calcule g(x)
            M_x = logistic(np.inner(alpha, x)+beta)
            # On trouve la direction de plus grande pente
            steepest_direction = -grad_L(M_x, y)
            # On fait quelques pas dans cette direction
            theta += learning_rate*steepest_direction

    return theta
```
<!-- #endregion -->

Faites bien attention √† la diff√©rence‚ÄØ: au lieu d'attendre d'avoir calcul√© tous les
$\operatorname{grad}L(M(x_i), y_i)$ avant de modifier $Œ∏$, on va le modifier √† chaque fois.


- **Avantage**‚ÄØ: on modifie beaucoup plus souvent le param√®tre, si tout se passe bien, on devrait
  arriver √† une bonne approximation tr√®s vite.
- **Inconv√©nient**‚ÄØ: il se pourrait qu'en essayant de faire baisser $L(M(x_0), y_0)$, on fasse
  augmenter $L(M(x_1), y_1)$.

Notre espoir ici, c'est que cette situation n'arrivera pas, et qu'un bon param√®tre pour un certain
couple $(x, y)$ soit aussi un bon param√®tre pour $tous$ les couples `(exemple, classe)`.


Ce nouvel algorithme s'appelle l'**algorithme de descente de gradient stochastique**, et il est
crucial pour nous, parce qu'on ne pourra en pratique quasiment jamais faire de descente de gradient
globale.


Il ne nous reste plus qu'√† savoir comment on calcule `grad_L`. On ne fera pas la preuve, mais on a


$$
\begin{equation}
    \operatorname{grad}_Œ∏L(M(x), y) =
        \begin{pmatrix}
            \frac{‚àÇL(M(x), y)}{‚àÇŒ±_1}\\
            \vdots\\
             \frac{‚àÇL(M(x), y)}{‚àÇŒ±_n}\\
             \frac{‚àÇL(M(x), y)}{‚àÇŒ≤}\\
        \end{pmatrix}
\end{equation}
$$


Avec pour tout $i$

$$\frac{‚àÇL(M(x), y)}{‚àÇŒ±_i} = (M(x)-y)x_i$$


et


$$\frac{‚àÇL(M(x), y)}{‚àÇŒ≤} = M(x)-y$$

<!-- #region -->
## üßê Exo üßê

### 1. Calculer le gradient

√âcrire une fonction qui calcule $\operatorname{grad}_Œ∏L(M(x), y)$.
<!-- #endregion -->

```python
def grad_L(x: np.ndarray, alpha: np.ndarray, beta: np.ndarray, y: np.ndarray) -> np.ndarray:
    grad = np.zeros(alpha.size+beta.size)  # √Ä vous‚ÄØ!
    return grad

grad_L(np.array([5, 10]), np.array([0.6, -0.4]), np.array([-0.01]), 0)
```

### 2. Descendre le gradient

S'en servir pour apprendre les poids √† donner aux *features* pr√©c√©dentes √† l'aide du [mini-corpus
IMDB](data/imdb_smol.tar.gz) en utilisant l'algorithme de descente de gradient stochastique.

```python
def descent(
    featurized_corpus: pl.DataFrame,
    theta_0: np.ndarray,
    learning_rate: float,
    n_steps: int
) -> np.ndarray:
    theta = theta_0
    for _ in range(n_steps):
        pass  # √Ä vous‚ÄØ!


descent(imdb_features, np.array([0.6, -0.4, 0.0]), 0.001, 100)
```

## R√©gression multinomiale

Un dernier point‚ÄØ: on a vu dans tout ceci comment utiliser la r√©gression logistique pour un probl√®me
de classification √† deux classes. Comment on l'√©tend √† $n$ classes‚ÄØ?


R√©fl√©chissons d√©j√† √† quoi ressemblerait la sortie d'un tel classifieur‚ÄØ:

Pour un probl√®me √† deux classes, le classifieur $M$ nous donne pour chaque exemple $x$ une
estimation $M(x)$ de la vraisemblance de la classe $1$, et on a vu que la vraisemblance de la classe
$0$ √©tait n√©cessairement $1-g(x)$ pour que la somme des vraisemblances fasse 1.


On peut le pr√©senter autrement‚ÄØ: consid√©rons le classifieur $K$ tel que pour tout exemple $x$

$$K(x) = \begin{pmatrix}1-M(x)\\M(x)\end{pmatrix}$$

$K$ nous donne un vecteur √† deux coordonn√©es, $K_0(x)$ et $K_1(x)$, qui sont respectivement les
vraisemblances des classes $0$ et $1$.


Pour un probl√®me √† $n$ classes, on va vouloir une vraisemblance par classe, on va donc proc√©der de
la fa√ßon suivante‚ÄØ:

On consid√®re des poids $(Œ±_1, Œ≤_1), ‚Ä¶, (Œ±_n, Œ≤_n)$. Ils d√©finissent un classifieur lin√©aire.

En effet, si on consid√®re les $z_i$ d√©finis pour tout exemple $x$ par

$$
    \begin{cases}
        z_1 = Œ±_1‚ãÖx + Œ≤_1\\
        \vdots\\
        z_n = Œ±_n‚ãÖx + Œ≤_n
    \end{cases}
$$

On peut choisir la classe $y$ √† affecter √† $x$ en prenant $y=\operatorname{argmax}\limits_i z_i$


Il reste √† normaliser pour avoir des vraisemblances. Pour √ßa on utilise une fonction **tr√®s**
importante‚ÄØ: la fonction $\operatorname{softmax}$, d√©finie ainsi‚ÄØ:

$$\operatorname{softmax}(z_1, ‚Ä¶, z_n) = \left(\frac{e^{z_1}}{\sum_i e^{z_i}}, ‚Ä¶,
\frac{e^{z_n}}{\sum_i e^{z_i}}\right)$$

Contrairement √† la fonction logistique qui prenait un nombre en entr√©e et renvoyait un nombre,
$\operatorname{softmax}$ prend en entr√©e un **vecteur** non-normalis√© et renvoie un vecteur
normalis√©.


Pourquoi elle s'appelle *softmax*‚ÄØ? Consid√©rez le vecteur $v = (0.1, -0.5, 2.1, 2, 1.6)$. Son
maximum $\max(v)$, c'est $2.1$, et ce qu'on appelle $\operatorname{argmax}(v)$, la position du
maximum, c'est $3$.

Pour *argmax* √† la place d'une position, on peut aussi le voir comme un masque‚ÄØ: $(0, 0, 1, 0, 0)$,
autrement dit un vecteur dont les valeurs sont $0$ pour chaque position, sauf la position du
maximum, qui contient un $1$ (on parle de repr√©sentation *one-hot*). Visualisons ces vecteur‚ÄØ:

```python
v = np.array([0.1, -0.5, 2.1, 1.8, 0.6])
```

```python
plt.bar(np.arange(v.shape[0]), v)
plt.title("Coordonn√©es de $v$")
plt.show()
```


```python
plt.bar(np.arange(v.shape[0]), v == v.max())
plt.title("Coordonn√©es de $\operatorname{argmax}(v)$")
plt.show()
```

Et softmax‚ÄØ? Regardez (on l'importe depuis
[SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html), un des
adelphes de NumPy, pour ne pas avoir √† le recoder nous-m√™me).

```python
from scipy.special import softmax
plt.bar(np.arange(v.shape[0]), softmax(v))
plt.title("Coordonn√©es de $\operatorname{softmax}(v)$")
plt.show()
```

On voit que c'est un genre d'entre-deux entre $v$ et $\operatorname{argmax}(v)$‚ÄØ:

- La distribution des coordonn√©es ressemble √† celle de $v$‚Ä¶
- mais les coordonn√©es sont toutes entre $0$ et $1$.
- Le max est tir√© vers $1$ et toutes les autres coordonn√©es vers $0$.
- La somme des coordonn√©es fait $1$.

Si, si, je vous assure‚ÄØ:

```python
softmax(v).sum()
```

√Ä l'erreur d'arrondi en virgule flottante pr√®s on est bien.


Autrement dit, on a, comme pour la fonction logistique, une fonction qui *normalise* les valeurs
tout en pr√©servant certaines propri√©t√©s.


Revenons √† nos moutons‚ÄØ: on d√©finit enfin le classifieur logistique multinomial $f$ de la fa√ßon
suivante‚ÄØ: pour tout exemple $x$, on a

$$f(x) = \operatorname{softmax}(Œ±_1‚ãÖx+Œ≤_1, ‚Ä¶, Œ±_n‚ãÖx+Œ≤_n) = \left(\frac{e^{Œ±_1‚ãÖx+Œ≤_1}}{\sum_i
e^{Œ±_i‚ãÖx+Œ≤_i}}, ‚Ä¶, \frac{e^{Œ±_n‚ãÖx+Œ≤_n}}{\sum_i e^{Œ±_i‚ãÖx+Œ≤_i}}\right)$$

et on choisit pour $x$ la classe

$$y = \operatorname{argmax}\limits_i f_i(x) = \operatorname{argmax}\limits_i
\frac{e^{Œ±_i‚ãÖx+Œ≤_i}}{\sum_j e^{Œ±_j‚ãÖx+Œ≤_j}}$$

Comme la fonction exponentielle est croissante, ce sera la m√™me classe que le classifieur lin√©aire
pr√©c√©dent. Comme pour le cas √† deux classe, la diff√©rence se fera lors de l'apprentissage. Je vous
laisse aller en lire les d√©tails dans *Speech and Language Processing*, mais l'id√©e est la m√™me‚ÄØ: on
utilise la log-vraisemblance n√©gative de la classe correcte comme fonction de co√ªt, et on optimise
les param√®tres avec l'algo de descente de gradient stochastique.


Un dernier d√©tail‚ÄØ?

Qu'est-ce qui se passe si on prend ce qu'on vient de voir pour $n=2$‚ÄØ? Est-ce qu'on retombe sur le
cas √† deux classe vu pr√©c√©demment‚ÄØ?


Oui, regarde‚ÄØ: dans ce cas

$$
    \begin{align}
        f_1(x)
            &= \frac{e^{Œ±_1‚ãÖx+Œ≤_1}}{e^{Œ±_0‚ãÖx+Œ≤_0}+e^{Œ±_1‚ãÖx+Œ≤_1}}\\
            &= \frac{1}{
                \frac{e^{Œ±_0‚ãÖx+Œ≤_0}}{e^{Œ±_1‚ãÖx+Œ≤_1}} + 1
            }\\
            &= \frac{1}{e^{(Œ±_0‚ãÖx+Œ≤_0)-(Œ±_1‚ãÖx+Œ≤_1)} + 1}\\
            &= \frac{1}{1 + e^{(Œ±_0-Œ±_1)‚ãÖx+(Œ≤_0-Œ≤_1)}}\\
            &= œÉ((Œ±_0-Œ±_1)‚ãÖx+(Œ≤_0-Œ≤_1))
    \end{align}
$$


Autrement dit, appliquer ce qu'on vient de voir pour le cas multinomial, si $n=2$, c'est comme
appliquer ce qu'on a vu pour deux classes, avec $w=Œ±_0-Œ±_1$ et $b=Œ≤_0-Œ≤_1$.

## La suite

Vous devriez maintenant avoir quelques id√©es de plusieurs concepts importants‚ÄØ:

- Le concept de classifieur lin√©aire
- Le concept de fonction de co√ªt
- L'algorithme de descente de gradient stochastique
- La fonction softmax

Pour la suite de vos aventures au pays des classifieurs logistiques, je vous recommande plut√¥t
d'utiliser [leur impl√©mentation dans
scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).
