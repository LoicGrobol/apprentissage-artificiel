@inproceedings{2020ParticipatoryResearchLowresourced,
  title = {Participatory {{Research}} for {{Low-resourced Machine Translation}}: {{A Case Study}} in {{African Languages}}},
  shorttitle = {Participatory {{Research}} for {{Low-resourced Machine Translation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {{∀} and Nekoto, Wilhelmina and Marivate, Vukosi and Matsila, Tshinondiwa and Fasubaa, Timi and Fagbohungbe, Taiwo and Akinola, Solomon Oluwole and Muhammad, Shamsuddeen and Kabongo Kabenamualu, Salomon and Osei, Salomey and Sackey, Freshia and Niyongabo, Rubungo Andre and Macharm, Ricky and Ogayo, Perez and Ahia, Orevaoghene and Berhe, Musie Meressa and Adeyemi, Mofetoluwa and Mokgesi-Selinga, Masabata and Okegbemi, Lawrence and Martinus, Laura and Tajudeen, Kolawole and Degila, Kevin and Ogueji, Kelechi and Siminyu, Kathleen and Kreutzer, Julia and Webster, Jason and Ali, Jamiil Toure and Abbott, Jade and Orife, Iroro and Ezeani, Ignatius and Dangana, Idris Abdulkadir and Kamper, Herman and Elsahar, Hady and Duru, Goodness and Kioko, Ghollah and Espoir, Murhabazi and family=Biljon, given=Elan, prefix=van, useprefix=true and Whitenack, Daniel and Onyefuluchi, Christopher and Emezue, Chris Chinenye and Dossou, Bonaventure F. P. and Sibanda, Blessing and Bassey, Blessing and Olabiyi, Ayodele and Ramkilowan, Arshath and Öktem, Alp and Akinfaderin, Adewale and Bashir, Abdallah},
  date = {2020-11},
  pages = {2144--2160},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2020.findings-emnlp.195},
  url = {https://aclanthology.org/2020.findings-emnlp.195},
  urldate = {2022-10-21},
  abstract = {Research in NLP lacks geographic diversity, and the question of how NLP can be scaled to low-resourced languages has not yet been adequately solved. `Low-resourced'-ness is a complex problem going beyond data availability and reflects systemic problems in society. In this paper, we focus on the task of Machine Translation (MT), that plays a crucial role for information accessibility and communication worldwide. Despite immense improvements in MT over the past decade, MT is centered around a few high-resourced languages. As MT researchers cannot solve the problem of low-resourcedness alone, we propose participatory research as a means to involve all necessary agents required in the MT development process. We demonstrate the feasibility and scalability of participatory research with a case study on MT for African languages. Its implementation leads to a collection of novel translation datasets, MT benchmarks for over 30 languages, with human evaluations for a third of them, and enables participants without formal training to make a unique scientific contribution. Benchmarks, models, data, code, and evaluation results are released at https://github.com/masakhane-io/masakhane-mt.},
  eventtitle = {{{EMNLP-Findings}} 2020},
  venue = {Online}
}

@inproceedings{adelani2022FewThousandTranslations,
  title = {A {{Few Thousand Translations Go}} a {{Long Way}}! {{Leveraging Pre-trained Models}} for {{African News Translation}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Adelani, David and Alabi, Jesujoba and Fan, Angela and Kreutzer, Julia and Shen, Xiaoyu and Reid, Machel and Ruiter, Dana and Klakow, Dietrich and Nabende, Peter and Chang, Ernie and Gwadabe, Tajuddeen and Sackey, Freshia and Dossou, Bonaventure F. P. and Emezue, Chris and Leong, Colin and Beukman, Michael and Muhammad, Shamsuddeen and Jarso, Guyo and Yousuf, Oreen and Niyongabo Rubungo, Andre and Hacheme, Gilles and Wairagala, Eric Peter and Nasir, Muhammad Umair and Ajibade, Benjamin and Ajayi, Tunde and Gitau, Yvonne and Abbott, Jade and Ahmed, Mohamed and Ochieng, Millicent and Aremu, Anuoluwapo and Ogayo, Perez and Mukiibi, Jonathan and Ouoba Kabore, Fatoumata and Kalipe, Godson and Mbaye, Derguene and Tapo, Allahsera Auguste and Memdjokam Koagne, Victoire and Munkoh-Buabeng, Edwin and Wagner, Valencia and Abdulmumin, Idris and Awokoya, Ayodele and Buzaaba, Happy and Sibanda, Blessing and Bukula, Andiswa and Manthalu, Sam},
  date = {2022-07},
  pages = {3053--3070},
  publisher = {Association for Computational Linguistics},
  location = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.223},
  url = {https://aclanthology.org/2022.naacl-main.223},
  urldate = {2022-10-08},
  abstract = {Recent advances in the pre-training for language models leverage large-scale datasets to create multilingual models. However, low-resource languages are mostly left out in these datasets. This is primarily because many widely spoken languages that are not well represented on the web and therefore excluded from the large-scale crawls for datasets. Furthermore, downstream users of these models are restricted to the selection of languages originally chosen for pre-training. This work investigates how to optimally leverage existing pre-trained models to create low-resource translation systems for 16 African languages. We focus on two questions: 1) How can pre-trained models be used for languages not included in the initial pretraining? and 2) How can the resulting translation models effectively transfer to new domains? To answer these questions, we create a novel African news corpus covering 16 languages, of which eight languages are not part of any existing evaluation dataset. We demonstrate that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.},
  eventtitle = {{{NAACL-HLT}} 2022}
}

@inproceedings{agic2019JW300WideCoverageParallel,
  title = {{{JW300}}: {{A Wide-Coverage Parallel Corpus}} for {{Low-Resource Languages}}},
  shorttitle = {{{JW300}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Agić, Željko and Vulić, Ivan},
  date = {2019-07},
  pages = {3204--3210},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/P19-1310},
  url = {https://aclanthology.org/P19-1310},
  urldate = {2022-10-17},
  abstract = {Viable cross-lingual transfer critically depends on the availability of parallel texts. Shortage of such resources imposes a development and evaluation bottleneck in multilingual processing. We introduce JW300, a parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average. In this paper, we present the resource and showcase its utility in experiments with cross-lingual word embedding induction and multi-source part-of-speech projection.},
  eventtitle = {{{ACL}} 2019},
  venue = {Firenze, Italia}
}

@article{artetxe2019MassivelyMultilingualSentence,
  title = {Massively {{Multilingual Sentence Embeddings}} for {{Zero-Shot Cross-Lingual Transfer}} and {{Beyond}}},
  author = {Artetxe, Mikel and Schwenk, Holger},
  date = {2019-09-01},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {597--610},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00288},
  url = {https://doi.org/10.1162/tacl_a_00288},
  urldate = {2022-11-29},
  abstract = {We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.}
}

@inproceedings{aulamo2020OpusToolsParallelCorpus,
  title = {{{OpusTools}} and {{Parallel Corpus Diagnostics}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Aulamo, Mikko and Sulubacak, Umut and Virpioja, Sami and Tiedemann, Jörg},
  date = {2020-05},
  pages = {3782--3789},
  publisher = {European Language Resources Association},
  location = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.467},
  urldate = {2023-10-20},
  abstract = {This paper introduces OpusTools, a package for downloading and processing parallel corpora included in the OPUS corpus collection. The package implements tools for accessing compressed data in their archived release format and make it possible to easily convert between common formats. OpusTools also includes tools for language identification and data filtering as well as tools for importing data from various sources into the OPUS format. We show the use of these tools in parallel corpus creation and data diagnostics. The latter is especially useful for the identification of potential problems and errors in the extensive data set. Using these tools, we can now monitor the validity of data sets and improve the overall quality and consitency of the data collection.},
  eventtitle = {{{LREC}} 2020},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@inproceedings{barrault2019Findings2019Conference,
  title = {Findings of the 2019 {{Conference}} on {{Machine Translation}} ({{WMT19}})},
  booktitle = {Proceedings of the {{Fourth Conference}} on {{Machine Translation}} ({{Volume}} 2: {{Shared Task Papers}}, {{Day}} 1)},
  author = {Barrault, Loïc and Bojar, Ondřej and Costa-jussà, Marta R. and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Koehn, Philipp and Malmasi, Shervin and Monz, Christof and Müller, Mathias and Pal, Santanu and Post, Matt and Zampieri, Marcos},
  date = {2019-08},
  pages = {1--61},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/W19-5301},
  url = {https://aclanthology.org/W19-5301},
  urldate = {2023-10-19},
  abstract = {This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.},
  eventtitle = {{{WMT}} 2019}
}

@incollection{bengio2006NeuralProbabilisticLanguage,
  title = {Neural {{Probabilistic Language Models}}},
  booktitle = {Innovations in {{Machine Learning}}: {{Theory}} and {{Applications}}},
  author = {Bengio, Yoshua and Schwenk, Holger and Senécal, Jean-Sébastien and Morin, Fréderic and Gauvain, Jean-Luc},
  editor = {Holmes, Dawn E. and Jain, Lakhmi C.},
  date = {2006},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  pages = {137--186},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-33486-6_6},
  url = {https://doi.org/10.1007/3-540-33486-6_6},
  urldate = {2019-09-20},
  abstract = {A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.},
  isbn = {978-3-540-33486-6},
  langid = {english},
  keywords = {Importance Sampling,Language Model,Proposal Distribution,Speech Recognition,Training Corpus}
}

@inproceedings{bird2020DecolonisingSpeechLanguage,
  title = {Decolonising {{Speech}} and {{Language Technology}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Bird, Steven},
  date = {2020-12},
  pages = {3504--3519},
  publisher = {International Committee on Computational Linguistics},
  doi = {10.18653/v1/2020.coling-main.313},
  url = {https://aclanthology.org/2020.coling-main.313},
  urldate = {2022-10-08},
  abstract = {After generations of exploitation, Indigenous people often respond negatively to the idea that their languages are data ready for the taking. By treating Indigenous knowledge as a commodity, speech and language technologists risk disenfranchising local knowledge authorities, reenacting the causes of language endangerment. Scholars in related fields have responded to calls for decolonisation, and we in the speech and language technology community need to follow suit, and explore what this means for our practices that involve Indigenous languages and the communities who own them. This paper reviews colonising discourses in speech and language technology, and suggests new ways of working with Indigenous communities, and seeks to open a discussion of a postcolonial approach to computational methods for supporting language vitality.},
  eventtitle = {{{COLING}} 2020},
  venue = {Barcelona, España}
}

@unpublished{bird2021LT4AllRethinkingAgenda,
  type = {Keynote speech},
  title = {{{LT4All}}!? {{Rethinking}} the {{Agenda}}},
  author = {Bird, Steven},
  date = {2021-11-09},
  url = {https://2021.emnlp.org/keynotes},
  abstract = {The majority of the world’s languages are oral, emergent, untranslatable, and tightly coupled to a place. Yet it seems that the agenda is to supply all languages with the technologies that have been developed for written languages. It is as though standardised writing were the optimal way to safeguard the future of any language. It is as though the function of a language is exclusively for transmitting information, and that the same information can be rendered into any language. It is as though we can capture and model language data independently of people, purpose, and place. What would it be like if language technologies respected the self-determination of a local speech community and supported aspirations concerning the local repertoire of speech varieties? The answer will be different in different places, but there may be value in taking a close look at an individual community and trying to discern broader themes. In this talk I will share from my experience of living and working in a remote Aboriginal community in the far north of Australia. Here, local people have been teaching me participatory, relational, strengths-based approaches that my students and I have been exploring in the design of language technologies. I will reflect on five years of personal experiences in this space and share thoughts concerning an agenda for language technology in the interest of minority speech communities, and hopes for creating a world that sustains its languages.},
  eventtitle = {The 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  venue = {Online}
}

@article{bird2021SparseTranscription,
  title = {Sparse {{Transcription}}},
  author = {Bird, Steven},
  date = {2021-02-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {46},
  number = {4},
  pages = {713--744},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00387},
  url = {https://doi.org/10.1162/coli_a_00387},
  urldate = {2022-10-14},
  abstract = {The transcription bottleneck is often cited as a major obstacle for efforts to document the world’s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine translation, and recruit linguists to provide narrow phonetic transcriptions and sentence-aligned translations. However, I believe that these approaches are not a good fit with the available data and skills, or with long-established practices that are essentially word-based. In seeking a more effective approach, I consider a century of transcription practice and a wide range of computational approaches, before proposing a computational model based on spoken term detection that I call “sparse transcription.” This represents a shift away from current assumptions that we transcribe phones, transcribe fully, and transcribe first. Instead, sparse transcription combines the older practice of word-level transcription with interpretive, iterative, and interactive processes that are amenable to wider participation and that open the way to new methods for processing oral languages.}
}

@online{branwen2019NeuralNetTank,
  title = {The {{Neural Net Tank Urban Legend}}},
  author = {Branwen, Gwern},
  date = {2019-02-07},
  url = {https://www.gwern.net/Tanks},
  urldate = {2019-02-21},
  abstract = {AI folklore tells a story about a neural network trained to detect tanks which instead learned to detect time of day; investigating, this probably never happened.},
  langid = {english},
  keywords = {artificial intelligence,counterexample,evolutionary algorithm,failure,machine learning}
}

@inproceedings{brunila2022WhatCompanyWords,
  title = {What Company Do Words Keep? {{Revisiting}} the Distributional Semantics of {{J}}.{{R}}. {{Firth}} \& {{Zellig Harris}}},
  shorttitle = {What Company Do Words Keep?},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Brunila, Mikael and LaViolette, Jack},
  editor = {Carpuat, Marine and family=Marneffe, given=Marie-Catherine, prefix=de, useprefix=true and Meza Ruiz, Ivan Vladimir},
  date = {2022-07},
  pages = {4403--4417},
  publisher = {Association for Computational Linguistics},
  location = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.327},
  url = {https://aclanthology.org/2022.naacl-main.327},
  urldate = {2024-06-04},
  abstract = {The power of word embeddings is attributed to the linguistic theory that similar words will appear in similar contexts. This idea is specifically invoked by noting that “you shall know a word by the company it keeps,” a quote from British linguist J.R. Firth who, along with his American colleague Zellig Harris, is often credited with the invention of “distributional semantics.” While both Firth and Harris are cited in all major NLP textbooks and many foundational papers, the content and differences between their theories is seldom discussed. Engaging in a close reading of their work, we discover two distinct and in many ways divergent theories of meaning. One focuses exclusively on the internal workings of linguistic forms, while the other invites us to consider words in new company—not just with other linguistic elements, but also in a broader cultural and situational context. Contrasting these theories from the perspective of current debates in NLP, we discover in Firth a figure who could guide the field towards a more culturally grounded notion of semantics. We consider how an expanded notion of “context” might be modeled in practice through two different strategies: comparative stratification and syntagmatic extension.},
  eventtitle = {{{NAACL-HLT}} 2022}
}

@inproceedings{bustamante2020NoDataCrawl,
  title = {No {{Data}} to {{Crawl}}? {{Monolingual Corpus Creation}} from {{PDF Files}} of {{Truly}} Low-{{Resource Languages}} in {{Peru}}},
  shorttitle = {No {{Data}} to {{Crawl}}?},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Bustamante, Gina and Oncevay, Arturo and Zariquiey, Roberto},
  date = {2020-05},
  pages = {2914--2923},
  publisher = {European Language Resources Association},
  location = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.356},
  urldate = {2022-10-14},
  abstract = {We introduce new monolingual corpora for four indigenous and endangered languages from Peru: Shipibo-konibo, Ashaninka, Yanesha and Yine. Given the total absence of these languages in the web, the extraction and processing of texts from PDF files is relevant in a truly low-resource language scenario. Our procedure for monolingual corpus creation considers language-specific and language-agnostic steps, and focuses on educational PDF files with multilingual sentences, noisy pages and low-structured content. Through an evaluation based on language modelling and character-level perplexity on a subset of manually extracted sentences, we determine that our method allows the creation of clean corpora for the four languages, a key resource for natural language processing tasks nowadays.},
  eventtitle = {{{LREC}} 2020},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@inproceedings{candito2012CorpusSequoiaAnnotation,
  title = {Le corpus Sequoia : annotation syntaxique et exploitation pour l'adaptation d'analyseur par pont lexical},
  shorttitle = {Le corpus Sequoia},
  booktitle = {Actes de la conférence conjointe JEP-TALN-RECITAL 2012},
  author = {Candito, Marie and Seddah, Djamé},
  date = {2012-06},
  volume = {2},
  publisher = {Association pour le Traitement Automatique des Langues},
  url = {https://hal.inria.fr/hal-00698938},
  urldate = {2020-02-17},
  abstract = {Nous présentons dans cet article la méthodologie de constitution et les caractéristiques du corpus Sequoia, un corpus en français, syntaxiquement annoté d'après un schéma d'annotation très proche de celui du French Treebank (Abeillé et Barrier, 2004), et librement disponible, en constituants et en dépendances. Le corpus comporte des phrases de quatre origines : Europarl français, le journal l'Est Républicain, Wikipédia Fr et des documents de l'Agence Européenne du Médicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous présentons une application de ce corpus : l'évaluation d'une technique d'adaptation d'analyseurs syntaxiques probabilistes à des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entraînés. Cette technique utilise des clusters de mots obtenus d'abord par regroupement morphologique à l'aide d'un lexique, puis par regroupement non supervisé, et permet une nette amélioration de l'analyse des domaines cibles (le corpus Sequoia), tout en préservant le même niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, à la différence d'autres techniques d'adaptation comme le self-training.},
  eventtitle = {TALN 2012},
  langid = {french},
  venue = {Grenoble, France}
}

@unpublished{chaudhary2019LittleAnnotationDoes,
  title = {A {{Little Annotation}} Does a {{Lot}} of {{Good}}: {{A Study}} in {{Bootstrapping Low-resource Named Entity Recognizers}}},
  shorttitle = {A {{Little Annotation}} Does a {{Lot}} of {{Good}}},
  author = {Chaudhary, Aditi and Xie, Jiateng and Sheikh, Zaid and Neubig, Graham and Carbonell, Jaime G.},
  date = {2019-08-23},
  eprint = {1908.08983},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1908.08983},
  urldate = {2019-09-03},
  abstract = {Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now several proposed approaches involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. This paper poses the question: given this recent progress, and limited human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we find a dual-strategy approach best, starting with a cross-lingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data.},
  keywords = {Computer Science - Computation and Language}
}

@standard{comrie2015LeipzigGlossingRules,
  title = {The {{Leipzig Glossing Rules}}: {{Conventions}} for Interlinear Morpheme-by-Morpheme Glosses},
  author = {Comrie, Bernard and Haspelmath, Martin and Bickel},
  date = {2015-05-31},
  publisher = {Max Planck Institute for Evolutionary Anthropology},
  url = {https://www.eva.mpg.de/lingua/pdf/Glossing-Rules.pdf},
  urldate = {2023-10-20}
}

@inproceedings{delhoneux2022ZeroShotDependencyParsing,
  title = {Zero-{{Shot Dependency Parsing}} with {{Worst-Case Aware Automated Curriculum Learning}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {family=Lhoneux, given=Miryam, prefix=de, useprefix=true and Zhang, Sheng and Søgaard, Anders},
  date = {2022-05},
  pages = {578--587},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2022.acl-short.64},
  url = {https://aclanthology.org/2022.acl-short.64},
  urldate = {2022-06-07},
  abstract = {Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on outlier languages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting.},
  eventtitle = {{{ACL}} 2022},
  venue = {Baile Átha Claith, Éire}
}

@inproceedings{devlin2019BERTPretrainingDeep,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/N19-1423},
  url = {https://www.aclweb.org/anthology/N19-1423},
  urldate = {2021-01-12},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019},
  venue = {Minneapolis, Minnesota, USA}
}

@thesis{dowling2022InvestigationEnglishIrishMachine,
  type = {doctoral},
  title = {An Investigation of {{English-Irish}} Machine Translation and Associated Resources},
  author = {Dowling, Meghan},
  date = {2022-02},
  journaltitle = {Dowling, Meghan ORCID: 0000-0003-1637-4923 {$<$}https://orcid.org/0000-0003-1637-4923{$>$}  (2022) An investigation of English-Irish machine translation and associated resources.  PhD thesis, Dublin City University.},
  institution = {Dublin City University. School of Computing},
  url = {https://doras.dcu.ie/26574/},
  urldate = {2022-11-30},
  abstract = {As an official language in both Ireland and the European Union (EU), there is a high demand for English-Irish (EN-GA) translation in public administration. The difficulty that translators currently face in meeting this demand leads to the need for reliable domain-specific user-driven EN-GA machine translation (MT). This landscape provides a timely opportunity to address some research questions surrounding MT for the EN-GA language pair. To this end, we assess the corpora available for training data-driven MT systems, including publicly-available data, data collected through EU-supported data collection efforts and web-crawling, showing that though Irish is a low-resource language it is possible to increase the corpora available through concerted data collection efforts. We investigate how increased corpora affect domain-specific (public administration) statistical MT (SMT) and neural MT (NMT) systems using automatic metrics. The effect that different SMT and NMT parameters have on these automatic values is also explored, using sentence-level metrics to identify specific areas where output differs greatly between MT systems and providing a linguistic analysis of each. With EN-GA SMT and NMT automatic evaluation scores showing inconclusive results, we investigate the usefulness of EN-GA hybrid MT through the use of monolingual data as a source of artificial data creation via backtranslation. We evaluate these results using automatic metrics and linguistic analysis. Although results indicate that the addition of artificial data did not have a positive impact on EN-GA MT, repeated experiments involving Scottish Gaelic show that the method holds promise, given suitable conditions. Finally, given that the intended use-case of EN-GA MT is in the workflow of a professional translator, we conduct an in-depth human evaluation study for EN-GA SMT and NMT, providing a human-derived assessment of EN-GA MT quality and comparison of EN-GA SMT and NMT. We include a survey of translator opinions and recommendations surrounding EN-GA SMT and NMT as well as an analysis of data gathered through the post-editing of MT output. We compare these results to those generated automatically and provide recommendations for future work on EN-GA MT, in particular with regards to its use in a professional translation workflow within public administration.},
  langid = {english}
}

@inproceedings{el-kishky2020CCAlignedMassiveCollection,
  title = {{{CCAligned}}: {{A Massive Collection}} of {{Cross-Lingual Web-Document Pairs}}},
  shorttitle = {{{CCAligned}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {El-Kishky, Ahmed and Chaudhary, Vishrav and Guzmán, Francisco and Koehn, Philipp},
  date = {2020-11},
  pages = {5960--5969},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-main.480},
  url = {https://aclanthology.org/2020.emnlp-main.480},
  urldate = {2022-11-29},
  abstract = {Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5\% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.},
  eventtitle = {{{EMNLP}} 2020}
}

@inproceedings{emezue2021MMTAfricaMultilingualMachine,
  title = {{{MMTAfrica}}: {{Multilingual Machine Translation}} for {{African Languages}}},
  shorttitle = {{{MMTAfrica}}},
  booktitle = {Proceedings of the {{Sixth Conference}} on {{Machine Translation}}},
  author = {Emezue, Chris Chinenye and Dossou, Bonaventure F. P.},
  date = {2021-11},
  pages = {398--411},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  url = {https://aclanthology.org/2021.wmt-1.48},
  urldate = {2022-11-30},
  abstract = {In this paper, we focus on the task of multilingual machine translation for African languages and describe our contribution in the 2021 WMT Shared Task: Large-Scale Multilingual Machine Translation. We introduce MMTAfrica, the first many-to-many multilingual translation system for six African languages: Fon (fon), Igbo (ibo), Kinyarwanda (kin), Swahili/Kiswahili (swa), Xhosa (xho), and Yoruba (yor) and two non-African languages: English (eng) and French (fra). For multilingual translation concerning African languages, we introduce a novel backtranslation and reconstruction objective, BT\&REC, inspired by the random online back translation and T5 modelling framework respectively, to effectively leverage monolingual data. Additionally, we report improvements from MMTAfrica over the FLORES 101 benchmarks (spBLEU gains ranging from +0.58 in Swahili to French to +19.46 in French to Xhosa).},
  eventtitle = {{{WMT}} 2021}
}

@article{fan2021EnglishcentricMultilingualMachine,
  title = {Beyond English-Centric Multilingual Machine Translation},
  author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
  date = {2021-01-01},
  journaltitle = {The Journal of Machine Learning Research},
  volume = {22},
  number = {1},
  pages = {107:4839--107:4886},
  issn = {1532-4435},
  url = {https://dl.acm.org/doi/abs/10.5555/3546258.3546365},
  abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model},
  keywords = {⛔ No DOI found,bitext mining,many-to-many,model scaling,multilingual machine translation,neural networks}
}

@incollection{firth1957SynopsisLinguisticTheory,
  title = {A {{Synopsis}} of {{Linguistic Theory}}, 1930-1955},
  booktitle = {Studies in {{Linguistic Analysis}}},
  author = {Firth, John R.},
  date = {1957},
  series = {Publications of the {{Philological Society}}},
  pages = {Oxford},
  publisher = {Blackwell},
  url = {/paper/A-Synopsis-of-Linguistic-Theory%2C-1930-1955-Firth/88b3959b6f5333e5358eac43970a5fa29b54642c},
  urldate = {2021-06-16},
  langid = {english}
}

@article{forcada2011ApertiumFreeOpensource,
  title = {Apertium: A Free/Open-Source Platform for Rule-Based Machine Translation},
  shorttitle = {Apertium},
  author = {Forcada, Mikel L. and Ginestí-Rosell, Mireia and Nordfalk, Jacob and O’Regan, Jim and Ortiz-Rojas, Sergio and Pérez-Ortiz, Juan Antonio and Sánchez-Martínez, Felipe and Ramírez-Sánchez, Gema and Tyers, Francis M.},
  date = {2011-06-01},
  journaltitle = {Machine Translation},
  shortjournal = {Machine Translation},
  volume = {25},
  number = {2},
  pages = {127--144},
  issn = {1573-0573},
  doi = {10.1007/s10590-011-9090-0},
  url = {https://doi.org/10.1007/s10590-011-9090-0},
  urldate = {2022-11-28},
  abstract = {Apertium is a free/open-source platform for rule-based machine translation. It is being widely used to build machine translation systems for a variety of language pairs, especially in those cases (mainly with related-language pairs) where shallow transfer suffices to produce good quality translations, although it has also proven useful in assimilation scenarios with more distant pairs involved. This article summarises the Apertium platform: the translation engine, the encoding of linguistic data, and the tools developed around the platform. The present limitations of the platform and the challenges posed for the coming years are also discussed. Finally, evaluation results for some of the most active language pairs are presented. An appendix describes Apertium as a free/open-source project.},
  langid = {english},
  keywords = {Apertium,Finite-state transducers,Free/open-source machine translation,Rule-based machine translation,Shallow transfer}
}

@online{goyal2021FLORES101EvaluationBenchmark,
  title = {The {{FLORES-101 Evaluation Benchmark}} for {{Low-Resource}} and {{Multilingual Machine Translation}}},
  author = {Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzman, Francisco and Fan, Angela},
  date = {2021-06-06},
  eprint = {2106.03193},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.03193},
  url = {http://arxiv.org/abs/2106.03193},
  urldate = {2022-10-14},
  abstract = {One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the FLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@online{haddow2022SurveyLowResourceMachine,
  title = {Survey of {{Low-Resource Machine Translation}}},
  author = {Haddow, Barry and Bawden, Rachel and Barone, Antonio Valerio Miceli and Helcl, Jindřich and Birch, Alexandra},
  date = {2022-02-07},
  eprint = {2109.00486},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.00486},
  url = {http://arxiv.org/abs/2109.00486},
  urldate = {2022-10-08},
  abstract = {We present a survey covering the state of the art in low-resource machine translation research. There are currently around 7000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a summary of this topical research field and provide a description of the techniques evaluated by researchers in several recent shared tasks in low-resource MT.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{howard2018UniversalLanguageModel,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-07},
  pages = {328--339},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/P18-1031},
  url = {https://www.aclweb.org/anthology/P18-1031},
  urldate = {2021-01-13},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.},
  eventtitle = {{{ACL}} 2018},
  venue = {Melbourne, Australia}
}

@inproceedings{joshi2020StateFateLinguistic,
  title = {The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  date = {2020-07},
  pages = {6282--6293},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.560},
  url = {https://aclanthology.org/2020.acl-main.560},
  urldate = {2022-10-21},
  abstract = {Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.},
  eventtitle = {{{ACL}} 2020},
  venue = {Online}
}

@online{jouitteau2009ARBRESWikigrammaireDialectes,
  title = {{{ARBRES}}, Wikigrammaire Des Dialectes Du Breton et Centre de Ressources Pour Son Étude Linguistique Formelle},
  author = {Jouitteau, Mélanie},
  date = {2009/2024},
  url = {http://arbres.iker.cnrs.fr}
}

@incollection{jouitteau2023OutilsNumeriquesTraitement,
  title = {Outils Numériques et Traitement Automatique Du Breton},
  booktitle = {Langues Régionales de {{France}}: Nouvelles Approches, Nouvelles Méthodologies, Revitalisation},
  author = {Jouitteau, Mélanie and Bideault, Reun},
  editor = {Rialland, Annie and Russo, Michela},
  date = {2023},
  pages = {37--74},
  publisher = {Société Linguistique de Paris},
  url = {https://hal.science/hal-03918268},
  urldate = {2024-02-15},
  abstract = {In this article, formal linguist Mélanie Jouitteau and web developer Reun Bideault present a synthesis of the numeric and NLP tools available or in development for Breton. They discuss the resources for its development. NLP of Breton is still objectively poorly developed, but some new tools have just been made available, which opens a real potential for development. We present a state-of-the-art of the field, and we detail how the first tree bank Universal Dependencies, created by Tyers \& Ravishankar (2018) could be reinforced by 25000 additional glossed sentences in the databank of the wikigrammar ARBRES (Jouitteau (2009-).},
  keywords = {breton,Breton,Celtic,celtique,Celtique,NLP (Natural Language Processing),NLP Natural Language Processing,TAL (Traitement Automatique des Langues),TAL Traitement Automatique des Langues,wikigrammaires,Wikigrammaires,Wikigrammars}
}

@article{khanna2021RecentAdvancesApertium,
  title = {Recent Advances in {{Apertium}}, a Free/Open-Source Rule-Based Machine Translation Platform for Low-Resource Languages},
  author = {Khanna, Tanmai and Washington, Jonathan N. and Tyers, Francis M. and Bayatlı, Sevilay and Swanson, Daniel G. and Pirinen, Tommi A. and Tang, Irene and Alòs i Font, Hèctor},
  date = {2021-12-01},
  journaltitle = {Machine Translation},
  shortjournal = {Machine Translation},
  volume = {35},
  number = {4},
  pages = {475--502},
  issn = {1573-0573},
  doi = {10.1007/s10590-021-09260-6},
  url = {https://doi.org/10.1007/s10590-021-09260-6},
  urldate = {2022-11-28},
  abstract = {This paper presents an overview of Apertium, a free and open-source rule-based machine translation platform. Translation in Apertium happens through a pipeline of modular tools, and the platform continues to be improved as more language pairs are added. Several advances have been implemented since the last publication, including some new optional modules: a module that allows rules to process recursive structures at the structural transfer stage, a module that deals with contiguous and discontiguous multi-word expressions, and a module that resolves anaphora to aid translation. Also highlighted is the hybridisation of Apertium through statistical modules that augment the pipeline, and statistical methods that augment existing modules. This includes morphological disambiguation, weighted structural transfer, and lexical selection modules that learn from limited data. The paper also discusses how a platform like Apertium can be a critical part of access to language technology for so-called low-resource languages, which might be ignored or deemed unapproachable by popular corpus-based translation technologies. Finally, the paper presents some of the released and unreleased language pairs, concluding with a brief look at some supplementary Apertium tools that prove valuable to users as well as language developers. All Apertium-related code, including language data, is free/open-source and available at https://github.com/apertium.},
  langid = {english},
  keywords = {Hybrid machine translation,Low-resource languages,Machine translation,Rule-based machine translation}
}

@inproceedings{koehn2005EuroparlParallelCorpus,
  title = {Europarl: {{A Parallel Corpus}} for {{Statistical Machine Translation}}},
  shorttitle = {Europarl},
  booktitle = {Proceedings of {{Machine Translation Summit X}}: {{Papers}}},
  author = {Koehn, Philipp},
  date = {2005-09-13},
  pages = {79--86},
  location = {Phuket, Thailand},
  url = {https://aclanthology.org/2005.mtsummit-papers.11},
  urldate = {2022-11-29},
  abstract = {We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.},
  eventtitle = {{{MTSummit}} 2005}
}

@online{krakovna2018SpecificationGamingExamples,
  title = {Specification Gaming Examples in {{AI}} - Master List},
  author = {Krakovna, Victoria},
  date = {2018-04-02},
  url = {https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml},
  urldate = {2019-02-21},
  keywords = {artificial intelligence,counterexample,evolutionary algorithm,failure,machine learning}
}

@article{lafourcade2020JeuxDeMotsReseauLexicosemantique,
  title = {JeuxDeMots : Un réseau lexico-sémantique pour le français, issu de jeux et d’inférences},
  shorttitle = {JeuxDeMots},
  author = {Lafourcade, Mathieu and Le Brun, Nathalie},
  date = {2020-12-01},
  journaltitle = {Lexique. Revue en Sciences du Langage},
  number = {27},
  pages = {47--86},
  publisher = {Université de Lille},
  issn = {2804-7397},
  doi = {10.54563/lexique.773},
  url = {http://www.peren-revues.fr/lexique/773},
  urldate = {2024-06-04},
  abstract = {Le projet JeuxDeMots a pour but de construire une base de connaissances de sens commun et de spécialité, en français, à l’aide de jeux, d’approches contributives, et de mécanismes d'inférence. Une dizaine de jeux ont été conçus dans le cadre de ce projet, chacun permettant de collecter des informations spécifiques, ou de consolider les informations acquises via les autres jeux. Dans cet article nous décrirons quantitativement et qualitativement les données collectées et construites, depuis le lancement du projet durant l’été~2007. Nous décrirons en particulier les aspects suivants~: la structure du réseau lexical et sémantique JeuxDeMots, certains types de relations (sémantiques, ontologiques, subjectives, rôles sémantiques, associations d’idées, etc.), l’annotation de relations (méta-informations), les raffinements sémantiques (gestion de la polysémie), la création de termes agglomérés permettant la représentation de connaissances plus riches (relations à n-arguments). Enfin, nous décrirons des méthodes d’acquisition complémentaires, à savoir ce que peuvent faire des robots effectuant des contributions endogènes sur les données, ainsi qu’un chatbot réalisant des inférences.},
  langid = {french}
}

@inproceedings{langedijk2022MetaLearningFastCrossLingual,
  title = {Meta-{{Learning}} for {{Fast Cross-Lingual Adaptation}} in {{Dependency Parsing}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Langedijk, Anna and Dankers, Verna and Lippe, Phillip and Bos, Sander and Cardenas Guevara, Bryan and Yannakoudakis, Helen and Shutova, Ekaterina},
  date = {2022-05},
  pages = {8503--8520},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2022.acl-long.582},
  url = {https://aclanthology.org/2022.acl-long.582},
  urldate = {2022-06-07},
  abstract = {Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing. We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages. We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen, typologically diverse, and low-resource languages, in a few-shot learning setup.},
  eventtitle = {{{ACL}} 2022},
  venue = {Baile Átha Claith, Éire}
}

@inproceedings{lewis2020BARTDenoisingSequencetoSequence,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  date = {2020-07},
  pages = {7871--7880},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.703},
  url = {https://aclanthology.org/2020.acl-main.703},
  urldate = {2022-11-29},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
  eventtitle = {{{ACL}} 2020}
}

@inproceedings{lison2016OpenSubtitles2016ExtractingLarge,
  title = {{{OpenSubtitles2016}}: {{Extracting Large Parallel Corpora}} from {{Movie}} and {{TV Subtitles}}},
  shorttitle = {{{OpenSubtitles2016}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Lison, Pierre and Tiedemann, Jörg},
  date = {2016-05},
  pages = {923--929},
  publisher = {European Language Resources Association (ELRA)},
  location = {Portorož, Slovenia},
  url = {https://aclanthology.org/L16-1147},
  urldate = {2022-11-29},
  abstract = {We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.},
  eventtitle = {{{LREC}} 2016}
}

@article{liu2020MultilingualDenoisingPretraining,
  title = {Multilingual {{Denoising Pre-training}} for {{Neural Machine Translation}}},
  author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  date = {2020-11-01},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {726--742},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00343},
  url = {https://doi.org/10.1162/tacl_a_00343},
  urldate = {2022-11-29},
  abstract = {This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1}
}

@inproceedings{mccarthy2020JohnsHopkinsUniversity,
  title = {The {{Johns Hopkins University Bible Corpus}}: 1600+ {{Tongues}} for {{Typological Exploration}}},
  shorttitle = {The {{Johns Hopkins University Bible Corpus}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {McCarthy, Arya D. and Wicks, Rachel and Lewis, Dylan and Mueller, Aaron and Wu, Winston and Adams, Oliver and Nicolai, Garrett and Post, Matt and Yarowsky, David},
  date = {2020-05},
  pages = {2884--2892},
  publisher = {European Language Resources Association},
  url = {https://aclanthology.org/2020.lrec-1.352},
  urldate = {2022-10-17},
  abstract = {We present findings from the creation of a massively parallel corpus in over 1600 languages, the Johns Hopkins University Bible Corpus (JHUBC). The corpus consists of over 4000 unique translations of the Christian Bible and counting. Our data is derived from scraping several online resources and merging them with existing corpora, combining them under a common scheme that is verse-parallel across all translations. We detail our effort to scrape, clean, align, and utilize this ripe multilingual dataset. The corpus captures the great typological variety of the world's languages. We catalog this by showing highly similar proportions of representation of Ethnologue's typological features in our corpus. We also give an example application: projecting pronoun features like clusivity across alignments to richly annotate languages which do not mark the distinction.},
  eventtitle = {{{LREC}} 2020},
  isbn = {979-10-95546-34-4},
  langid = {english},
  venue = {Marseille, France}
}

@online{mehta2023SemanticTokenizerEnhanced,
  title = {Semantic {{Tokenizer}} for {{Enhanced Natural Language Processing}}},
  author = {Mehta, Sandeep and Shah, Darpan and Kulkarni, Ravindra and Caragea, Cornelia},
  date = {2023-04-24},
  eprint = {2304.12404},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.12404},
  url = {http://arxiv.org/abs/2304.12404},
  urldate = {2024-06-03},
  abstract = {Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{mikolov2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomáš and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2013-05-02},
  location = {Scottsdale, Arizona, USA},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2019-09-28},
  eventtitle = {1st {{International Conference}} on {{Learning Representations}}}
}

@article{millour2018EcouteLocuteursProduction,
  title = {À l'écoute des locuteurs : production participative de ressources langagières pour des langues non standardisées},
  shorttitle = {À l'écoute des locuteurs},
  author = {Millour, Alice and Fort, Karën},
  date = {2018-12},
  journaltitle = {Traitement Automatique des Langues},
  publisher = {ATALA (Association pour le Traitement Automatique des Langues)},
  url = {https://hal.archives-ouvertes.fr/hal-01995758},
  urldate = {2022-10-14},
  langid = {french},
  keywords = {⛔ No DOI found,crowdsourcing,non-standardized languages,part-of-speech annotation}
}

@inproceedings{millour2020RepliquerEtendrePour,
  title = {Répliquer et Étendre Pour l'alsacien ”{{Étiquetage}} En Parties Du Discours de Langues Peu Dotées Par Spécialisation Des Plongements Lexicaux”},
  booktitle = {6e Conférence Conjointe {{Journées}} d'{{Études}} Sur La {{Parole}} ({{JEP}}, 33e Édition), {{Traitement Automatique}} Des {{Langues Naturelles}} ({{TALN}}, 27e Édition), {{Rencontre}} Des {{Étudiants Chercheurs}} En {{Informatique}} Pour Le {{Traitement Automatique}} Des {{Langues}} ({{RÉCITAL}}, 22e Édition). 2e Atelier {{Éthique}} et {{TRaitemeNt Automatique}} Des {{Langues}} ({{ETeRNAL}})},
  author = {Millour, Alice and Fort, Karën and Magistry, Pierre},
  editor = {Adda, Gilles and Amblard, Maxime and Fort, Karën},
  date = {2020-06},
  pages = {29--37},
  publisher = {ATALA},
  location = {Nancy, France},
  url = {https://hal.archives-ouvertes.fr/hal-02750224},
  urldate = {2022-10-14},
  keywords = {étiquetage en parties du discours,langues peu dotées,low-resourced l...,POS-tagging,Réplicabilité,replicability,variation.}
}

@book{mosely2010AtlasWorldLanguages,
  title = {Atlas of the World's Languages in Danger},
  author = {Mosely, Christopher and Nicolas, Alexandre},
  date = {2010},
  edition = {3},
  publisher = {UNESCO},
  location = {Paris},
  url = {https://unesdoc.unesco.org/ark:/48223/pf0000187026},
  urldate = {2023-10-20},
  isbn = {978-92-3-104096-2},
  langid = {english}
}

@inproceedings{nicolas2020CreatingExpertKnowledge,
  title = {Creating {{Expert Knowledge}} by {{Relying}} on {{Language Learners}}: A {{Generic Approach}} for {{Mass-Producing Language Resources}} by {{Combining Implicit Crowdsourcing}} and {{Language Learning}}},
  shorttitle = {Creating {{Expert Knowledge}} by {{Relying}} on {{Language Learners}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Nicolas, Lionel and Lyding, Verena and Borg, Claudia and Forascu, Corina and Fort, Karën and Zdravkova, Katerina and Kosem, Iztok and Čibej, Jaka and Arhar Holdt, Špela and Millour, Alice and König, Alexander and Rodosthenous, Christos and Sangati, Federico and family=Hassan, given=Umair, prefix=ul, useprefix=true and Katinskaia, Anisia and Barreiro, Anabela and Aparaschivei, Lavinia and HaCohen-Kerner, Yaakov},
  date = {2020-05},
  pages = {268--278},
  publisher = {European Language Resources Association},
  url = {https://aclanthology.org/2020.lrec-1.34},
  urldate = {2022-10-14},
  abstract = {We introduce in this paper a generic approach to combine implicit crowdsourcing and language learning in order to mass-produce language resources (LRs) for any language for which a crowd of language learners can be involved. We present the approach by explaining its core paradigm that consists in pairing specific types of LRs with specific exercises, by detailing both its strengths and challenges, and by discussing how much these challenges have been addressed at present. Accordingly, we also report on on-going proof-of-concept efforts aiming at developing the first prototypical implementation of the approach in order to correct and extend an LR called ConceptNet based on the input crowdsourced from language learners. We then present an international network called the European Network for Combining Language Learning with Crowdsourcing Techniques (enetCollect) that provides the context to accelerate the implementation of this generic approach. Finally, we exemplify how it can be used in several language learning scenarios to produce a multitude of NLP resources and how it can therefore alleviate the long-standing NLP issue of the lack of LRs.},
  eventtitle = {{{LREC}} 2020},
  isbn = {979-10-95546-34-4},
  langid = {english},
  venue = {Marseille, France}
}

@online{nllbteam2022NoLanguageLeft,
  title = {No {{Language Left Behind}}: {{Scaling Human-Centered Machine Translation}}},
  shorttitle = {No {{Language Left Behind}}},
  author = {{NLLB Team} and Costa-jussà, Marta R. and Cross, James and Çelebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzmán, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},
  date = {2022-08-25},
  eprint = {2207.04672},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.04672},
  url = {http://arxiv.org/abs/2207.04672},
  urldate = {2023-04-13},
  abstract = {Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44\% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.},
  pubstate = {preprint},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,I.2.7}
}

@inproceedings{ortizsuarez2019AsynchronousPipelineProcessing,
  title = {Asynchronous {{Pipeline}} for {{Processing Huge Corpora}} on {{Medium}} to {{Low Resource Infrastructures}}},
  booktitle = {7th {{Workshop}} on the {{Challenges}} in the {{Management}} of {{Large Corpora}}},
  author = {Ortiz Suárez, Pedro Javier and Sagot, Benoît and Romary, Laurent},
  editor = {Bański, Piotr and Barbaresi, Adrien and Biber, Hanno and Breiteneder, Evelyn and Clematide, Simon and Kupietz, Marc and Lüngen, Harald and Iliadi, Caroline},
  date = {2019-07},
  publisher = {Leibniz-Institut für Deutsche Sprache},
  url = {https://hal.inria.fr/hal-02148693},
  urldate = {2020-01-31},
  abstract = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of  heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  eventtitle = {{{CMLCè7}}},
  langid = {english},
  venue = {Caerdydd, Cymru, Deyrnas Unedig}
}

@incollection{otoole2020ResponsibilityLanguageMovement,
  title = {Responsibility, {{Language Movement}}, and {{Social Transformation}}: The {{Shifting Value}} of Te Reo for Non-{{Māori}} in {{Aotearoa New Zealand}}},
  shorttitle = {Responsibility, {{Language Movement}}, and {{Social Transformation}}},
  booktitle = {Responsibility and {{Language Practices}} in {{Place}}},
  author = {O’Toole, Michelle},
  editor = {Siragusa, Laura and Ferguson, Jenanne K.},
  date = {2020},
  volume = {5},
  eprint = {j.ctv199tdgh.13},
  eprinttype = {jstor},
  pages = {195--212},
  publisher = {Finnish Literature Society},
  doi = {10.2307/j.ctv199tdgh.13},
  url = {https://www.jstor.org/stable/j.ctv199tdgh.13},
  urldate = {2022-10-14},
  abstract = {‘I just feel that, as, as a citizen of this country that it’s really important to acknowledge the first people that were here and—and the importance of their language,’ said ‘Robyn,’ a New Zealander of European descent, to me during an interview. Robyn strongly felt a duty to demonstrate respect for Māori culture and language, and hence was learning te reo at evening beginners’ classes.¹ This motivation, along with similar ones expressed by other non-Maori participants during my doctoral fieldwork, prompted me to consider the role and responsibility of non-heritage speakers in language revitalisation efforts. The New Zealand Government}
}

@inproceedings{papineni2002BleuMethodAutomatic,
  title = {Bleu: A {{Method}} for {{Automatic Evaluation}} of {{Machine Translation}}},
  shorttitle = {Bleu},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  date = {2002-07},
  pages = {311--318},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/1073083.1073135},
  url = {https://aclanthology.org/P02-1040},
  urldate = {2021-07-06},
  eventtitle = {{{ACL}} 2002},
  venue = {Philadelphia, Pennsylvania, USA}
}

@inproceedings{peters2018DeepContextualizedWord,
  title = {Deep {{Contextualized Word Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018-06},
  volume = {1},
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/N18-1202},
  url = {http://aclweb.org/anthology/N18-1202},
  urldate = {2019-02-16},
  eventtitle = {{{NAACL}}: {{HLT}} 2018},
  venue = {New Orleans, Louisiana, USA},
  keywords = {ELMo,embeddings,language model,neural network,neural network/architecture,transfer learning}
}

@inproceedings{popovic2017ChrFWordsHelping,
  title = {{{chrF}}++: Words Helping Character n-Grams},
  shorttitle = {{{chrF}}++},
  booktitle = {Proceedings of the {{Second Conference}} on {{Machine Translation}}},
  author = {Popović, Maja},
  date = {2017-09},
  pages = {612--618},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/W17-4770},
  url = {https://aclanthology.org/W17-4770},
  urldate = {2023-10-10},
  eventtitle = {{{WMT}} 2017}
}

@inproceedings{post2018CallClarityReporting,
  title = {A {{Call}} for {{Clarity}} in {{Reporting BLEU Scores}}},
  booktitle = {Proceedings of the {{Third Conference}} on {{Machine Translation}}: {{Research Papers}}},
  author = {Post, Matt},
  date = {2018-10},
  pages = {186--191},
  publisher = {Association for Computational Linguistics},
  location = {Brussels, Belgium},
  doi = {10.18653/v1/W18-6319},
  url = {https://aclanthology.org/W18-6319},
  urldate = {2023-02-22},
  abstract = {The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to “the” BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.},
  eventtitle = {{{WMT}} 2018}
}

@article{prudhommeaux2021AutomaticSpeechRecognition,
  title = {Automatic {{Speech Recognition}} for {{Supporting Endangered Language Documentation}}},
  author = {Prud’hommeaux, Emily and Jimerson, Robbie and Hatcher, Richard and Michelson, Karin},
  date = {2021-11},
  journaltitle = {Language Documentation \& Conservation},
  volume = {15},
  publisher = {University of Hawaii Press},
  issn = {1934-5275},
  url = {http://hdl.handle.net/10125/74666},
  urldate = {2022-10-14},
  abstract = {Generating accurate word-level transcripts of recorded speech for language documentation is difficult and time-consuming, even for skilled speakers of the target language. Automatic speech recognition (ASR) has the potential to streamline transcription efforts for endangered language documentation, but the practical utility of ASR for this purpose has not been fully explored. In this paper, we present results of a study in which both linguists and community members, with varying levels of language proficiency, transcribe audio recordings of an endangered language under timed conditions with and without the assistance of ASR. We find that both time-to-transcribe and transcription error rates are significantly reduced when correcting ASR for language learners of all levels. Despite these improvements, most community members in our study express a preference for unassisted transcription, highlighting the need for developers to directly engage with stakeholders when designing and deploying technologies for supporting language documentation.},
  langid = {american},
  keywords = {⛔ No DOI found}
}

@report{radford2018ImprovingLanguageUnderstanding,
  type = {Technical report},
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018-06-11},
  pages = {12},
  institution = {OpenAI},
  url = {https://openai.com/blog/language-unsupervised/},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english}
}

@article{raffel2020ExploringLimitsTransfer,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2020},
  journaltitle = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v21/20-074.html},
  urldate = {2022-11-29},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  keywords = {⛔ No DOI found}
}

@thesis{ruder2019NeuralTransferLearning,
  type = {phdthesis},
  title = {Neural {{Transfer Learning}} for {{Natural Language Processing}}},
  author = {Ruder, Sebastian},
  date = {2019},
  institution = {Ollscoil na hÉireann},
  location = {Gaillimh, Éire},
  url = {http://ruder.io/thesis/},
  abstract = {The current generation of neural network-based natural language processing modelsexcels at learning from large amounts of labelled data.  Given these capabilities, naturallanguage processing is increasingly applied to new tasks, new domains, and new languages.Current models, however, are sensitive to noise and adversarial examples and prone tooverfitting.  This brittleness, together with the cost of attention, challenges the supervisedlearning paradigm.Transfer learning allows us to leverage knowledge acquired from related data in order toimprove performance on a target task.Implicittransfer learning in the form of pretrainedword representations has been a common component in natural language processing.  Inthis dissertation, we argue that moreexplicittransfer learning is key to deal with thedearth of training data and to improve downstream performance of natural languageprocessing models.  We show experimental results transferring knowledge from relateddomains, tasks, and languages that support this hypothesis.We  make  several  contributions  to  transfer  learning  for  natural  language  processing:Firstly, we propose new methods to automatically select relevant data for supervisedand unsupervised domain adaptation.  Secondly, we propose two novel architectures thatimprove sharing in multi-task learning and outperform single-task learning as well as thestate-of-the-art.  Thirdly, we analyze the limitations of current models for unsupervisedcross-lingual transfer and propose a method to mitigate them as well as a novel latent-variable cross-lingual word embedding model.  Finally, we propose a framework based onfine-tuning language models for sequential transfer learning and analyze the adaptationphase.},
  langid = {english},
  keywords = {neural network,transfer learning}
}

@article{sanchez-cartagena2015GeneralisedAlignmentTemplate,
  title = {A Generalised Alignment Template Formalism and Its Application to the Inference of Shallow-Transfer Machine Translation Rules from Scarce Bilingual Corpora},
  author = {Sánchez-Cartagena, Víctor M. and Pérez-Ortiz, Juan Antonio and Sánchez-Martínez, Felipe},
  date = {2015-07-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  series = {Hybrid {{Machine Translation}}: Integration of Linguistics and Statistics},
  volume = {32},
  number = {1},
  pages = {46--90},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2014.10.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230814001028},
  urldate = {2022-11-15},
  abstract = {Statistical and rule-based methods are complementary approaches to machine translation (MT) that have different strengths and weaknesses. This complementarity has, over the last few years, resulted in the consolidation of a growing interest in hybrid systems that combine both data-driven and linguistic approaches. In this paper, we address the situation in which the amount of bilingual resources that is available for a particular language pair is not sufficiently large to train a competitive statistical MT system, but the cost and slow development cycles of rule-based MT systems cannot be afforded either. In this context, we formalise a new method that uses scarce parallel corpora to automatically infer a set of shallow-transfer rules to be integrated into a rule-based MT system, thus avoiding the need for human experts to handcraft these rules. Our work is based on the alignment template approach to phrase-based statistical MT, but the definition of the alignment template is extended to encompass different generalisation levels. It is also greatly inspired by the work of Sánchez-Martínez and Forcada (2009) in which alignment templates were also considered for shallow-transfer rule inference. However, our approach overcomes many relevant limitations of that work, principally those related to the inability to find the correct generalisation level for the alignment templates, and to select the subset of alignment templates that ensures an adequate segmentation of the input sentences by the rules eventually obtained. Unlike previous approaches in literature, our formalism does not require linguistic knowledge about the languages involved in the translation. Moreover, it is the first time that conflicts between rules are resolved by choosing the most appropriate ones according to a global minimisation function rather than proceeding in a pairwise greedy fashion. Experiments conducted using five different language pairs with the free/open-source rule-based MT platform Apertium show that translation quality significantly improves when compared to the method proposed by Sánchez-Martínez and Forcada (2009), and is close to that obtained using handcrafted rules. For some language pairs, our approach is even able to outperform them. Moreover, the resulting number of rules is considerably smaller, which eases human revision and maintenance.},
  langid = {english},
  keywords = {Hybrid machine translation,Machine translation,Transfer rule inference}
}

@inproceedings{sanchez-cartagena2020MultisourceApproachBreton,
  title = {A Multi-Source Approach for {{Breton}}–{{French}} Hybrid Machine Translation},
  booktitle = {Proceedings of the 22nd {{Annual Conference}} of the {{European Association}} for {{Machine Translation}}},
  author = {Sánchez-Cartagena, Víctor M. and Forcada, Mikel L. and Sánchez-Martínez, Felipe},
  date = {2020-11},
  pages = {61--70},
  publisher = {European Association for Machine Translation},
  location = {Lisboa, Portugal},
  url = {https://aclanthology.org/2020.eamt-1.8},
  urldate = {2022-11-15},
  abstract = {Corpus-based approaches to machine translation (MT) have difficulties when the amount of parallel corpora to use for training is scarce, especially if the languages involved in the translation are highly inflected. This problem can be addressed from different perspectives, including data augmentation, transfer learning, and the use of additional resources, such as those used in rule-based MT. This paper focuses on the hybridisation of rule-based MT and neural MT for the Breton–French under-resourced language pair in an attempt to study to what extent the rule-based MT resources help improve the translation quality of the neural MT system for this particular under-resourced language pair. We combine both translation approaches in a multi-source neural MT architecture and find out that, even though the rule-based system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality.},
  eventtitle = {{{EAMT}} 2020}
}

@inproceedings{schwenk2021CCMatrixMiningBillions,
  title = {{{CCMatrix}}: {{Mining Billions}} of {{High-Quality Parallel Sentences}} on the {{Web}}},
  shorttitle = {{{CCMatrix}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Schwenk, Holger and Wenzek, Guillaume and Edunov, Sergey and Grave, Edouard and Joulin, Armand and Fan, Angela},
  date = {2021-08},
  pages = {6490--6500},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.507},
  url = {https://aclanthology.org/2021.acl-long.507},
  urldate = {2022-11-29},
  abstract = {We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT'19 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT'19 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available.},
  eventtitle = {{{ACL-IJCNLP}} 2021}
}

@inproceedings{schwenk2021WikiMatrixMining135M,
  title = {{{WikiMatrix}}: {{Mining 135M Parallel Sentences}} in 1620 {{Language Pairs}} from {{Wikipedia}}},
  shorttitle = {{{WikiMatrix}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Schwenk, Holger and Chaudhary, Vishrav and Sun, Shuo and Gong, Hongyu and Guzmán, Francisco},
  date = {2021-04},
  pages = {1351--1361},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.eacl-main.115},
  url = {https://aclanthology.org/2021.eacl-main.115},
  urldate = {2022-11-29},
  abstract = {We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but we systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 16720 different language pairs, out of which only 34M are aligned with English. This corpus is freely available. To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.},
  eventtitle = {{{EACL}} 2021}
}

@inproceedings{simons2022AssessingDigitalLanguage,
  title = {Assessing {{Digital Language Support}} on a {{Global Scale}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Simons, Gary F. and Thomas, Abbey L. L. and White, Chad K. K.},
  date = {2022-10},
  pages = {4299--4305},
  publisher = {International Committee on Computational Linguistics},
  location = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.379},
  urldate = {2023-10-19},
  abstract = {The users of endangered languages struggle to thrive in a digitally-mediated world. We have developed an automated method for assessing how well every language recognized by ISO 639 is faring in terms of digital language support. The assessment is based on scraping the names of supported languages from the websites of 143 digital tools selected to represent a full range of ways that digital technology can support languages. The method uses Mokken scale analysis to produce an explainable model for quantifying digital language support and monitoring it on a global scale.},
  eventtitle = {{{COLING}} 2022}
}

@inproceedings{snover2006StudyTranslationEdit,
  title = {A {{Study}} of {{Translation Edit Rate}} with {{Targeted Human Annotation}}},
  booktitle = {Proceedings of the 7th {{Conference}} of the {{Association}} for {{Machine Translation}} in the {{Americas}}: {{Technical Papers}}},
  author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Rich and Micciulla, Linnea and Makhoul, John},
  date = {2006-08-08},
  pages = {223--231},
  publisher = {Association for Machine Translation in the Americas},
  location = {Cambridge, Massachusetts, USA},
  url = {https://aclanthology.org/2006.amta-papers.25},
  urldate = {2023-10-10},
  abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU—even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as—or better than—a second human judgment does.},
  eventtitle = {{{AMTA}} 2006}
}

@online{tang2020MultilingualTranslationExtensible,
  title = {Multilingual {{Translation}} with {{Extensible Multilingual Pretraining}} and {{Finetuning}}},
  author = {Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela},
  date = {2020-08-02},
  eprint = {2008.00401},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2008.00401},
  url = {http://arxiv.org/abs/2008.00401},
  urldate = {2022-11-30},
  abstract = {Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{tiedemann2012ParallelDataTools,
  title = {Parallel {{Data}}, {{Tools}} and {{Interfaces}} in {{OPUS}}},
  booktitle = {Proceedings of the {{Eighth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'12)},
  author = {Tiedemann, Jörg},
  date = {2012-05},
  pages = {2214--2218},
  publisher = {European Language Resources Association (ELRA)},
  location = {Istanbul, Turkey},
  url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf},
  urldate = {2022-11-29},
  abstract = {This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.},
  eventtitle = {{{LREC}} 2012}
}

@inproceedings{tiedemann2020OPUSMTBuildingOpen,
  title = {{{OPUS-MT}} – {{Building}} Open Translation Services for the {{World}}},
  booktitle = {Proceedings of the 22nd {{Annual Conference}} of the {{European Association}} for {{Machine Translation}}},
  author = {Tiedemann, Jörg and Thottingal, Santhosh},
  date = {2020-11},
  pages = {479--480},
  publisher = {European Association for Machine Translation},
  location = {Lisboa, Portugal},
  url = {https://aclanthology.org/2020.eamt-1.61},
  urldate = {2023-10-20},
  abstract = {This paper presents OPUS-MT a project that focuses on the development of free resources and tools for machine translation. The current status is a repository of over 1,000 pre-trained neural machine translation models that are ready to be launched in on-line translation services. For this we also provide open source implementations of web applications that can run efficiently on average desktop hardware with a straightforward setup and installation.},
  eventtitle = {{{EAMT}} 2020}
}

@inproceedings{tyers2009RuleBasedAugmentationTraining,
  title = {Rule-{{Based Augmentation}} of {{Training Data}} in {{Breton-French Statistical Machine Translation}}},
  booktitle = {Proceedings of the 13th {{Annual}} Conference of the {{European Association}} for {{Machine Translation}}},
  author = {Tyers, Francis M.},
  date = {2009-05-14},
  publisher = {European Association for Machine Translation},
  url = {https://aclanthology.org/2009.eamt-1.29},
  urldate = {2022-10-23},
  eventtitle = {{{EAMT}} 2009},
  venue = {Barcelona, España}
}

@inproceedings{tyers2010RulebasedBretonFrench,
  title = {Rule-Based {{Breton}} to {{French}} Machine Translation},
  booktitle = {Proceedings of the 14th {{Annual Conference}} of the {{European Association}} for {{Machine Translation}}},
  author = {Tyers, Francis},
  date = {2010-05-27},
  publisher = {European Association for Machine Translation},
  location = {Saint Raphaël, France},
  url = {https://aclanthology.org/2010.eamt-1.13},
  urldate = {2023-10-10},
  eventtitle = {{{EAMT}} 2010}
}

@online{vigen2024CapitaConsumptionMargarine,
  title = {Per Capita Consumption of Margarine Correlates with {{The}} Divorce Rate in {{Maine}} (R=0.993)},
  author = {Vigen, Tyler},
  date = {2024},
  url = {https://tylervigen.com/spurious/correlation/5920_per-capita-consumption-of-margarine_correlates-with_the-divorce-rate-in-maine},
  urldate = {2024-06-03},
  abstract = {Perhaps as people used less margarine, they became less slippery in their relationships. The lack of artificial spread may have kept the couples from buttering each other up, leading to a decrease in overall marital strife. That's the reality when you can't believe it's not butter - it's a recipe for marital success. Alternatively, it could be that as the margarine consumption decreased, so did the overall slickness in the state, leading to fewer instances of partners feeling like they couldn't grip the marriage.}
}

@online{villalobos2022TrendsTrainingDataset,
  title = {Trends in {{Training Dataset Sizes}}},
  author = {Villalobos, Pablo},
  date = {2022-09-20T09:52:48+00:00},
  url = {https://epochai.org/blog/trends-in-training-dataset-sizes},
  urldate = {2024-06-03},
  abstract = {We collected a database of notable ML models and their training dataset sizes. We use this database to find historical growth trends in dataset size for different domains, particularly language and vision.},
  langid = {english},
  organization = {Epoch AI}
}

@inproceedings{xu2020LRSpeechExtremelyLowResource,
  title = {{{LRSpeech}}: {{Extremely Low-Resource Speech Synthesis}} and {{Recognition}}},
  shorttitle = {{{LRSpeech}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Xu, Jin and Tan, Xu and Ren, Yi and Qin, Tao and Li, Jian and Zhao, Sheng and Liu, Tie-Yan},
  date = {2020-08-23},
  series = {{{KDD}} '20},
  pages = {2802--2812},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3394486.3403331},
  url = {https://doi.org/10.1145/3394486.3403331},
  urldate = {2022-10-14},
  abstract = {Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than \$98\%\$ intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.},
  isbn = {978-1-4503-7998-4},
  keywords = {automatic speech recognition,dual transformation,knowledge distillation,low resource,rare language,speech synthesis,text to speech,transfer learning}
}

@inproceedings{zhang2020ChrEnCherokeeEnglishMachine,
  title = {{{ChrEn}}: {{Cherokee-English Machine Translation}} for {{Endangered Language Revitalization}}},
  shorttitle = {{{ChrEn}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Zhang, Shiyue and Frey, Benjamin and Bansal, Mohit},
  date = {2020-11},
  pages = {577--595},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-main.43},
  url = {https://aclanthology.org/2020.emnlp-main.43},
  urldate = {2022-10-14},
  abstract = {Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.},
  eventtitle = {{{EMNLP}} 2020},
  venue = {Online}
}

@inproceedings{zhang2022HowCanNLP,
  title = {How Can {{NLP Help Revitalize Endangered Languages}}? {{A Case Study}} and {{Roadmap}} for the {{Cherokee Language}}},
  shorttitle = {How Can {{NLP Help Revitalize Endangered Languages}}?},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistic}}},
  author = {Zhang, Shiyue and Frey, Benjamin and Bansal, Mohit},
  date = {2022-05},
  pages = {1529--1541},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2022.acl-long.108},
  urldate = {2022-05-23},
  abstract = {More than 43\% of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism. Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet. In this work, we focus on discussing how NLP can help revitalize endangered languages. We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities, and we discuss three ways in which NLP can potentially assist in language education. We then take Cherokee, a severely-endangered Native American language, as a case study. After reviewing the language's history, linguistic features, and existing resources, we (in collaboration with Cherokee community members) arrive at a few meaningful ways NLP practitioners can collaborate with community partners. We suggest two approaches to enrich the Cherokee language's resources with machine-in-the-loop processing, and discuss several NLP tools that people from the Cherokee community have shown interest in. We hope that our work serves not only to inform the NLP community about Cherokee, but also to provide inspiration for future work on endangered languages in general.},
  eventtitle = {{{ACL}} 2022},
  venue = {Baile Átha Cliath, Éire}
}
