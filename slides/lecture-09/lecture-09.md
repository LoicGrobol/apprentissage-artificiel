---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

[comment]: <> "LTeX: language=fr"

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 9‚ÄØ: Classification de documents partie 1, *Na√Øve Bayes* et R√©gression Logistique
======================================================================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-11
<!-- #endregion -->

```python
from IPython.display import display
```

## Pitch

La **classification de documents** est une t√¢che de TAL qui consiste √† ranger un document dans une et
une seule classe parmi un ensemble pr√©d√©fini.

Elle est tr√®s importante historiquement, car elle a √©t√© une des passerelles entre l'informatique (o√π
on peut la voir comme un cas particulier de la t√¢che g√©n√©rale de classification) et le TAL. En
pratique, comme beaucoup des t√¢ches de TAL peuvent se reformuler comme des t√¢ches de classification,
elle est aussi d'une importance cruciale.

En ce qui nous concerne, elle est aussi int√©ressante parce que les techniques classiques de
classification par apprentissage vont nous donner l'occasion de (re?)d√©couvrir des concepts qui vont
nous servir dans toute la suite de ce cours.
On va l'aborder au travers de deux techniques‚ÄØ: *Na√Øve Bayes* (le ¬´‚ÄØmod√®le bay√©sien na√Øf‚ÄØ¬ª üôÑ) et la
r√©gression logistique, appliqu√©es au mod√®le de repr√©sentation des documents par **sacs de mots**.

On se basera pour la th√©orie et les notations sur les chapitres 4 et 5 de [*Speech and Language
Processing*](https://web.stanford.edu/~jurafsky/slp3/) de Daniel Jurafsky et James H. Martin, qu'il
est donc bon de garder √† port√©e de main.

Pour √©viter d'avoir √† pr√©dater des donn√©es, on va se servir du [dataset d'exemple de `scikit-learn`
*20
newsgroups*](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html)
qu'on a [d√©j√†](../lecture-07/lecture-07.md#Classification-de-textes) rencontr√©, en revanche on √©vitera de se servir directement des fonctions de `scikit-learn`. On sait d√©j√† faire et l'objectif ici est de le faire √† la mano pour bien comprendre ce qui se passe. On se servira aussi
pas mal de NumPy, n'h√©sitez donc pas √† aller revoir [le cours qui le
concerne](../lecture-06/lecture-06.md).

**C'est parti‚ÄØ!**

```python
%pip install -U numpy scikit-learn
```

```python
import numpy as np
```

## Le dataset

```python
from sklearn.datasets import fetch_20newsgroups

categories = [
    "sci.crypt",
    "sci.electronics",
    "sci.med",
    "sci.space",
]

data_train = fetch_20newsgroups(
    subset='train',
    categories=categories,
    shuffle=True,
)

data_test = fetch_20newsgroups(
    subset='test',
    categories=categories,
    shuffle=True,
)
```

```python
print(data_train.DESCR)
```

<!-- #region slideshow={"slide_type": "subslide"} -->
Bon, voyons ce que ce truc a concr√®tement dans le ventre
<!-- #endregion -->

```python
print(len(data_train.data))
print(len(data_test.data))
```

```python
data_train.keys()
```

```python
type(data_train.data)
```

```python
data_train.data[0]
```

```python
type(data_train.target)
```

```python
data_train.target[0]
```

```python
data_train.target_names[0]
```

Bon, c'est plut√¥t clair

## Sac de mots


On va commencer par transformer ces documents en sacs de mots. Pour √ßa on [recycle](../lecture-08/lecture-08.md)

```python
import re
def poor_mans_tokenizer_and_normalizer(s):
    # Cette fois-ci on vire les nombres, les signes de ponctuation et les trucs bizarres
    return [w.lower() for w in re.split(r"\s|\W", s.strip()) if w and all(c.isalpha() for c in w)]
```

```python
from collections import Counter

def get_counts(doc):
    return Counter(poor_mans_tokenizer_and_normalizer(doc))

get_counts(data_train.data[0])
```

Maintenant il faut le faire pour tous les docs üèπ

```python
bows = [get_counts(doc) for doc in data_train.data]
```

Et il nous faut r√©cup√©rer tout le vocabulaire

```python
vocab = sorted(set().union(*bows)) # Pourquoi `sorted` √† votre avis‚ÄØ?
len(vocab)
```

Et pour rendre le tout facile √† manipuler on va en en faire un tableau NumPy de la forme `len(train_data)√ólen(vocab)` qui
tel que le contenu de la cellule `(i, j)` soit le nombre d'occurrence du mot `i` dans le document `j`. On a [d√©j√†](../lecture-06/lecture-06.md#%F0%9F%91%9C-Exo%E2%80%AF:-les-sacs-de-mots-%F0%9F%91%9C) fait √ßa.


On commence par faire un dict avec le vocabulaire

```python
w_to_i = {w: i for i, w in enumerate(vocab)}
```

```python
bow_array = np.zeros((len(bows), len(vocab)))
for i, bag in enumerate(bows):
    for w, c in bag.items():
        bow_array[i, w_to_i[w]] = c
```

On peut aussi faire comme √ßa mais c'est **beaucoup** plus lent. Est-ce que vous pouvez devinez pourquoi‚ÄØ?

```python
# bow_array = np.array(
#     [
#         [bag[w] for w in vocab]
#         for bag in bows
#     ]
# )
```

```python
bow_array
```

## Le mod√®le *Na√Øve Bayes*

√Ä suivre au tableau !

## üßôüèª Exo üßôüèª

√Ä vous de bosser maintenant. √âcrivez

1\. Une fonction qui prend en argument un tableau comme `data_train.target`¬†et qui renvoie un
tableau `class_probs` tel que `class_probs[c]` soit $P(c)$. On choisira le mod√®le de vraisemblance
maximal, soit ici simplement celui qui utilise pour probabilit√©s les fr√©quences empiriques $P(c) =
\frac{\text{nombre d'occurrences de $c$}}{\text{taille de l'√©chantillon}}$.

```python
def get_class_probs(target):
    pass  # Votre code ici
```

2\. Une fonction qui prend en argument un tableau comme `bow_array`¬†et renvoie un tableau
`word_probs` tel que `word_probs[c][w]` soit $P(w|c)$. On utilise toujours le mod√®le de
vraisemblance maximal mais en consid√©rant des occurrences bool√©enne et un lissage laplacien‚ÄØ:
$P(w|c)=\frac{\text{nombre de documents de $c$ dans lesquels $w$ appara√Æt} + 1}{\text{nombre de mots
dans l'ensemble des documents de $c$}+\text{taille du vocabulaire}}$.

N'h√©sitez pas √† √©crire des boucles, au moins pour commencer, avant de passer √† du NumPy fancy.

```python
def get_word_probs(bows):
    pass  # Votre code ici
```

Voil√†, on a un mod√®le de classification *Na√Øve Bayes* üëèüèª


Il reste √† savoir comment s'en servir. Je vous laisse coder √ßa vous-m√™mes. N'h√©sitez pas √† faire des
fonctions auxiliaires.


3\. Une fonction qui prend en argument un document et renvoie la classe la plus probable notre mod√®le. Pensez √† travailler en log-probabilit√©s

```python
def predict_class(doc):
    pass  # Votre code cic
```

Vous pouvez maintenant √©valuer le mod√®le en calculant son exactitude sur l'ensemble de test. 


4\. Un script qui entra√Æne le mod√®le et le sauvegarde (sous la forme qui vous para√Æt la plus
appropri√©e) et un qui charge le mod√®le et pr√©dit la classe de chacun des documents d'un corpus.


Courage, c'est pour votre bien. Si vous vous ennuyez √ßa peut √™tre le bon moment pour d√©couvrir [click](https://click.palletsprojects.com/en/8.0.x/).

5\. Comme ultime raffinement

## Classifieur logistique

## üß† Exo üß†

1\. Tracer avec matplotlib la courbe repr√©sentative de la fonction logistique.

2\. √Ä l'aide d'un lexique de sentiment, √©crivez un classifieur logistique √† deux features‚ÄØ: nombre
de mots positifs et nombre de mots n√©gatifs avec les poids respectifs $0.6$ et $0.4$ et pas de terme
de biais. Appliquez ce classifieur sur le mini-corpus LMDB et calculez son exactitude.

## Apprendre un classifieur logistique

## üò© Exo üò©

1\. Coder la log-vraisemblance n√©gative et son gradient

2\. S'en servir pour apprendre les poids √† donner aux features pr√©c√©dentes √† l'aide du mini-corpus LMDB
