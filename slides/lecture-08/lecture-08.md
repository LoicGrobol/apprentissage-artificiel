---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

[comment]: <> "LTeX: language=fr"

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 8‚ÄØ: Mod√®les de langues √† n-grammes
========================================

**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2021-10-11
<!-- #endregion -->

```python
from IPython.display import display
```

## Pitch

On va apprendre un mod√®le de langues √† n-grammes. On se basera pour la th√©orie et les notations sur
le chapitre 3 de [*Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/) de
Daniel Jurafsky et James H. Martin (garde le donc pas loin). Notre objectif ici sera de faire du
*sampling*.

Pour les donn√©es on va d'abord travailler avec [Le Ventre de
Paris](../../data/zola_ventre-de-paris.txt) qui est d√©j√† dans ce repo pour les tests puis avec [le
corpus CIDRE](https://www.ortolang.fr/market/corpora/cidre) pour passer √† l'√©chelle, mais on
pourrait aussi utiliser Wikipedia (par exemple en utilisant
[WikiExtractor](https://github.com/attardi/wikiextractor)) ou [OSCAR](https://oscar-corpus.com/).

On va devoir faire les choses suivantes

- Extraire les unigrammes et le n-grammes d'un corpus (pour un certain n)
- Calculer les probas normalis√©es des bigrammes
- Les sauvegarder (par exemple dans un TSV)
- Sampler des phrases √† partir du mod√®le
- (En option) √©valuer le mod√®le sur un corpus de test
- Wrapper tout √ßa dans des jolis scripts

On va essayer de faire les choses √† la main, sans trop utiliser de biblioth√®ques, pour bien
comprendre ce qui se passe.

## Premier prototype.


On va commencer par faire en entier le cas des bigrammes sur *Le Ventre de Paris* et on g√©n√©ralisera
ensuite.

### Lire et compter


On commence par lire un fichier et en extraire les unigrammes (ce qui nous donne le vocabulaire) et
les bigrammes. On va pour l'instant faire √ßa tr√®s basiquement avec une b√™te tokenisation sur les
espaces et les signes de ponctuation.

```python
import re
def poor_mans_tokenizer(s):
    return [w for w in re.split(r"\s|(\W)", s.strip()) if w]
```

Vous voyez pour quoi on ne fait pas simplement un `split()`‚ÄØ?

```python
from collections import Counter
unigrams = Counter()
bigrams = Counter()
with open("../../data/zola_ventre-de-paris.txt") as in_stream:
    for line in in_stream:
        words = poor_mans_tokenizer(line.strip())
        unigrams.update(words)
        bigrams.update(zip(words[:-1], words[1:]))
display(unigrams.most_common(10))
display(bigrams.most_common(10))
```

(Si vous trouvez `zip(words[:-1], words[1:])` obscur, faites quelques tests pour voir pourquoi √ßa marche.)

### Calculer les probas


On va ensuite estimer les probas de g√©n√©rer un certain mot $w_1$ sachant que le mot pr√©c√©dent est
$w_0$. On le fait en utilisant la formule du maximum de vraissemblance:

\begin{equation}
   P(w_1|w_0) = \frac{\text{nombre d'occurences du bigramme $w_0 w_1$}}{\text{nombre d'occurrences de l'unigramme $w_0$}}
\end{equation}

Pour que ce soit plus agr√©able √† sampler on va utiliser un dictionnaire de dictionnaires‚ÄØ:
`prob[v][w]` stockera $P(w|v)$.

```python
from collections import defaultdict

probs = defaultdict(dict)
for (v, w), c in bigrams.items():
    probs[v][w] = c/unigrams[v]

# Pour ne pas masquer des erreurs pendant le sampling, on en refait un dict normal
probs = dict(probs)
probs
```

Un autre truc un peu p√©nible c'est qu'en tenant compte de la casse comme on le fait, on s√©pare en
deux les comptes de chaque mot (suivant qu'il se trouve ou non en d√©but de phrase). C'est pas
compl√®tement une erreur mais c'est un peu d√©sagr√©able, on va normaliser tout √ßa.

```python
def poor_mans_tokenizer_and_normalizer(s):
    return [w.lower() for w in re.split(r"\s|(\W)", s.strip()) if w]
```

### G√©n√©rer

Pour l'instant on ne va pas se pr√©occuper de sauvegarder le mod√®le on va l'utiliser directement pour
sampler. Le principe est simple‚ÄØ: on sample le premier mot, puis on sample le deuxi√®me mot en
prenant le premier qu'on vient de g√©n√©rer et ainsi de suite.


Est-ce que vous voyez le probl√®me‚ÄØ?


Comment on sample le premier mot‚ÄØ?

Et quand est-ce qu'on d√©cide de s'arr√™ter‚ÄØ?


On rouvre le bouquin et on trouve

>  We‚Äôll first need to augment each sentence with a special symbol `<s>` at the beginning of the
> sentence, to give us the bigram context of the first word. We‚Äôll also need a special end-symbol.
> `</s>`


Oups


Allez, on corrige

```python
unigrams = Counter()
bigrams = Counter()
with open("../../data/zola_ventre-de-paris.txt") as in_stream:
    for line in in_stream:
        words = poor_mans_tokenizer_and_normalizer(line.strip())
        if "<s>" in words or "</s>" in words:
            raise ValueError(f"Symboles de d√©but/fin de phrases d√©j√† pr√©sents dans le corpus {line!r}")
        unigrams.update(("<s>", *words, "</s>"))
        bigrams.update(zip(("<s>", *words), (*words, "</s>")))

probs = defaultdict(dict)
for (v, w), c in bigrams.items():
    probs[v][w] = c/unigrams[v]

probs = dict(probs)
probs
```

Il y a encore un petit probl√®me

```python
probs["<s>"]["</s>"]
```

ü§î


On a compt√© les lignes vides üò§. √áa ne posait pas de probl√®me jusque-l√† puisque √ßa n'ajoutait rien
aux compteurs de n-grammes, mais maintenant √ßa nous fait des `["<s>", "</s>"]`.


C'est reparti

```python
unigrams = Counter()
bigrams = Counter()
with open("../../data/zola_ventre-de-paris.txt") as in_stream:
    for line in in_stream:
        # Voi-l√†
        if line.isspace():
            continue
        words = poor_mans_tokenizer_and_normalizer(line.strip())
        # Pourquoi on fait √ßa‚ÄØ?
        if "<s>" in words or "</s>" in words:
            raise ValueError(f"Symboles de d√©but/fin de phrases d√©j√† pr√©sents dans le corpus {line!r}")
        unigrams.update(("<s>", *words, "</s>"))
        bigrams.update(zip(("<s>", *words), (*words, "</s>")))

probs = defaultdict(dict)
for (v, w), c in bigrams.items():
    probs[v][w] = c/unigrams[v]

probs = dict(probs)
probs
```

### G√©n√©rer pour de vrai

**Bon c'est bon maintenant‚ÄØ?**


√Ä peu pr√®s. On va pouvoir sampler.


Pour √ßa on va piocher dans le module [`random`](https://docs.python.org/3/library/random.html) de la
biblioth√®que standard, et en particulier la fonction
[`random.choices`](https://docs.python.org/3/library/random.html#random.choices) qui permet de tirer
au sort dans une population finie en pr√©cisant les probabilit√©s de chacun de √©l√©ments. Le poids
n'ont en principe pas besoin d'√™tre normalis√©s (mais ils le seront ici, √©videmment).

```python
import random
```

Voyons d√©j√† comment choisir le premier mot

```python
candidates = list(probs["<s>"].keys())
#  On pourrait faire plus fancy avec `zip`, cherchez comment
weights = [probs["<s>"][c] for c in candidates] 
random.choices(candidates, weights)[0]  # `choices` renvoit une liste, voir sa doc
```

√áa marche, maintenant une phrase‚ÄØ! On sample mot par mot et on s'arr√™te quand on arrive √† `</s>`

```python
sent = ["<s>"]
while sent[-1] != "</s>":
    candidates = list(probs[sent[-1]].keys())
    weights = [probs[sent[-1]][c] for c in candidates]
    sent.append(random.choices(candidates, weights)[0])

print(" ".join(sent[1:-1]))
```

C'est rigolo, hein‚ÄØ?


Qu'est-ce que vous pensez des textes qu'on g√©n√®re‚ÄØ?

### Les trigrammes

Avant de g√©n√©raliser, on va voir comment passer aux trigrammes

```python
bigrams = Counter()
trigrams = Counter()
with open("../../data/zola_ventre-de-paris.txt") as in_stream:
    for line in in_stream:
        if line.isspace():
            continue
        words = poor_mans_tokenizer_and_normalizer(line.strip())
        if "<s>" in words or "</s>" in words:
            raise ValueError(f"Symboles de d√©but/fin de phrases d√©j√† pr√©sents dans le corpus {line!r}")
        words = ["<s>", "<s>", *words, "</s>"]
        bigrams.update(zip(words[:-1], words[1:]))
        # On pourrait faire comme avec les bigrammes mais √ßa g√©n√©ralisera mieux comme √ßa
        # √Ä votre avis pourquoi des tuples‚ÄØ?
        trigrams.update((tuple(words[i-2:i]), w) for i, w in enumerate(words[2:], start=2))

probs = defaultdict(dict)
for ((u, v), w), c in trigrams.items():
    probs[(u, v)][w] = c/bigrams[(u, v)]

probs = dict(probs)
probs
```

```python
sent = ["<s>", "<s>"]
while sent[-1] != "</s>":
    candidates = list(probs[tuple(sent[-2:])].keys())
    weights = [probs[tuple(sent[-2:])][c] for c in candidates]
    sent.append(random.choices(candidates, weights)[0])

print(" ".join(sent[2:-1]))
```

## Les n-grammes

On passe aux n-grammes‚ÄØ? On va essayer de les faire de fa√ßon un peu plus compacte.

```python
def get_ngrams_probs(path, n=2):
    ngrams = defaultdict(Counter)
    with open(path) as in_stream:
        for line in in_stream:
            if line.isspace():
                continue
            words = poor_mans_tokenizer_and_normalizer(line.strip())
            if "<s>" in words or "</s>" in words:
                raise ValueError(f"Symboles de d√©but/fin de phrases d√©j√† pr√©sents dans le corpus {line!r}")
            words = [*("<s>" for _ in range(n-1)), *words, "</s>"]
            for i, w in enumerate(words[n-1:], start=n-1):
                ngrams[tuple(words[i-n+1:i])][w] += 1
    probs = defaultdict(dict)
    for trigger, targets in ngrams.items():
        trigger_occurences = sum(targets.values())
        for t, c in targets.items():
            probs[trigger][t] = c/trigger_occurences
    return dict(probs)
```

```python
get_ngrams_probs("../../data/zola_ventre-de-paris.txt", 4)
```

```python
def sample_from_probs(probs, n):
    # On pourrait inf√©rer n automatiquement mais fleeeeemme
    sent = ["<s>" for _ in range(n-1)]
    while sent[-1] != "</s>":
        candidates = list(probs[tuple(sent[-n+1:])].keys())
        weights = [probs[tuple(sent[-n+1:])][c] for c in candidates]
        sent.append(random.choices(candidates, weights)[0])
    return " ".join(sent[n-1:-1])
```

```python
probs = get_ngrams_probs("../../data/zola_ventre-de-paris.txt", 4)
```

```python
sample_from_probs(probs, 4)
```

Vous voyez un probl√®me‚ÄØ?

## Un peu d'originalit√©

Le mod√®le ici marche, mais comme le corpus est un peu petit, il manque souvent d'originalit√© pour
des grandes valeurs de $n$. Il y a plusieurs fa√ßons d'y rem√©dier et les sections 3.4 et 3.5 de
*Speech and Language Processing donnent plus de d√©tails √† ce sujet.
