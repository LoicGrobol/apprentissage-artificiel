---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
TP 1 : ModÃ¨les de langues Ã  n-grammes
========================================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

## ModÃ¨les de langues

Qu'est-ce que vous pensez des phrases suivantesâ€¯?

> Bonjour, Ã§a vaâ€¯?


> Je reconnais l'existence du kiwi-fruit.


> Les idÃ©es vertes incolores dorment furieusement.


> Vous dÃ©sastre rÃ©jouirez de que ce aucun.


> oijj eofiz ipjij paihefoÃ®ozenui.


Est-ce qu'il y en a qui vous parlent plus que d'autresâ€¯? Pourquoiâ€¯?


Pour plein de raisons, Ã©tant donnÃ© un langage (et une variÃ©tÃ© de ce langage, etc.), il y a des
phrases qu'on risque de voir ou d'entendre plus souvent que d'autres.


On peut dire ainsi que certaines phrases sont plus **vraisemblables** que d'autres.


On peut y penser de la maniÃ¨re suivante (pour l'instant)â€¯:

- On prend toutes les phrases qui ont Ã©tÃ© un jour prononcÃ©es dans cette langue.
- On les Ã©crit toutes (avec rÃ©pÃ©tition) sur des bouts de papiers.
- On met les bouts de papier dans une urne gÃ©ante, on touille et on en choisit un.


On peut alors parler de *probabilitÃ©* d'avoir choisi une phrase donnÃ©e. Et se demanderâ€¯:

> Si j'ai une phrase, par exemple Â«â€¯Toi dont le trÃ´ne Ã©tincelle, Ã´ immortelle Aphrodite.â€¯Â», comment
> estimer cette probabilitÃ©â€¯?


Un modÃ¨le de langue, c'est un **modÃ¨le** qui permet d'**estimer** la **vraisemblance** d'une
**phrase**.


Notre objectif aujourd'hui c'est de voir comment on fait Ã§a, d'abord en thÃ©orie, puis en pratique
sur une application marrante et trÃ¨s trÃ¨s trÃ¨s Ã  la modeâ€¯: la gÃ©nÃ©ration de textes.


Ã€Â quoi Ã§a sertâ€¯?


Ã€ plein de trucs

- Traduction automatiqueâ€¯:
  - $P(\text{moche temps pour la saison}) > P(\text{sale temps pour la saison})$
- Correction orthographiqueâ€¯:
  - Je ne peux pas **croitre** cette histoire
  - $P(\text{peux pas croire cette}) > P(\text{peux pas croitre cette})$
- Reconnaissance de la parole (ASR)
  - $P(\text{Par les temps qui courent}) â‰« P(\text{Parle et t'en qui cours})$
- RÃ©sumÃ© automatique, questions/rÃ©ponsesâ€¦


On se basera pour la thÃ©orie et les notations sur le chapitre 3 de [*Speech and Language
Processing*](https://web.stanford.edu/~jurafsky/slp3/) de Daniel Jurafsky et James H. Martin. Ã€ ta
place, je le garderais donc Ã  portÃ©e de main, le poly *et* les slides (et je prendrais le temps de
lire les chapitres prÃ©cÃ©dents au calme).

## Formalisons (un peu)


On veut assigner des probabilitÃ©s (â‰ˆ) Ã  des sÃ©quences de mots.


Si on note une sÃ©quence de mots $S = w_1, w_2, â€¦, w_n$, on notera sa probabilitÃ© $P(w_1, w_2, â€¦,
w_n)$.


### Estimateur du maximum de vraisemblance

Rappelâ€¯: on peut estimer la probabilitÃ© d'un truc en calculant sa frÃ©quence d'apparition.


Par exemple, si on veut estimer la probabilitÃ© qu'un dÃ© truquÃ© fasse 6â€¯:

- On lance le dÃ© un grand nombre de fois (mettons qu'on choisisse 1000), on parle d'**Ã©chantillon**.
- On compte le nombre de fois qu'on a obtenu 6, imaginons que c'est 271.
- On calcule la **frÃ©quence d'apparition** de 6â€¯: \frac{271}{1000} = 0.271.
- On **choisit** cette valeur comme estimation de la probabilitÃ© d'avoir 6


Notez que c'est bien une estimation, et qu'elle n'est pas infaillible. On peut obtenir 1000 fois 6
de suite, mÃªme avec un dÃ© Ã©quilibrÃ©. C'est improbable, mais Ã§a peut arriver, et dans ce cas notre
estimation de la probabilitÃ© sera affreusement fausse.


Cette faÃ§on d'estimer une probabilitÃ©, c'est (un cas particulier de) l'**estimateur du maximum de
vraisemblance**. La faÃ§on la plus simple d'estimer des probabilitÃ©s.


Ok, super, il donne quoi cet estimateur pour notre problÃ¨meâ€¯? En quoi Ã§a consisteâ€¯? Ã€ votre avisâ€¯?


Et bien imaginons qu'on veuille dÃ©terminer la probabilitÃ© d'une phrase, par exemple Â«â€¯le petit chat
est contentâ€¯Â».

- On prend un gros corpus (c'est notre Ã©chantillon).
- On regarde combien de fois cette phrase apparaÃ®t.
- Et on divise par la taille du corpus.


Voyons ce que Ã§a donneâ€¯:

- [Combien de pages sur Google pour cette
  requÃªte](https://www.google.com/search?q=%22le+petit+chat+est+content%22).
- Combien de pages au total dans l'index de Googleâ€¯? Dur Ã  savoir, mais probablement de l'ordre de
  grandeur de $100â€¯000â€¯000â€¯000$.

On estimerait alors la probabilitÃ© de cette phrase Ã  $0.00000000008$.


Ok, parfait, on a finiâ€¯?


C'est quoi la probabilitÃ© de Â«â€¯je reconnais l'existence du kiwi-fruitâ€¯Â» alorsâ€¯?


<https://www.google.com/search?q=%22je+reconnais+l'existence+du+kiwi-fruit%22>


Alorsâ€¯?


$0$â€¯?


Mais Â«â€¯Vous dÃ©sastre rÃ©jouirez de que ce aucunâ€¯Â». Ã‡a serait zÃ©ro aussi alorsâ€¯? Est-ce que vraiment
on veut mettre la mÃªme probabilitÃ© Ã  ces deux phrasesâ€¯?


Oups.


Le problÃ¨me, c'est que l'Ã©chantillon qu'il nous faudrait ce n'est pas un Ã©chantillon de tout ce qui
a dÃ©jÃ  Ã©tÃ© produit comme phrase, mais un Ã©chantillon de tout ce qui **pourrait** Ãªtre produit. Et
Ã©videmment ce n'est pas accessible.

### DÃ©composer pour rÃ©gner


Ok, [essayons encore](https://www.youtube.com/watch?v=Xg4Pa3DORCE).


Il nous faut une faÃ§on plus subtile de procÃ©der. On va se reposer pour Ã§a sur une propriÃ©tÃ©
intÃ©ressante du langage humainâ€¯:


Si je disâ€¯: Â«â€¯je suis en train d'Ã©crire sur leâ€¦â€¯Â». Quel est le mot suivant d'aprÃ¨s-vousâ€¯?


Il y a Ã©videmment plusieurs solutions. Mais *certaines semblent plus vraisemblables*. ğŸ§.


Autrement ditâ€¯: il y a une corrÃ©lation (attention, pas un conditionnement total) imposÃ©e par le
dÃ©but d'une phrase sur sa suite.


On va s'appuyer sur Ã§a pour proposerÂ un modÃ¨le de langue qui soit **implÃ©mentable** (et aprÃ¨s ~~on~~
vous allez l'implÃ©menter).


On va imaginer un modÃ¨le de langue qui fonctionne comme un **processus alÃ©atoire**, c'est-Ã -dire
comme une sÃ©rie de dÃ©cisions alÃ©atoires. En l'occurrence, on va imaginer un processus oÃ¹ la phrase
est gÃ©nÃ©rÃ©e mot par mot.


Autrement ditâ€¯:

- On choisit le premier mot $w_0$ en regardant pour un corpus Ã©chantillon les frÃ©quences des mots
  apparaissant en dÃ©but de phrase.
- On choisit le deuxiÃ¨me mot $w_1$ en regardant les frÃ©quences des mots apparaissant en deuxiÃ¨me
  position dans les phrases qui commencent par $w_0$.
- On choisit $w_2$ en regardant les mots qui apparaissent en troisiÃ¨me position dans les phrases qui
  commencent par $w_0, w_1$
- â€¦

<!-- TODO: commencer par regarder le calcul des probas sur un petit exemple comme dans J&M avant de passer au cas gÃ©nÃ©ral -->


Les probabilitÃ©s ici sont plus faciles Ã  estimerâ€¯:

La probabilitÃ© $P([w_0, *])$ (qu'on notera aussi $P(w_0)$) qu'un mot apparaisse en dÃ©but de phrase,
c'est

\begin{equation}
    P(w_0) = \frac{\text{Nombre de phrases qui commencent par $w_0$}}{\text{Nombre de phrases dans le corpus}}
\end{equation}


La probabilitÃ© $P([w_0, w_1, *]~|~[w_0, *])$, ou $P(w_1|w_0)$ qu'une phrase commence par $w_0, w_1$
sachant qu'elle commence par $w_1$ (on parle de probabilitÃ© conditionnelle), c'est

\begin{equation}
    P(w_1|w_0) = \frac{\text{Nombre de phrases qui commencent par $w_0, w_1$}}{\text{Nombre de phrases qui commencent par $w_0$}}
\end{equation}

et ainsi de suite.


Et c'est quoi alors la probabilitÃ© de la phrase entiÃ¨reâ€¯? Et bien, c'est simplement le produit des
probabilitÃ©s, comme quand on suit une sÃ©rie d'expÃ©riences avec un arbreâ€¯:

\begin{equation}
    P(w_0, w_1, â€¦, w_n) = P(w_0) Ã— P(w_1|w_0) Ã— P(w_2|w_0, w1) Ã— â€¦ Ã— P(w_n |Â w_0, w_1, â€¦, w_{n-1})
\end{equation}

### N-grammes

Ã‰videmment Ã§a ne pouvait pas Ãªtre si simple.


**Ã‰videmment.**


Le problÃ¨me ici, c'est que la procÃ©dure itÃ©rative qu'on a dÃ©crite marche bien en dÃ©but de phrase,
mais en fin de phrase on retombe sur le problÃ¨me prÃ©cÃ©dent.

\begin{equation}
    P(\text{vert}~|~\text{Je}, \text{reconnais}, \text{l'}, \text{existence}, \text{du}, \text{kiwi-fruit})
\end{equation}


On va donc faire une hypothÃ¨se un peu grossiÃ¨reâ€¯: on va supposer par exemple que

\begin{equation}
    P(w_3~|~w_0, w_1, w_2) = P(w_3~|~w_2)
\end{equation}

Autrement dit la probabilitÃ© d'apparition d'un mot ne dÃ©pend que des $n-1$ (ici $1$) mots
prÃ©cÃ©dents. Nous donnant ainsi un **modÃ¨le de langue Ã  n-grams** (ici bigrammes).

## Ã€ vous de jouerâ€¯!

Notre objectif ici sera de faire de la **gÃ©nÃ©ration de textes**.

Pour les donnÃ©es on va d'abord travailler avec [Le Ventre de
Paris](../../data/zola_ventre-de-paris.txt) qui est dÃ©jÃ  dans ce repo pour les tests puis avec [le
corpus CIDRE](https://www.ortolang.fr/market/corpora/cidre) pour passer Ã  l'Ã©chelle, mais on
pourrait aussi utiliser Wikipedia (par exemple en utilisant
[WikiExtractor](https://github.com/attardi/wikiextractor)) ou [OSCAR](https://oscar-corpus.com/).

On va devoir faire les choses suivantes (pour un modÃ¨le Ã  bigrammes)

- Extraire les unigrammes et les bigrammes d'un corpus
- Calculer les probas normalisÃ©es des bigrammes
- Sampler des phrases Ã  partir du modÃ¨le

On va essayer de faire les choses Ã  la main, sans trop utiliser de bibliothÃ¨ques, pour bien
comprendre ce qui se passe.

Puis on Ã©tendra Ã  des trigrammes et des n-grammes.

## âœ‚ï¸ Tokenization âœ‚ï¸

1\. Ã‰crire une fonction `crude_tokenizer` qui prend comme argument une chaine de caractÃ¨res et
    renvoie la liste des mots de cette chaÃ®ne en sÃ©parant sur les espaces.

```python tags=["raises-exception"]
def crude_tokenizer(s):
    pass # Ã€ toi de coder

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', "l'existence", 'du', 'kiwi-fruit.'
]
```

2\. Modifier la fonction `crude_tokenizer` pour qu'elle sÃ©pare aussi suivant les caractÃ¨res
   non alphanumÃ©riques. **Indice** Ã§a peut Ãªtre utile de revoir [la doc sur les expressions
   rÃ©guliÃ¨res](https://docs.python.org/3/library/re.html) ou de relire [un tuto Ã  ce
   sujet](https://realpython.com/regex-python/).

```python tags=["raises-exception"]
def crude_tokenizer(s):
    pass # Ã€ toi de coder

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', 'l', 'existence', 'du', 'kiwi', 'fruit'
]
```

3\. On aimerait maintenant garder les apostrophes Ã  la fin du mot qui les prÃ©cÃ¨de, ainsi que les
mots composÃ©s ensemble.

```python tags=["raises-exception"]
def crude_tokenizer(s):
    pass # Ã€ toi de coder

assert crude_tokenizer("Je reconnais l'existence du kiwi-fruit.") == [
    'Je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit'
]
```

4\. Ã‰crire une fonction `crude_tokenizer_and_normalizer` qui en plus de tokenizer comme prÃ©cÃ©demment
met tous les mots en minuscules

On peut Ã©videmment copier-coller le code au-dessus, mais on peut aussi rÃ©utiliser ce qu'on a dÃ©jÃ 
dÃ©finiâ€¯:

```python tags=["raises-exception"]
def crude_tokenizer_and_normalizer(s):
    pass # Ã€ toi de coder

asser = crude_tokenizer_and_normalizer("Je reconnais l'existence du kiwi-fruit.") == [
    'je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit'
]
```

## ğŸ’œ Extraire les bigrammes ğŸ’œ

Ã‰crire une fonction `extract_bigrams` qui prend en entrÃ©e une liste de mots et renvoie la liste des
bigrammes correspondants sous forme de couples de mots.


Version directe

```python tags=["raises-exception"]
def extract_bigrams(words):
    pass # Ã€ toi de coder

assert extract_bigrams(['je', 'reconnais', "l'", 'existence', 'du', 'kiwi-fruit']) == [
    ('je', 'reconnais'),
     ('reconnais', "l'"),
     ("l'", 'existence'),
     ('existence', 'du'),
     ('du', 'kiwi-fruit')
]
```


## ğŸ”¢ Compter ğŸ”¢


Ã‰crire une fonction `read_corpus` qui prend en argument un chemin vers un fichier texte, l'ouvre, le
tokenize et y compte les unigrammes et les bigrammes en renvoyant deux `Counter` associant
respectivement Ã  chaque mot et Ã  chaque bigramme leurs nombres d'occurrences.

```python tags=["raises-exception"]
from collections import Counter
    
def read_corpus(file_path):
    unigrams = Counter()
    bigrams = Counter()
    pass # Ã€ toi de coder
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('de', 5292), ('la', 3565), ('les', 2746), ('il', 2443)]
assert bigram_counts.most_common(4) == [
    (('de', 'la'), 754),
     (("qu'", 'il'), 424),
     (('Ã ', 'la'), 336),
     (("d'", 'une'), 321)
]
```

## ğŸ¤“ Estimer les probas ğŸ¤“


On va ensuite estimer les probabilitÃ©s de transition, c'est-Ã -dire la probabilitÃ© de gÃ©nÃ©rer un
certain mot $w_1$ sachant que le mot prÃ©cÃ©dent est $w_0$. On le fait en utilisant la formule du
maximum de vraisemblanceâ€¯:

\begin{equation}
   P(w_1|w_0) := P\!\left([w_0, w_1]~|~[w_0, *]\right)
    = \frac{
        \text{nombre d'occurrences du bigramme $w_0 w_1$}
      }{
        \text{nombre d'occurrences de l'unigramme $w_0$}}
\end{equation}

Pour que ce soit plus agrÃ©able Ã  sampler on va utiliser un dictionnaire de dictionnairesâ€¯:
`probs[v][w]` stockera $P(w|v)$.

Ã€ vous de jouerâ€¯: Ã©crire une fonction `get_probs`, qui prend en entrÃ©e les compteurs de bigrammes
et d'unigrammes et renvoie le dictionnaire `probs`.

```python tags=["raises-exception"]
def get_probs(unigram_counts, bigram_counts):
    pass # Ã€ toi de coder

probs = get_probs(unigram_counts, bigram_counts)
assert probs["je"]["dÃ©jeune"] == 0.002232142857142857
```

**Astuce** on peut utilise un `defaultdict`.

## ğŸ’ğŸ» GÃ©nÃ©rer un mot ğŸ’ğŸ»

**Bon c'est bon maintenantâ€¯?**


Ouiâ€¯! On va enfin pouvoir gÃ©nÃ©rer des trucsâ€¯!


Pour Ã§a on va piocher dans le module [`random`](https://docs.python.org/3/library/random.html) de la
bibliothÃ¨que standard, et en particulier la fonction
[`random.choices`](https://docs.python.org/3/library/random.html#random.choices) qui permet de tirer
au sort dans une population finie en prÃ©cisant les probabilitÃ©s (ou *poids*) de chacun des Ã©lÃ©ments.
Les poids n'ont en principe pas besoin d'Ãªtre normalisÃ©s (mais ils le seront ici, vu comme on les a
construits).

```python
import random
```

Voici par exemple comment choisir un Ã©lÃ©ment dans la liste `["a", "b", "c"]` en donnant comme
probabilitÃ©s respectives Ã  ses Ã©lÃ©ments $0.5$, $0.25$ et $0.25$

```python
candidates = ["a", "b", "c"]
weights = [0.5, 0.25, 0.25]
random.choices(candidates, weights, k=1)[0]  # Attention: `choices` renvoit une liste
```

Ã€ vous de jouerâ€¯: Ã©crire une fonction `gen_next_word` qui prend en entrÃ©e le dictionnaire `probs` et
un mot et renvoie en sortie un mot suivant, choisi en suivant les probabilitÃ©s estimÃ©es prÃ©cÃ©demment


## ğŸ¤” GÃ©nÃ©rer un texte  ğŸ¤”

On va maintenant pouvoir utiliser notre modÃ¨le pour gÃ©nÃ©rer du texte. Le principe est simpleâ€¯: on
choisit le premier mot, puis on choisit le deuxiÃ¨me mot en prenant en compte celui qu'on vient de
gÃ©nÃ©rer (le premier donc si vous suivez) et ainsi de suite.


**Questions**

- Comment on choisit le premier motâ€¯?
- Et quand est-ce qu'on dÃ©cide de s'arrÃªterâ€¯?


Jurafsky et Martin nous disent

<!-- LTeX: language=en-Us -->
> Weâ€™ll first need to augment each sentence with a special symbol `<s>` at the beginning of the
> sentence, to give us the bigram context of the first word. Weâ€™ll also need a special end-symbol.
> `</s>`
<!-- LTeX: language=fr -->

Heureusement on a un fichier bien faitâ€¯: il y a une seule phrase par ligne.


1\. Modifier `read_corpus` pour ajouter Ã  la volÃ©e `<s>` au dÃ©but de chaque ligne et `</s>` Ã  la fin
de chaque ligne.

```python tags=["raises-exception"]
def read_corpus(file_path):
    pass # Ã€ toi de coder
    
    return unigrams, bigrams


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('<s>', 8945), ('</s>', 8945), ('de', 5292), ('la', 3565)]
assert bigram_counts.most_common(4) == [
    (('<s>', '</s>'), 1811),
    (('<s>', 'il'), 775),
    (('de', 'la'), 754),
    (('<s>', 'elle'), 576)
]
```

Il y a encore un petit problÃ¨me

```python tags=["raises-exception"]
bigram_counts.most_common(1)
```

ğŸ¤”


On a comptÃ© les lignes vides ğŸ˜¤. Ã‡a ne posait pas de problÃ¨me jusque-lÃ  puisque Ã§a n'ajoutait rien
aux compteurs de n-grammes, mais maintenant Ã§a nous fait des `["<s>", "</s>"]`.


2\. Modifier `read_corpus` pour ignorer les lignes vides

```python tags=["raises-exception"]
def read_corpus(file_path):
    pass # Ã€ toi de coder


unigram_counts, bigram_counts = read_corpus("data/zola_ventre-de-paris.txt")

assert unigram_counts.most_common(4) == [('<s>', 7145), ('</s>', 7145), ('de', 5292), ('la', 3565)]
assert bigram_counts.most_common(4) == [
    (('<s>', 'il'), 775),
    (('de', 'la'), 754),
    (('<s>', 'elle'), 576),
    (("qu'", 'il'), 424)
]

probs = get_probs(unigram_counts, bigram_counts)
assert probs["<s>"]["le"] == 0.0298110566829951
```

## ğŸ˜Œ GÃ©nÃ©rer pour de vrai ğŸ˜Œ

Ã‰crire une fonction `sample` qui prend en argument les probabilitÃ©s de bigrammes (sous la forme d'un
dictionnaire de dictionnaires comme notre `prob`) et gÃ©nÃ¨re une phrase en partant de `<s>` et en
ajoutant des mots itÃ©rativement, s'arrÃªtant quand `</s>` a Ã©tÃ© choisi.

```python tags=["raises-exception"]
def generate(bigram_probs):
    pass # Ã€ toi de coder
```

Pas de `assert` ici comme on a de l'alÃ©atoire, mais la cellule suivante permet de tester si Ã§a
marcheâ€¯:

```python
print(generate(probs))
```

Et ici pour avoir du texte qui ressemble Ã  quelque choseâ€¯:

```python tags=["raises-exception"]
print(" ".join(generate(probs)[1:-1]))
```

C'est rigolo, heinâ€¯?


Qu'est-ce que vous pensez des textes qu'on gÃ©nÃ¨reâ€¯?

## ğŸ§ Aller plus loin ğŸ§


En vous inspirant de ce qui a Ã©tÃ© fait, coder un gÃ©nÃ©rateur de phrases Ã  partir de trigrammes, puis
de n-grammes arbitraires.
